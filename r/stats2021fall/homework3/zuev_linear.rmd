# Task 1

## Setup

```{r}
library(tidyverse)
library(nortest) # for `pearson.test`
param.alpha <- .01
dd <- data.frame(
  x = c(2, 2, 3, 3, 2, 2, 3, 3, 1, 2, 1, 3, 3, 2, 3, 3, 5,
        4, 2, 3, 2, 4, 2, 5, 3, 1, 4, 4, 3, 3, 2, 0, 1, 3,
        2, 4, 1, 2, 4, 4, 3, 1, 3, 3, 3, 3, 3, 3, 3, 3),
  y = c(7.73, 8.48, 6.32, 7.77, 9.27, 16.50, 0.76, 15.88, 17.74, 3.60, 16.51, 7.36, 5.70, 20.78, 7.93, 9.48, 17.41,
        15.39, 13.35, 18.13, 6.53, 3.47, 9.34, 5.04, 8.08, 15.46, 4.49, 8.59, 16.29, 14.91, 9.28, 2.52, 14.78, 7.19,
        1.76, 12.47, 8.11, 24.42, 9.73, 7.09, 0.61, 10.16, 14.71, 9.94, 23.16, 13.07, 9.46, 3.90, 4.79, 14.32))
```

## (a) Fitting a linear model and plotting the data

```{r}

least_squares <- function(x = ? matrix, dd = ? data.frame){
  y <- as.matrix(dd$y)
  b <- solve(x %*% t(x)) %*% (x %*% y) # (xx')^{-1}xy
  y_hat <- t(x) %*% b
  print(
    dd %>%
      mutate(y_hat = y_hat) %>%
      ggplot(aes(x, y)) +
      geom_point() +
      geom_line(aes(y = y_hat), color = "red") +
      theme_minimal())
  list("coef" = b, "y" = y, "y_hat" = y_hat)
} -> matrix

order1.regression <- dd %>%
  mutate(bias = 1) %>%
  select(bias, x) %>%
  as.matrix %>%
  t %>%
  least_squares(dd = dd)

```

## (b) The same with the square term
```{r}
order2.x <- dd %>%
  mutate(bias = 1, x_sq = x^2) %>%
  select(bias, x, x_sq) %>%
  as.matrix %>%
  t
order2.regression <- least_squares(x = order2.x, dd = dd)

```
## (c) Testing residuals of the polynomial model for normality

```{r}
order2.residuals <- unlist(order2.regression["y"]) - unlist(order2.regression["y_hat"])
hist(order2.residuals)
pearson.test(order2.residuals)

data.frame(x = order2.residuals) %>% ggplot(aes(x)) +
  geom_histogram(aes(y = ..density..), bins = 6, colour = "black", fill = "white") +
  stat_function(fun = dnorm, args = list(mean = mean(order2.residuals), sd = sd(order2.residuals)), colour = "red")
```

## (d) confidence intervals for coefficients

```{r}
conf.q <- 2 # the length of the parameter vector (we look at two parameters)
dd$x2 <- dd$x^2
order2.linregr <- lm(dd$y ~ dd$x + dd$x2) # marvelous! Built-in method's results match my implementation's results
confint(order2.linregr, parm = c(2, 3), level = 1 - param.alpha / conf.q)

order2.x.r <- Matrix::rankMatrix(as.matrix(order2.x))[1]
order2.x.n <- ncol(order2.x)
dd.n <- nrow(dd)
dd.r <- Matrix::rankMatrix(order2.x)
bonferroni.quantile <- qf(p = 1 - param.alpha / conf.q, df1 = conf.q, df2 = dd.n - dd.r)

conf.c <- rbind(c(0, 0), c(1, 0), c(0, 1))
conf.s2 <- var(order2.linregr$residuals)
conf.xxt_inv <- t(solve(order2.x %*% t(order2.x)))
conf.gamma <- conf.s2 * t(conf.c) %*%
  conf.xxt_inv %*%
  conf.c

order2.tested_coefs <- t(conf.c) %*% order2.regression$coef
order2.confdelta <- bonferroni.quantile * sqrt(diag(conf.gamma))# TODO
order2.confint <- data.frame(lo = order2.tested_coefs - order2.confdelta, hi = order2.tested_coefs + order2.confdelta)
print(order2.confint)
```
## (e) linear dependency / independence hypotheses
```{r}
hypot.c1 <- rbind(c(1, 0), c(0, 1), c(0, 0))
hypot.c2 <- matrix(c(1, 0, 0), nrow = 3)
hypot.f_stats <- list("dependent" = hypot.c1, "independent" = hypot.c2) %>%
  lapply(function(c = ? matrix) {
    hypot.beta <- t(c) %*% conf.xxt_inv %*% c
    psi <- t(c) %*% order2.regression$coef
    conf.f_stat <- t(psi) %*% hypot.beta %*% psi / (conf.q * conf.s2)
    return(conf.f_stat)
  } -> numeric) %>%
  unlist
hypot.pvals <- hypot.f_stats %>% lapply(function(q = ? numeric) 1 - pf(q = q, df1 = conf.q, df2 = dd.n - dd.r))

lm.square <- lm(y~x+x2, data=dd)
lm.first <- lm(y~x, data=dd)
lm.zero <- lm(y~1, data=dd)

anova(lm.first, lm.square)
cat("--------\n")
anova(lm.zero, lm.square)

```
## (f) AIC, BIC

```{r}
order0.linregr <- lm(dd$y ~ 1)
order1.linreg <- lm(dd$y ~ dd$x)

regrs <- list(order0.linregr, order1.linreg, order2.linregr)
lapply(regrs, summary)
lapply(regrs, AIC)
lapply(regrs, BIC)
```
