\documentclass{article}

\usepackage{amsmath}
\usepackage{amssymb}

\usepackage{hyperref}
\hypersetup{pdfstartview=FitH,  linkcolor=blue, urlcolor=blue, colorlinks=true}

\usepackage[left=20mm, top=20mm, right=20mm, bottom=20mm, footskip=10mm]{geometry}
\usepackage{caption}
\usepackage{subcaption} % captions for subfigures
\usepackage{graphicx}
\usepackage{float} % force pictures position with \begin{figure}[H]
\newcommand{\myPictWidth}{.75\textwidth}

\begin{document}

\title{Statistics in bioinformatics. Lab 2}
\author{Valerii Zuev}
\maketitle

\begin{center}
Variant 4
\end{center}

\tableofcontents

\setcounter{section}{-1}
\section{Setup}

<<cache=F>>=

library(tidyverse) # dplyr, ggplot2, purrr, ...

x <- c(-4.503, -2.342, +0.437, +0.473, +1.531, -1.864, -2.283, -1.011,
       -2.743, -3.224, -3.284, +0.561, -0.626, -1.950, -0.825, +2.213,
       -2.503, -2.371, +0.278, +0.361, +0.653, -1.057, -1.404, -1.059,
       +1.673, -0.429, -2.543, +0.758, -1.195, -3.485, -0.863, -1.398,
       -4.310, +3.068, -3.704, -2.858, -4.464, -3.021, -1.593, +2.710,
       -3.191, +2.498, -0.024, +1.903, +1.244, -0.048, +1.354, -3.339,
       +1.330, -3.136)

param.alpha2 <- .01
param.c <- -1.4
param.d <- 1.
param.h <- .8
param.alpha0 <- -7.
param.sigma0 <- 2.
param.alpha1 <- -1.
param.sigma1 <- 2.

@

\section{Task 1}

\subsection{Variation series}

<<cache=T>>=

data <- as.data.frame(x) %>%
  mutate(id = row_number()) %>%
  arrange(x) %>%
  mutate(rank = row_number())

data %>% ggplot(aes(x = rank, ymin = 0, ymax = x)) +
  geom_linerange() +
  ggtitle(label = "Variational series") +
  theme(plot.title = element_text(hjust = .5))

@

\subsection{ECDF}

<<cache=T>>=
data %>% ggplot(aes(x)) + stat_ecdf()
@

\subsection{Histogram and the "frequency polygon"}

<<cache=T>>=
data.hist <- data %>% ggplot(aes(x)) + geom_histogram(binwidth = param.h)
data.hist.values <- ggplot_build(data.hist)$data[[1]] %>% 
  mutate(x = .5 * (xmin + xmax))
data.hist + geom_line(data = data.hist.values, aes(x, y), colour = "red")
@

\section{Task 2. Sample characteristics}

<<cache=T>>=
x.mean <- mean(x)
x.s2 <- mean(x^2) - x.mean^2
x.med <- median(x)
x.asym <- mean((x - x.mean)^3) / (x.s2^(3 / 2))
x.kurt <- mean((x - x.mean)^4) / (x.s2^2)

x.n <- length(x)
x.p_cd <- sum(param.c <= x & x <= param.d) / x.n

c("mean" = x.mean, "s2" = x.s2, "median" = x.med, 
  "asymmetry" = x.asym, "kurtosis" = x.kurt, "p(c<=x<=d)" = x.p_cd) %>% print
@

\section{Task 3. MLE and Method of Moments estimates for the Gaussian distribution parameters}

\subsection{Estimates}

\[ ] \vec x = \{ x_i \}_{i=1}^n: x_i \sim \mathcal N(a, \sigma^2) \]
The task is to find estimates $ \hat \sigma^2_{MLE}, \hat a $ via Maximum Likelihood Estimation and $ \hat \sigma^2_{MM}, \hat a_{MM} $
via Method of Moments.

\begin{enumerate}
 \item MLE:
 \begin{gather*}
  \ln L(\theta|\vec x) \to \max_{\theta}, \quad \theta = \begin{pmatrix} a \\ \sigma \end{pmatrix} \\
  L(\theta | \vec x) = (2 \pi \sigma)^{- \frac{n}{2}} \exp \left( - \sum_i \frac{(x_i - a)^2}{2 \sigma^2} \right)
 \end{gather*}
 
 Taking the partial derivatives, we get
 \[ \hat a_{MLE} = \bar x, \quad \hat \sigma^2_{MLE} = \frac{\sum_{i=1}^n (x_i - \bar x)^2}{n} \]
 
 \item Method of moments:
 
 In this method, the population moments are estimated by empirical moments.
 Provided that $ a $ is the mean and $ \sigma^2 $ is the variance in case of normal distribution, we immediately derive that a Method of Moments estimate is exactly the same as the Maximum Likelihood estimate.
 
\end{enumerate}

So, under assumption of normality MLE for mean is the sample mean and MLE for variance is the sample variance; the method of moments estimator for mean/variance is also the sample mean / sample variance

<<cache=T>>=
x.mean.mle <- x.mean
x.s2.mle <- x.s2

x.mean.moments <- x.mean
x.s2.moments <- x.s2
@

\subsection{Bias}

The estimate for mean is unbiased.
The unbiased estimate for variance is $ sd^2 = \frac{\sum_i   (x_i - \bar x)^2}{n - 1} $, and the bias is the difference.

<<cache=T>>=
cat(abs(x.s2 - var(x)))
@

\section{Task 4. Confidence intervals for distribution parameters}

As shown in theory, $ \hat a = \bar x \sim t(n-1) $, $ \frac{\hat \sigma^2}{a^2} \sim \chi^2(n) $.
Thus we may calculate confidence intervals as follows: 

<<cache=T>>=

t.quantile <- qt((1 - param.alpha2) / 2, x.n - 1)
x.mean.ci <- x.mean + c(lo = -t.quantile, 
                        hi = t.quantile) * sqrt(x.s2 / (x.n - 1))

chisq.quantile <- qchisq(p = (1 - param.alpha2) / 2, df = x.n)
x.s2.ci <- x.s2 * (1 + c(lo = -1 / chisq.quantile,
                         hi = 1 / chisq.quantile)) # Ivchenko, Medvedev p. 85 (42/123)

data.frame(stat = c("mean", "s^2"),
           lo = c(x.mean.ci["lo"], x.s2.ci["lo"]),
           hi = c(x.mean.ci["hi"], x.s2.ci["hi"])) %>% print

@

\section{Task 5. Kolmogorov test}

Kolmogorov theorem (Ivchenko, Medvedev p. 107-108 (53/123) and p. 15 (7/123)):

\begin{gather*}
] D_n := \max_{i} \left| F_n(x_i) - F(x_i) \right| \Rightarrow \\
\lim_{n \to \infty} P(\sqrt n D_n \le t) = K(t)
\end{gather*}

where $ F_n(x) $ is the empirical CDF and $ F(x) $ is the theoretical CDF; $ K(t) $ is the CDF for the Kolmogorov distribution:
\[ K(t) = \sum_{k=-\infty}^\infty (-1)^k \exp(- 2 k^2 t^2) \]

<<cache=T>>=
data$cdfEmpirical <- data$rank / nrow(data)

x.cdf.expected <- function(x) pnorm(x, param.alpha0, param.sigma0)
data$cdfTheor <- x.cdf.expected(data$x)
custom.d <- max(abs(data$cdfEmpirical - data$cdfTheor))
@
for $ \alpha_2 = 0.01 $ Kolmogorov distribution quantile $ q_{1 - \alpha} = 1.6276 $
<<cache=F>>=
cat(custom.d * sqrt(x.n))
@

The maximum alpha with which we cannot reject the null hypothesis is the p-value.

<<cache=F>>=
kolm.cdf <- function(x) {
  k_range <- -30:30 # an approximation; the precise range is -Inf:Inf
  ll <- k_range %>% 
    lapply(function(k = ? numeric){ 
      (-1)^k * exp(-2 * (k^2 * x^2)) 
      } -> numeric)
  do.call(rbind, ll) %>% colSums
}

cat("p-value:", 1 - kolm.cdf(custom.d * sqrt(x.n)))
@

We should always reject the null hypothesis ($ H_0 : $ the sample comes from the distribution with CDF $ F(x) $) because the P-value is almost zero.

Checking with the built-in Kolmogorov test:

<<cache=T>>=
ks.test(x = x, y = x.cdf.expected)
@

\section{Task 6. Chi squared test (simple hypothesis)}

It can be shown (Ivchenko, Medvedev, p. 109 (54/123)) that
\[ X_n^2 := \sum_{j=1}^n \frac{(\nu_j - np_j)^2}{np_j} \sim \chi^2(N-1) \]

where $ (\nu_1, ..., \nu_N) $ are the counts for data $ \vec x $ and bins $ 1 $ to $ N $, and $ p_j $ are the probabilities for these bins in case the hypothesis is true.

Thus we may calculate the statistics $ X_n^2 $ and the corresponding p-value:

<<cache=T>>=
brk <- c(-Inf, -2, 0, 2, 4, 6, Inf)
my.chisq.test <- function(x = ? numeric, cdf.expected){
  x.hist <- hist(x, plot = F, breaks = brk)
  nu <- x.hist$counts
  pr <- cdf.expected(tail(brk, -1)) - cdf.expected(head(brk, -1))
  expected_counts <- length(x) * pr
  return(sum((nu - expected_counts)^2 / expected_counts))
} -> numeric

x.chi2.simple <- my.chisq.test(x, cdf.expected = x.cdf.expected)
cat("p-value: ", 1 - pchisq(x.chi2.simple, df = length(brk) - 1))
@

We reject the null hypothesis because $ p-value = 1 - F_{\chi^2}(stat\_value) \approx 0 $

\section{Task 7. Chi squared test for complex hypothesis}

The complex hypothesis $ x_i \sim \mathcal N(a, \sigma), \quad a, \sigma \in  (-\infty; \infty) $ may be tested using the empirical estimates for $ a $ and $ \sigma $:

<<cache=F>>=
my.chisq.test.complex <- function (x = ? numeric){
  x.mean <- mean(x)
  x.sd <- sqrt(mean(x^2) - x.mean^2)
  return(my.chisq.test(x = x, cdf.expected = function(x) pnorm(x, x.mean, x.sd)))
}

x.chi2.complex <- my.chisq.test.complex(x)

cat("p-value:", 1 - pchisq(x.chi2.complex, df = length(brk) - 1))
@

$ \alpha_2 = 0.01 \Rightarrow $ we do not reject the null hypothesis

\section{Task 8. The most powerful test for simple hypothesis}

We test the hypothesis
\[ H_0 : x_i \sim \mathcal N (a_0, \sigma^2) \]
against the alternative
\[ H_1 : x_i \sim N(a_1, \sigma^2) \]

According to Neyman-Pearson lemma, the most powerful test is the likelihood ratio test with the rejection region of the null hypothesis

\[ \vec x : W = \frac{L(\theta_0 | \vec x)}{L(\theta_1 | \vec x)} \le C_\alpha, \quad C_\alpha : P(W \le C_\alpha) = \alpha \]

The log-likelihood ratio

\[ \ln W = \ln \frac{L_1}{L_0}= \frac{\sum_{i=1}^n (x_i -a_1)^2 }{2 \sigma^2} = \frac{\sum_{i=1}^n (x_i - a_0)^2 }{2 \sigma^2} = \frac{(a_1 - a_0) \sum_i x_i}{2 \sigma^2} \]

Under the null hypothesis, $ \sum_i x_i \sim \mathcal N(na_0, n\sigma^2) \Rightarrow $ we may introduce $ \tilde C_\alpha$ -- $1 - \alpha$-quantile of this distribution, $ \sum x_i \gtrless \tilde C_\alpha $.

First, we should plot the empirical PDF and two theoretical PDFs:

<<cache=F>>=
data %>% ggplot(aes(x)) +
  geom_histogram(aes(y = ..density..), bins = 6, colour = "black", fill = "white") +
  stat_function(fun = dnorm, args = list(mean = param.alpha0, sd = param.sigma0), colour = "red") +
  stat_function(fun = dnorm, args = list(mean = param.alpha1, sd = param.sigma0), colour = "blue") +
  theme_minimal()
@

Obviously we should choose the distribution with the higher meah which is $ H_1 $.


<<cache=F>>=
c_alpha2 <- qnorm(mean=param.alpha0 * x.n, sd=param.sigma0 * x.n, p = 1 - param.alpha2)
cat("Accepting H_0: ", mean(x) < c_alpha2, "\n")
cat("power:", 1 - pnorm(q=c_alpha2, mean = param.alpha1 * x.n, sd = param.sigma0 * x.n))
@

The test suggests that we prefer $ H_0 $ over $ H_1 $ as expected. \\

What happens if we swap $ H_0 $ with $ H_1 $?
Now the rejection region of the null hypothesis is in the lower tail of the distribution $\mathcal N (\alpha_1, \sigma_0)$.

<<>>=
c_alpha2a <- qnorm(mean = param.alpha1 * x.n, sd = param.sigma0 * x.n, p = param.alpha2)
cat("Accepting hypothesis with mean=alpha_1: ", mean(x) > c_alpha2a, "\n")
cat("power:", 1 - pnorm(q = c_alpha2a, mean = param.alpha0 * x.n, sd = param.sigma0 * x.n))
@
\end{document}
