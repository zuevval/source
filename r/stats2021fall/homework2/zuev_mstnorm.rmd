## Setting up the parameters

```{r message = F, warning = F}
library(tidyverse) # dplyr, ggplot2, purrr, ...

x <- c(-4.503, -2.342, +0.437, +0.473, +1.531, -1.864, -2.283, -1.011,
       -2.743, -3.224, -3.284, +0.561, -0.626, -1.950, -0.825, +2.213,
       -2.503, -2.371, +0.278, +0.361, +0.653, -1.057, -1.404, -1.059,
       +1.673, -0.429, -2.543, +0.758, -1.195, -3.485, -0.863, -1.398,
       -4.310, +3.068, -3.704, -2.858, -4.464, -3.021, -1.593, +2.710,
       -3.191, +2.498, -0.024, +1.903, +1.244, -0.048, +1.354, -3.339,
       +1.330, -3.136)

param.alpha2 <- .01
param.c <- -1.4
param.d <- 1.
param.h <- .8
param.alpha0 <- -7.
param.sigma0 <- 2.
param.alpha1 <- -1.
param.sigma1 <- 2.
```
## Task 1

### Variational series

```{r}
data <- as.data.frame(x) %>%
  mutate(id = row_number()) %>%
  arrange(x) %>%
  mutate(rank = row_number())

data %>% ggplot(aes(x = x, y = 0)) +
  geom_point() +
  ggtitle(label = "Variational series") +
  theme_minimal() +
  theme(plot.title = element_text(hjust = .5),
        axis.title.y = element_blank(),
        axis.text.y = element_blank(),
        axis.ticks.y = element_blank())

```

### ECDF

```{r}
data %>% ggplot(aes(x)) +
  stat_ecdf() +
  theme_minimal()
```

### Histogram and the "frequency polygon"

```{r}
data.hist <- data %>% ggplot(aes(x)) + geom_histogram(binwidth = param.h)
data.hist.values <- ggplot_build(data.hist)$data[[1]] %>% mutate(x = .5 * (xmin + xmax))
data.hist +
  geom_line(data = data.hist.values, aes(x, y), colour = "red") +
  theme_minimal()
```
## Task 2. Sample characteristics
```{r}
x.mean <- mean(x)
x.s2 <- mean(x^2) - x.mean^2
x.med <- median(x)
x.asym <- mean((x - x.mean)^3) / (x.s2^(3 / 2))
x.kurt <- mean((x - x.mean)^4) / (x.s2^2) - 3

x.n <- length(x)
x.p_cd <- sum(param.c <= x & x <= param.d) / x.n

c("mean" = x.mean, "s2" = x.s2, "median" = x.med, "asymmetry" = x.asym, "kurtosis" = x.kurt, "p(c<=x<=d)" = x.p_cd) %>% print
```
## Task 3. MLE and method of moments estimates for the Gauss distribution parameters
Under assumption of normality MLE for mean is the sample mean and MLE for variance is the sample variance.

The method of moments estimator for mean/variance is also the sample mean / sample variance
```{r}
x.mean.mle <- x.mean
x.s2.mle <- x.s2

x.mean.moments <- x.mean
x.s2.moments <- x.s2
```

## Task 4. Confidence intervals for distribution parameters

```{r}
t.quantile <- qt(1 - param.alpha2 / 2, x.n - 1)
x.mean.ci <- x.mean + c(lo = -t.quantile, hi = t.quantile) * sqrt(x.s2 / (x.n - 1))


x.s2.ci <- x.s2 *
  (x.n - 1) *
  (c( # Ivchenko, Medvedev p. 85 (42/123)
    lo = 1 / qchisq(p = 1 - param.alpha2 / 2, df = x.n - 1),
    hi = 1 / qchisq(p = param.alpha2 / 2, df = x.n - 1)))

data.frame(stat = c("mean", "s^2"),
           value = c(x.mean, x.s2),
           lo = c(x.mean.ci["lo"], x.s2.ci["lo"]),
           hi = c(x.mean.ci["hi"], x.s2.ci["hi"])) %>% print

```

## Task 5. Kolmogorov test

Using built-in Kolmogorov test:

```{r}
x.cdf.expected <- function(x) pnorm(x, param.alpha0, param.sigma0)
ks.test(x = x, y = x.cdf.expected)
```

Custom version: Ivchenko, Medvedev p. 107-108 (53/123)

```{r}
data$cdfEmpirical <- data$rank / nrow(data)
data$cdfTheor <- x.cdf.expected(data$x)
custom.d <- max(abs(data$cdfEmpirical - data$cdfTheor))

# for alpha2 = 0.01 Kolmogorov distribution quantile q_alpha = 1.6276
cat(custom.d * sqrt(x.n))
```
The maximum alpha with which we cannot reject the null hypothesis is the p-value.
```{r}
kolm.cdf <- function(x) {
  k_range <- -30:30 # an approximation; the precise range is -Inf:Inf
  ll <- k_range %>% lapply(function(k = ? numeric){ (-1)^k * exp(-2 * (k^2 * x^2)) } -> numeric)
  do.call(rbind, ll) %>% colSums
}

cat("p-value:", 1 - kolm.cdf(custom.d * sqrt(x.n)))

```

## Task 6. Chi squared test (simple hypothesis)

```{r}
brk <- c(-Inf, -2, 0, 2, 4, 6, Inf)
my.chisq.test <- function(x = ? numeric, cdf.expected){
  x.hist <- hist(x, plot = F, breaks = brk)
  nu <- x.hist$counts
  pr <- cdf.expected(tail(brk, -1)) - cdf.expected(head(brk, -1))
  expected_counts <- length(x) * pr
  return(sum((nu - expected_counts)^2 / expected_counts))
} -> numeric

x.chi2.simple <- my.chisq.test(x, cdf.expected = x.cdf.expected)
cat("p-value: ", 1 - pchisq(x.chi2.simple, df = length(brk) - 1))

```
We reject the null hypothesis because $ p-value = 1 - F_{\chi^2}(stat\_value) \approx 0 $

# Task 7. Chi-squared test for complex alternative
```{r}
my.chisq.test.complex <- function(x = ? numeric) {
  x.mean <- mean(x)
  x.sd <- mean(x^2) - x.mean^2
  return(my.chisq.test(x = x, cdf.expected = function(x) pnorm(x, x.mean, x.sd)))
}

x.chi2.complex <- my.chisq.test.complex(x)

cat("p-value:", 1 - pchisq(x.chi2.complex, df = length(brk) - 1))
```
$ \alpha_2 = 0.01 \Rightarrow $ we do not reject the null hypothesis

## Task 8

We test the hypothesis
\[ H_0 : x_i \sim \mathcal N (a_0, \sigma^2) \]
against the alternative
\[ H_1 : x_i \sim N(a_1, \sigma^2) \]

According to Neyman-Pearson lemma, the most powerful test is the likelihood ratio test with the rejection region of the null hypothesis

\[ \vec x : W = \frac{L(\theta_0 | \vec x)}{L(\theta_1 | \vec x)} \le C_\alpha, \quad C_\alpha : P(W \le C_\alpha) = \alpha \]

The log-likelihood ratio

\[ \ln W = \ln \frac{L_1}{L_0}= \frac{\sum_{i=1}^n (x_i -a_1)^2 }{2 \sigma^2} = \frac{\sum_{i=1}^n (x_i - a_0)^2 }{2 \sigma^2} = \frac{(a_1 - a_0) \sum_i x_i}{2 \sigma^2} \]

Under the null hypothesis, $ \sum_i x_i \sim \mathcal N(na_0, n\sigma^2) \Rightarrow $ we may introduce $ \tilde C_\alpha := q_{1 - \alpha}$ -- a quantile of this distribution, and the test takes the form
\[ \sum_i x_i \gtrless \tilde C_\alpha \]

First, we will plot the empirical PDF and two theoretical PDFs:

```{r}
data %>% ggplot(aes(x)) +
  geom_histogram(aes(y = ..density..), bins = 6, colour = "black", fill = "white") +
  stat_function(fun = dnorm, args = list(mean = param.alpha0, sd = param.sigma0), colour = "red") +
  stat_function(fun = dnorm, args = list(mean = param.alpha1, sd = param.sigma0), colour = "blue") +
  theme_minimal()
```

Even without the test it is evident that we should choose the distribution with the higher mean which is $ H_1 $.

Next, we calculate $\tilde C_\alpha$ and perform the test.
$\alpha_0 < \alpha_1 \Rightarrow$ when $ \sum_i x_i \le \tilde C_\alpha $, we resort to the null hypothesis, otherwise -- alternative.

```{r}
c_alpha2 <- qnorm(mean = param.alpha0 * x.n, sd = param.sigma0 * x.n, p = 1 - param.alpha2)
cat("Accepting H_0: ", sum(x) < c_alpha2, "\n")
cat("power:", 1 - pnorm(q = c_alpha2, mean = param.alpha1 * x.n, sd = param.sigma0 * x.n))
```
As expected, the test suggests that we prefer $ H_1 $ over $ H_0 $. \\

What happens if we swap $ H_0 $ with $ H_1 $?
Now the rejection region of the null hypothesis is in the lower tail of the distribution $\mathcal N (\alpha_1, \sigma_0)$.
```{r}
c_alpha2a <- qnorm(mean = param.alpha1 * x.n, sd = param.sigma0 * x.n, p = param.alpha2)
cat("Accepting hypothesis with mean=alpha_1: ", mean(x) > c_alpha2a, "\n")
cat("power:", pnorm(q = c_alpha2a, mean = param.alpha0 * x.n, sd = param.sigma0 * x.n))
```
