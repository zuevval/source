\documentclass[main.tex]{subfiles}
\begin{document}

\section{Дима. Методы настройки гиперпараметров SVM}

Интуиция: строим много-много машин и выбираем ту, которая даёт лучшее качество.

Рассмотрим задачу на стандартном примере: гауссово ядро $ \Rightarrow $ два параметра: параметр ядра и параметр регуляризации.

N-fold cross-validation.
Задача (в целом) -- уменьшить количество вычислений.

\begin{enumerate}[noitemsep]
	\item Grid Search -- перебор по сетке. Прост и даёт хорошую точность
	\item Эмпирические формулы
	\item Эвристические и генетические алгоритмы поиска (симуляция отжига, рой частиц и так далее)
	\item Linear search (линейный поиск)
	\item Grid-quadtree
\end{enumerate}

В статье по Grid Search оценивают LOO-оценку.
Параметры...

\subsection{Grid-Quadtree}

Алгоритм раньше использовался для отделения объектов на изображении.
Рекурсия: делим изображение на квадранты, берём начальный.
Если фигура целиком попадает в него, называем квадрант <<чёрным>> и останавливаем поиск.
Если фигура целиком лежит вне квадранта, называем узел <<белым>> и тоже прекращаем углубляться.
Иначе делим квадрант на четыре маленьких квадранта и работаем дальше.

Хотят найти не просто точку, а область.

Критерий остановки: задают некий минимальный размер ячейки

\section{Отбор признаков с SVM-RFE}

Пусть специалист-биолог прислал нам данные; пусть там, возможно, есть шумы.
Нужно отобрать значимые признаки (помогает уменьшать время, избегать переобучения).

Методы обёртывания, методы фильтров, встроенные методы (используют под капотом конкретный классификатор) и др.

RFE (recursive feature extraction (?), рекурсивное удаление признаков): жадный алгоритм, последовательно удаляет наименее значимый признак.

Достоинства и недостатки:
Метод исключения лучше отслеживает взаимосвязи между признаками, но гораздо дороже вычислительно

На изображении: кросс-валидация сверху по двум, снизу по десяти

\[ \Delta J(i) = \frac{1}{2} \alpha^T H \alpha - \frac{1}{2} \alpha^T H(-i) \alpha \]

(целевая функция $ J $ -- из двойственной задачи, $ \alpha $ -- двойственные переменные; прямая задача, напомним, -- Generalization Error, ожидаемый риск)
Ожидаемый риск: сперва есть функция потерь, которую интегрируем по...

\subsection{Приложения}

В биоинформатике (отбор генов, например) линейный SVM, оказывается, часто показывает себя лучше нелинейных.
Сам метод разработан для решения задачи селекции генов.

\section{Воспоминания об SVM}

Ожидаемый риск:
\[ R[f] = \int_{X \times Y} L(f(x), y) dP(x,y) \]
Поскольку распределение неизвестно, используем \emph{структурную минимизацию риска}: существует верхняя граница риска
\[ R_{emp}[f] = \frac{1}{l} \sum_{i=1}^l L(f(x_i), y_i) \]


Это общая задача обучения с учителем.

Построение линейного классификатора для линейно разделимых данных -- максимизация ширины полосы.

\begin{leftbar}
	Любые возникшие вопросы можно задавать по учебной почте, а также прямо перед зачётом.
\end{leftbar}

Максимизация ширины полосы $ \frac{2}{|| w ||} $ приводит нас к условию 
\[ ||w|| \to min, \quad \text{(TODO ограничения)} \]

... прямая задача
... условия Кароша-Куна-Таккера $ \Rightarrow $ двойственная задача.

Машина опорных векторов (линейная)

\[ \boxed{ f(x, \alpha) = \sum_{i=1}^l \alpha_i y_i + b } \]

Ключевое свойство машины опорных векторов: разреженность по $ \alpha $ (ненулевых $ \alpha_i $, соответствующих опорным векторам, меньше, чем нулевых).

\subsection{Линейно неразделимые данные}

Условия Лагранжа в форме Б<...>

Опорные векторы; связанные векторы

\subsection{Ядерный приём (Kernel trick)}

$ < \Phi(x), \Phi(x') > $

Теория Мерсера

Теория Зайна

Если функция двух аргументов обладает некоторыми свойствами, то она является ядром (существует соответствующее пространство и функция $ \Phi(x) $).

Формально заменяем все скалярные произведения на значения ядра, получаем нелинейную в общем случае разделяющую поверхность.

\[ \max_\alpha \left( \sum TODO + \sum TODO \right) \]

Фактически неявно применяем тот же линейный алгоритм, но в другом пространстве.

Теперь нам, впрочем, нужно настроить до обучения не только параметр регуляризации, но и параметр(ы) ядра. \\

... Потом мы взглянули на ядра поглубже и поняли, что многое зависит от представления данных.
В случае SVM всегда используется для представления данных матрица попарных сравнений.
Потом говорили про качество.

\subsection{Задача восстановления регрессии}

Если отклик -- любое вещественное число, то TODO

\end{document}