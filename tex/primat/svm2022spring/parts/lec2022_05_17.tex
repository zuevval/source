\documentclass[main.tex]{subfiles}
\begin{document}

\section{Дима. Методы настройки гиперпараметров SVM}

Интуиция: строим много-много машин и выбираем ту, которая даёт лучшее качество.

Рассмотрим задачу на стандартном примере: гауссово ядро $ \Rightarrow $ два параметра: параметр ядра и параметр регуляризации.

N-fold cross-validation.
Задача (в целом) -- уменьшить количество вычислений.

\begin{enumerate}[noitemsep]
	\item Grid Search -- перебор по сетке. Прост и даёт хорошую точность
	\item Эмпирические формулы
	\item Эвристические и генетические алгоритмы поиска (симуляция отжига, рой частиц и так далее)
	\item Linear search (линейный поиск)
	\item Grid-quadtree
\end{enumerate}

В статье по Grid Search оценивают LOO-оценку.
Параметры...

\subsection{Grid-Quadtree}

Алгоритм раньше использовался для отделения объектов на изображении.
Рекурсия: делим изображение на квадранты, берём начальный.
Если фигура целиком попадает в него, называем квадрант <<чёрным>> и останавливаем поиск.
Если фигура целиком лежит вне квадранта, называем узел <<белым>> и тоже прекращаем углубляться.
Иначе делим квадрант на четыре маленьких квадранта и работаем дальше.

Хотят найти не просто точку, а область.

Критерий остановки: задают некий минимальный размер ячейки

\section{Отбор признаков с SVM-RFE}

Пусть специалист-биолог прислал нам данные; пусть там, возможно, есть шумы.
Нужно отобрать значимые признаки (помогает уменьшать время, избегать переобучения).

Методы обёртывания, методы фильтров, встроенные методы (используют под капотом конкретный классификатор) и др.

RFE (recursive feature extraction (?), рекурсивное удаление признаков): жадный алгоритм, последовательно удаляет наименее значимый признак.

Достоинства и недостатки:
Метод исключения лучше отслеживает взаимосвязи между признаками, но гораздо дороже вычислительно

На изображении: кросс-валидация сверху по двум, снизу по десяти

\[ \Delta J(i) = \frac{1}{2} \alpha^T H \alpha - \frac{1}{2} \alpha^T H(-i) \alpha \]

(целевая функция $ J $ -- из двойственной задачи, $ \alpha $ -- двойственные переменные; прямая задача, напомним, -- Generalization Error, ожидаемый риск)
Ожидаемый риск: сперва есть функция потерь, которую интегрируем по...

\subsection{Приложения}

В биоинформатике (отбор генов, например) линейный SVM, оказывается, часто показывает себя лучше нелинейных.
Сам метод разработан для решения задачи селекции генов.

\end{document}