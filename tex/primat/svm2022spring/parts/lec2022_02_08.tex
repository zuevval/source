\documentclass[main.tex]{subfiles}
\begin{document}
\section{Лекция 1. Методы построения машин опорных векторов  }

8 февраля 2022 года.

\begin{leftbar}
	У нас будет одно -- у каждого индивидуальное -- задание.
	
	Книга Kernel Methods in Computational Biology, Bernard Sholkopf and others, the MIT Press, 2004 (Ядерные методы в вычислительной биологии) -- пожалуй, наиболее интересная для нас книжка.
	
	Книга Вапника: основные методы статистического обучения.
	
	Хороша (для нас) книга Мориса Foundations of machine learning (глава посвящена SVM).
	
	An Introduction to Statistical learning, 2015 -- здесь синтез SVM и ядерных методов.
\end{leftbar}


В первую очередь SVM мы будем использовать для задач классификации и регрессии (обучение с учителем).
Но в последнее время SVM активно применяется к задаче кластеризации.
Нас интересует задача статистического оценивания функций.

О \emph{больших данных} стали говорить в связи с тем, что они стали накапливаться в корпоративных хранилищах, лабораториях.
Дело не только в больших объёмах, но и в размерности.

Предобработка: стандартизация

Сопутствующие задачи: preprocessing, feature selection (отбор признаков, то есть процесс определения наиболее информативных из множества исходных), feature extraction (выделение признаков, то есть преобразование исходных).
В последнем случае обычно ставится задача понижения размерности.

\emph{Data Mining} -- интеллектуальный анализ данных. \\

Выделяют следующие типы закономерностей:

\begin{enumerate}[noitemsep]
	\item Классификация
	\item Регрессия
	\item Ранжирование (упорядочивание согласно некоторому критерию)
	\item Кластеризация
\end{enumerate}

Основное, что мы хотим решить с помощью наших методов -- <<проклятие размерности>> (проблему падение точности классических методов с ростом размерности задачи).

Об эффекте \emph{переобучения} говорят в случае, когда качество обученной функции на новых данных существенно хуже, чем на обучающих.

С одной стороны, ошибку обучения хочется сделать как можно меньше.
При этом очень важно сократить разрыв между ошибкой обучения и  \emph{ошибкой обобщения}.

\emph{Недообучение} -- тоже плохо.

\subsection{Почему SVM?}

Середина XX века, Вапник и Червонис реализовали идею: <<Из всех моделей, которые одинаково хорошо описывают явление, следует выбирать простейшую>> ($ \approx $ бритва Оккама).

SVM-методы практически не требуют дополнительной априорной информации (например, распределние шума или функциональный вид искомого решающего правила!).

Параметрический регрессионый анализ: $ y = f(x, \beta) + \xi $, где $ \beta $ -- параметр (не стохастический), $ \xi $ -- шум (вбирает в себя всю стохастичность).

SVM-методы могут работать с данными, не имеющими ясного векторного представления.
Это важно для биоинформатиков.

\subsection{Перспективные практические приложения}

\begin{enumerate}[noitemsep]
	\item Мониторинг
	\item Финансы, биохимия, фармакология
\end{enumerate}

\subsection{Задача обучения с учителем}

Задача обучения: выбор из множества функций одной, котрая наилучшим образом предсказывает ответ учителя.

Минимизация ожидаемого риска $ R $:

\[ R[f] = \int_{X \times Y} L(f(x), y) dP(x,y) \]

$ L $ -- Loss

Пусть $ f_0 $ -- искомая функция, которая в пространстве гипотез минимизирует функционал риска на всех возможных вариантах ответа.
$f_n$ -- 

\emph{Структурная минимизация риска} (предлагается статистической теорией обучения): если минимизация на основе функционала риска работает хорошо в асимптотическом случае, то структурная минимизация риска корректно работает с конечными выборками.

% TODO formula, describe "l"

Статистическая теория обучения доказывает, что при определённом условии верхняя граница ожидаемого риска, основанная на мере сложности функции:

\[ R[f] \le R_{emp}[f] + \Omega\left( \frac{h}{l}, \ln \frac{\eta}{l} \right) \]

где $ l $ -- число точек данных.
Верхняя граница, даваемая таким правилом, не зависит от распределения точек в выборке.

\textbf{Принцип структурной минимизации риска}: на заданном множестве...

Наша задача -- выбрать такое значение  $ h $ (этот параметр может быть равен, скажем, числу свободных параметров в модели), которое будет минимизировать ожидаемый риск.

На принцип структурной минимизации риска мы будем опираться при работе с  SVM.

\subsection{После перерыва}

Рассмотрим задачу бинарной классификации и функцию потерь вида <<число ошибок>> ($ L(f(x),y) = y \cdot H(x) + (1-y) \cdot H(1-x) $, $ H $ --  функция Хевисайда).

Последовательность из $ l $ точек может быть разделена на 2 класса $ 2^l $ способами.

% TODO SOME

\begin{leftbar}
	TODO some
\end{leftbar}

\subsection{Случай линейно разделимых данных}

Считаем, что наши образцы -- пары:
 $ (x_1, y_1), \dots, (x_n, y_n) \in \mathds R^n \times \{ 0, 1 \} $ -- тренировочная последовательность, причём данные линейно разделимы.
 
 Будем находить такое разделение, которое максимизирует зазор (margin), т. е. кратчайшее расстояние от разделяющей гиперплоскости до ближайшей к ней точке данных.
 
 Выберем масштаб для параметров разделяющей ГП такой, чтобы для векторов ТП (тренировочной последовательности) выполняось условие
 \[ \min_{1 \le i \le l} | (x, w) + b | = 1 \]
 
 Т. о. мы сужаем класс гиперплоскостей до класса \emph{канонических} гиперплоскостей.
 На множестве канонических гиперплоскостей мы выбираем... % TODO a bit
 
 Минимизируя 
 Т. о., мы приходим к задаче условной минимизации
 \[ \begin{cases}
 	\frac{1}{2} || w ||^2 \to \min_w \\
 	y_i \left( (w,x_i) + b_i \right) \ge 1, i \in \{ 1; l \} \text{ (условия классификации без ошибок)}
 \end{cases} \]

Составим функцию Лагранжа этой задачи.

\[ L_p (w, b, \alpha) = \frac{1}{2} || w ||^2 - \sum_{i=1}^{l} \alpha_i y_i \left( (w, x_i) + b + \sum \alpha_i \right) \]
где $ \alpha_i $ -- множители Лагранжа.

Можно решать прямую или эквивалентную ей двойственную задачу.

... условия Кароша-Куна-Таккера.
Двойственная задача -- задача квадратичного программирования с линейными ограничениями.

Тренировочные векторы, лежащие на оптимальных поддерживающих плоскостях, и называются \emph{опорными}.

Разреженность по $ \alpha $ 

Преодолеваем <<проклятие размерности>>.
С переобучением боремся так: класс потенциальных решающих правил уже выбран так, чтобы эффекта переобучения не было.

\begin{leftbar}
	Я советую (не требую, хотя это было бы неплохо) убедиться в том, что такое условие задачи даёт именно такую двойственную задачу
\end{leftbar}

\subsection{Случай линейно не разделимых данных}

Если данные не линейно разделимы (как правило, в случае, когда данные зашумлены), мы выберем такую каноническую гиперплоскость, которая разделяет точки настолько правильно, насколько это возможно.

В предыдущем разделе мы строили <<hard margin classifier>>, теперь построим <<soft margin classifier>>.

Введём неотрицательные \emph{рассабляющие переменные} $ \xi_i, i \in \{ 1;l \} $.
Теперь в процессе обучения минимизируем $ || w || $ и $ \xi_i $.

\[ \begin{cases}
	\min_{w, \xi} \frac{1}{2} || w ||^2 + C \sum_i \xi_i \\
	y_i((w, x_i) + b) \ge 1 - \xi_i
\end{cases} \]

$ C $ -- параметр регуляризации (подбирается до начала тренировки)

% TODO some

\emph{Связанные опорные векторы} -- те, что лежат внутри полосы.

Важно, что

\begin{enumerate}[noitemsep]
	\item Многие $ \alpha $  равны нулю
	\item В некотором смысле мы уже можем обобщать полученные результаты на случай нелинейной задачи.
	Следующий шаг -- разделение нелинейным правилом. 
\end{enumerate}

\end{document}