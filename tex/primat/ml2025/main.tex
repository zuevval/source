% !TeX spellcheck = en_US
% !TeX program = xelatex

\documentclass[a4paper,12pt]{article}
\usepackage[utf8]{inputenc} % russian, do not change
\usepackage[T2A, T1]{fontenc} % russian, do not change
\usepackage[english, russian]{babel} % russian, do not change

% fonts
\usepackage{fontspec} % different fonts
\setmainfont{Times New Roman}
\usepackage{setspace,amsmath}
\usepackage{amssymb} %common math symbols
\usepackage{dsfont}

\usepackage{enumitem}

\usepackage{hyperref}
\hypersetup{pdfstartview=FitH,  linkcolor=blue, urlcolor=blue, colorlinks=true}
\usepackage{graphicx}

\begin{document}

\title{Машинное обучение -- Трансформеры}
\maketitle
\tableofcontents
	
\section{Mar 12, 2025 (Wed) | Self-attention }

\subsection{A brief recap}

Мы сказали, что attention -- главный элемент современных систем.
Attention основан на рергессии Надарая-Уотсона.
Мне кажется, это более понятная

Есть два основных вида Attention: additive attention (фактически копируют RNN -- рекурсивные НН).
И главная -- Dot Product Attention, иначе называемый Scaled Dot Product Attention.
Скоринговая функция в этом attention'е -- softmax, то есть вероятности от произведения матриц.
Эти произведения матриц -- фактически произведения векторов в матричной форме.

\[ softmax \left( \frac{QK^T}{\sqrt d} \right) V \in \mathds R ^{n \times n}  \]
\[ V = X * W_V, \dots \]

Важно, что матрицы $ W_K, W_Q, W_V $ \emph{обучаемые}.

Говорят, что трансформер -- нейросеть, но фактически мы уже отходим от нейросетей.
Похоже на нейросеть вот в чём: мы вводим нелинейность, чтобы модель была более общей; а ещё -- обучаем методом обратного распространения ошибки.

И ещё мы рассмотрели важный элемент -- self-attention.
\[ \{ x_1, \dots, x_n \} \to \{ x_1^*, \dots, x_n^* \}, x_i \in \mathds R^d \]
Это когда мы пытаемся из последовательности старых элементов (векторов) каким-то образом получить новые.
И мы считаем attention между одной точкой и всей последовательностью.

\[ x, \{ x_1, \dots, x_n \} \]
\[ x^* = \sum_{i=1}^n \alpha (x, x_i) \cdot x_i \]

По сути это выпуклая комбинация всех точек последовательности.

После применения self-attention точки будут сближенны.
И это можно делать сколько угодно раз.
В первом трансформере ("Vanilla Transformer", Attention is all you need) предложили сделать self-attention 6 раз -- якобы 5 мало, 7 много.

Основная проблема: каждый вектор должен "поздороваться" с $ n-1 $ другими векторами контекста, поэтому сложность self-attention -- квадрат.

Self-attention как метод сглаживания (non-local means denoising)

Далее мы перешли к NLP, сказали, что есть RNN с вектором скрытых состоряний $ h_t $ в момент $ t $, который по сути является в некотором роде вектором внимания.
Он всё время обновляется.
На выходе имеем $ y $.
Фактически это внимание с аддитивной скоринговой функцией.

Проблема, как мы говорили, -- быстрое забывание.
Предложена LSTM, которая постаралась решить проблемы быстрого забывания, но, на мой взгляд, это не решило до конца проблему.

Ещё одна изначальная проблема метода: нужно как-то "развязать" выходную последовательность и входную, то есть сделать так, чтобы количество $ y $ и $ x $.
Вспомнили, что есть автокодеры -- encoder, decoder; т.о. сделали скрытый вектор (латентное представление) между encoder-частью и decoder-частью.
Это тоже была проблема: 
Но это тоже был прогресс, хоть и небольшой.

Прогресс пошёл, когда придумали attention layer.

Ещё мы рассмотрели многомерное внимание (multi-head attention), и сказали, что оно нужно, чтобы посмотреть на наши данные как бы с нескольких сторон.
Точнее, так: когда инициализируем веса, результат зависит от случайности.
Поэтому инициализируем несколько наборов, и потом результаты либо конкатенируются, либо усредняются.
Чтобы элементы были максимально различны, была предложена регуляризация.
Матрица должна быть ортогональна.

\[  || AA^T - I || ^2 \to \min  \]

\subsection{Self-Attention}

Есть входной набор токенов (например, Bank at the river).
Пусть для простоты один токен соответствует одному слову ($ t_i, i \in 1 \dots 4 $).
Для каждого токена $ t_i $ генерируем вектор $ v_i $, получаем набор векторов.

К полученной последовательности векторов применяем self-attention.
И наша задача -- чтобы вектор, соответствующий слову Bank, был близок в латентном пространстве к слову "берег"\hspace*{0pt}, а не к слову "банк".

Включается Vector Dot Product (это проще, чем считать взвешенную сумму).
Рассмотрим для примера вектор $ v_3 $.
Считаем его attention с остальными векторами.

\[
s_{31} = v_3 \cdot v_1 \quad % TODO multiline
s_{32} = v_3 \cdot v_2
\dots 
\]

А дальше нормализация: каждый из весов делим на сумму всех.
Вспоминаем Надарая-Уотсона.

\[ w_{3i} = \alpha (v_3, v_i) = \frac{K(v_3, v_i)}{\sum_{j=1}^4 K(v_3, v_j)}, \quad \sum_i w_{3i} = 1 \]
\[ y_3 = \sum_i w_{3i} v_i \]

Т.о. преобразовали исходную последовательность векторов в новую с учётом их внутреннего контекста.

А чего не хватает?
Не хватает обучения :)

Можно и не учить, в каком-то смысле пройдёт и так -- будет учитывать близость.
Но лучше обучить.
Добавляем веса к Values, Keys и Queries.

\[ v_i \Rightarrow v_i M_k,  \dots \]

$ M_k $ мы в предыдущем параграфе обозначали $ W_K $

Итак, делаем линейное преобразование $ v_i $, умножая на матрицы весов Keys, Queries, Values.
Потом Keys и Queries перемножаются (точнее, делается self-attention), нормализуем, ...

\subsection{Трансформер для машинного перевода}

На самом деле, архитектур трансформеров очень-очень много.
Vanilla Transformer был реализован для перевода, поэтому он брал архитектуру свёрточной нейросети с энкодером и декодером.
Это чисто авторегрессионная модель.

Важны не только слова, но и их последовательность.
Поэтому к каждому вектору добавляем некоторую информацию о порядке.
Просто конкатенируем.

позиционное кодирование (position coding): добавляем позиционные векторы $ p_i $ так: $ x_i \to x_i + p_i $

На выходе трансформера-кодировщика получаем фактически те же $ y_i $, но с positional encoding, с нескольким многократно применённым self-attention, с полносвязным слоем.
Что только не делают с нашими исходными словами в предложении, но несмотря на то, что кажется какой-то ерундой, это работает.

\hrule

Позиционное кодирование уже на следующей лекции.

\section{ Mar 19, 2025 (Wed) | Positional encoding }
\subsection{ A brief recap }

Самовнимание -- множество операций квадратичной сложности (эмбеддинг каждого токена аттендится с остальными).
Базой является регрессия Надарая-Уотсона.
Всё это умножается друг на друга, складывается; при этом каждый элемент умножается на матрицу.
Три матрицы: keys, queries, values (они обучаемы, и это самое главное).
Затем делается multi-head attention: несколько параллельно процессов self-attention.
Если $ m $ голов, то обучается $ 3 \cdot m $ различных матриц.

Архитектура трансформера условно состоит условно из двух элементов -- кодировщик и декодировщик.
Иногда декодировщик не нужен, но для перевода, генерации новых токенов (скажем, GPT и так далее) используются оба модуля.

Структура кодировщика:
позиционное кодирование (просто прибавляем вектор позиционного кодирования к исходному) $ \to $ multi-head attention $ \to $ нормировка (стандартизация): сводим какое-то распределение к стандартному нормальному (средне 0, дисперсия 1).
Дальше используется полносвязная сеть; зачем она нужна? -- вводится ещё нелинейность.
Дальше опять нормировка.

В некоторых ситуациях можно ускорить квадрат, если нам не нужно аттендить всё со всем.


\subsection{Позиционное кодирование}

Много идей было, это целое направление исследований.
Алгоритмов позиционного кодирования сейчас много, есть эффективные; мы рассмотрим классику.
Похоже на то, как в деревьях решений есть классический алгоритм (CART), а есть ещё куча алгоритмов, которые, возможно, в чём-то и эффективнее, но до сих пор используют классику -- CART.

<<Тупая>> идея: каждой позиции поставить в соответствие число в интервале [0;1].
Но плохо работает.
Критерии: должно быть уникальным; модель должна обобщаться на более длинные, притом расстояние между двумя временными шагами должно быть всегда одинаковым.

Пусть $ x_j \in \mathds R^d : xp_j = x + \hat p_j $

\[ \hat p_t^i = \begin{cases}
	\sin \left( w_k t \right), i = 2k \\
	\cos \left( w_k t \right), i = 2k + 1
\end{cases}, w_k = \frac{1}{TODO} \]

чтобы постоянно при увеличении k вектор $ w $ уменьшался и мы сжимались

Почему сложение, а не конкатенация?

\subsection{Декодер}

Авторегрессионная модель.

Всё то же: к \textbf{исходной последовательности токенов} применяем multi-head attention, ... и под конец используем векторы $ z $, полученные на выходе кодировщика -- пытаемся сделать attention с вектором $ h_t' $ (это называется cross-attention).

При помощи softmax получаем вероятности различных токенов.
Наиболее вероятный поступает на выход.

Эмбеддинг начала (<BOS> - begin of sentence) нужен для инициализации авторегрессии.
В конце выдаётся токен <EOS> -- end of sentence

Критерии обучения:
если перевод, то стандартная метрика -- максимальное правдоподобие (максимизируем сумму логарифмов правдоподобия).

Для машинных переводчиков  $ n $-грамы сейчас прошлое.
Более интересная вещь -- BERT: кодировщик без декодировщика.
Служит для обучения многих задач, и часто предобученный BERT служит для fine-tuning.
BERT -- в некотором смысле самообучающаяся модель: MLM -- masked language modeling.

\subsection{ViT = vision transformer}

Поскольку трансформеры оказались очень успешными для NLP, появилась идея: почему бы их не использовать в CV?
И оказалось, что трансформеры хороши, могут конкурировать с нейросетями, и даже не просто в анализе изображений, а в анализе мультимодальных данных.
Идея: почему бы не представить изображение в качестве последовательности токенов?
(на самом деле не только к картинкам так можно; у Андрея, например, была идея с деревьями).

Patch + Positional Encoding $ \to $ последовательность эмбеддингов патчей.
Добавляется ещё один символ -- CLS (классификационный символ), некоторый дополнительный эмбеддинг, который кодирует класс изображения.
Последовательность прогоняем через Transformer Encoder; всё стандартно, как в NLP.
На выходе трансформера у нас единственное декодирование -- эмбеддинг класса декодируем обратно в класс.

Что ещё интересного в ViT? (придумано позже): хорошо, пусть мы разбили на некоторые патчи, а можем на ещё более мелкие, и параллельно реализовать несколько ViT, которые одновременно обучаются end-to-end.
Немножко похоже на то, что было в нейросетях: скользящее окно и пулинг.

\subsection{Tabular data}

Изначальная идея -- Бабенко (Яндекс).
Потом за ними пошли кучи других трансформеров для табличных данных.

Идея -- сделать т.н. feature tokenizer.
Самая простая идея -- некоторая линейная функция от исходных признаков.
А где контекст?

Одна матрица для одного признака.
Категориальные -- обычно one-hot encoding (но после умножения на матрицу всё равно один вектор)

\begin{enumerate}[noitemsep]
	\item 
\end{enumerate}



\section{}
\subsection{}

\begin{enumerate}[noitemsep]
	\item 
\end{enumerate}
	
	
\end{document}