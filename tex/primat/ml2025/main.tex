% !TeX spellcheck = en_US
% !TeX program = xelatex

\documentclass[a4paper,12pt]{article}
\usepackage[utf8]{inputenc} % russian, do not change
\usepackage[T2A, T1]{fontenc} % russian, do not change
\usepackage[english, russian]{babel} % russian, do not change

% fonts
\usepackage{fontspec} % different fonts
\setmainfont{Times New Roman}
\usepackage{setspace,amsmath}
\usepackage{amssymb} %common math symbols
\usepackage{dsfont}

\usepackage{enumitem}

\usepackage{hyperref}
\hypersetup{pdfstartview=FitH,  linkcolor=blue, urlcolor=blue, colorlinks=true}
\usepackage{graphicx}

\begin{document}

\title{Машинное обучение -- Трансформеры}
\maketitle
\tableofcontents
	
\section{Mar 12, 2025 (Wed) | Self-attention }

\subsection{A brief recap}

Мы сказали, что attention -- главный элемент современных систем.
Attention основан на рергессии Надарая-Уотсона.
Мне кажется, это более понятная

Есть два основных вида Attention: additive attention (фактически копируют RNN -- рекурсивные НН).
И главная -- Dot Product Attention, иначе называемый Scaled Dot Product Attention.
Скоринговая функция в этом attention'е -- softmax, то есть вероятности от произведения матриц.
Эти произведения матриц -- фактически произведения векторов в матричной форме.

\[ softmax \left( \frac{QK^T}{\sqrt d} \right) V \in \mathds R ^{n \times n}  \]
\[ V = X * W_V, \dots \]

Важно, что матрицы $ W_K, W_Q, W_V $ \emph{обучаемые}.

Говорят, что трансформер -- нейросеть, но фактически мы уже отходим от нейросетей.
Похоже на нейросеть вот в чём: мы вводим нелинейность, чтобы модель была более общей; а ещё -- обучаем методом обратного распространения ошибки.

И ещё мы рассмотрели важный элемент -- self-attention.
\[ \{ x_1, \dots, x_n \} \to \{ x_1^*, \dots, x_n^* \}, x_i \in \mathds R^d \]
Это когда мы пытаемся из последовательности старых элементов (векторов) каким-то образом получить новые.
И мы считаем attention между одной точкой и всей последовательностью.

\[ x, \{ x_1, \dots, x_n \} \]
\[ x^* = \sum_{i=1}^n \alpha (x, x_i) \cdot x_i \]

По сути это выпуклая комбинация всех точек последовательности.

После применения self-attention точки будут сближенны.
И это можно делать сколько угодно раз.
В первом трансформере ("Vanilla Transformer", Attention is all you need) предложили сделать self-attention 6 раз -- якобы 5 мало, 7 много.

Основная проблема: каждый вектор должен "поздороваться" с $ n-1 $ другими векторами контекста, поэтому сложность self-attention -- квадрат.

Self-attention как метод сглаживания (non-local means denoising)

Далее мы перешли к NLP, сказали, что есть RNN с вектором скрытых состоряний $ h_t $ в момент $ t $, который по сути является в некотором роде вектором внимания.
Он всё время обновляется.
На выходе имеем $ y $.
Фактически это внимание с аддитивной скоринговой функцией.

Проблема, как мы говорили, -- быстрое забывание.
Предложена LSTM, которая постаралась решить проблемы быстрого забывания, но, на мой взгляд, это не решило до конца проблему.

Ещё одна изначальная проблема метода: нужно как-то "развязать" выходную последовательность и входную, то есть сделать так, чтобы количество $ y $ и $ x $.
Вспомнили, что есть автокодеры -- encoder, decoder; т.о. сделали скрытый вектор (латентное представление) между encoder-частью и decoder-частью.
Это тоже была проблема: 
Но это тоже был прогресс, хоть и небольшой.

Прогресс пошёл, когда придумали attention layer.

Ещё мы рассмотрели многомерное внимание (multi-head attention), и сказали, что оно нужно, чтобы посмотреть на наши данные как бы с нескольких сторон.
Точнее, так: когда инициализируем веса, результат зависит от случайности.
Поэтому инициализируем несколько наборов, и потом результаты либо конкатенируются, либо усредняются.
Чтобы элементы были максимально различны, была предложена регуляризация.
Матрица должна быть ортогональна.

\[  || AA^T - I || ^2 \to \min  \]

\subsection{Self-Attention}

Есть входной набор токенов (например, Bank at the river).
Пусть для простоты один токен соответствует одному слову ($ t_i, i \in 1 \dots 4 $).
Для каждого токена $ t_i $ генерируем вектор $ v_i $, получаем набор векторов.

К полученной последовательности векторов применяем self-attention.
И наша задача -- чтобы вектор, соответствующий слову Bank, был близок в латентном пространстве к слову "берег"\hspace*{0pt}, а не к слову "банк".

Включается Vector Dot Product (это проще, чем считать взвешенную сумму).
Рассмотрим для примера вектор $ v_3 $.
Считаем его attention с остальными векторами.

\[
s_{31} = v_3 \cdot v_1 \quad % TODO multiline
s_{32} = v_3 \cdot v_2
\dots 
\]

А дальше нормализация: каждый из весов делим на сумму всех.
Вспоминаем Надарая-Уотсона.

\[ w_{3i} = \alpha (v_3, v_i) = \frac{K(v_3, v_i)}{\sum_{j=1}^4 K(v_3, v_j)}, \quad \sum_i w_{3i} = 1 \]
\[ y_3 = \sum_i w_{3i} v_i \]

Т.о. преобразовали исходную последовательность векторов в новую с учётом их внутреннего контекста.

А чего не хватает?
Не хватает обучения :)

Можно и не учить, в каком-то смысле пройдёт и так -- будет учитывать близость.
Но лучше обучить.
Добавляем веса к Values, Keys и Queries.

\[ v_i \Rightarrow v_i M_k,  \dots \]

$ M_k $ мы в предыдущем параграфе обозначали $ W_K $

Итак, делаем линейное преобразование $ v_i $, умножая на матрицы весов Keys, Queries, Values.
Потом Keys и Queries перемножаются (точнее, делается self-attention), нормализуем, ...

\subsection{Трансформер для машинного перевода}

На самом деле, архитектур трансформеров очень-очень много.
Vanilla Transformer был реализован для перевода, поэтому он брал архитектуру свёрточной нейросети с энкодером и декодером.
Это чисто авторегрессионная модель.

Важны не только слова, но и их последовательность.
Поэтому к каждому вектору добавляем некоторую информацию о порядке.
Просто конкатенируем.

позиционное кодирование (position coding): добавляем позиционные векторы $ p_i $ так: $ x_i \to x_i + p_i $

На выходе трансформера-кодировщика получаем фактически те же $ y_i $, но с positional encoding, с нескольким многократно применённым self-attention, с полносвязным слоем.
Что только не делают с нашими исходными словами в предложении, но несмотря на то, что кажется какой-то ерундой, это работает.

\hrule

Позиционное кодирование уже на следующей лекции.

\section{}
\subsection{}

\begin{enumerate}[noitemsep]
	\item 
\end{enumerate}
	
	
\end{document}