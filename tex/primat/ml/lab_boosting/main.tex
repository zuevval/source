% compile with XeLaTeX or LuaLaTeX

\documentclass[a4paper,12pt]{article} %14pt - extarticle
\usepackage[utf8]{inputenc} % russian, do not change
\usepackage[T2A, T1]{fontenc} % russian, do not change
\usepackage[english, russian]{babel} % russian, do not change

% fonts
\usepackage{fontspec} % different fonts
\setmainfont{Times New Roman}
\usepackage{setspace,amsmath}
\usepackage{amssymb} %common math symbols
\usepackage{dsfont}

% utilities
\usepackage{subfiles}
\usepackage{hyperref}
\hypersetup{pdfstartview=FitH,  linkcolor=blue, urlcolor=blue, colorlinks=true}
\usepackage{graphicx}
\usepackage{caption}
\usepackage{subcaption} % captions for subfigures

% styling
\usepackage{indentfirst} % indent first line of section
\usepackage{float} % force pictures position
\floatstyle{plaintop} % force caption on top

% misc
\graphicspath{{./img/}}
\newcommand{\myPictWidth}{.95\textwidth}

\begin{document}
\subfile{titlepage}

\section{Бустинг (слабые классификаторы - деревья решений)}
\subsection{Задание}
Исследовать зависимость тестовой ошибки от количества деревьев в ансамбле для алгоритма AdaBoost.M1 на наборе данных Vehicle из пакета \texttt{mlbench} (поделить выборку на обучающие и тестовые данные в отношении 7:3). Построить график зависимости тестовой ошибки при числе деревьев, равном 1, 11, 21, ..., 301. Объяснить результаты.

\subsection{Теория}\label{section:boosting_theory}

\textit{Бустинг} -- алгоритм построения классификатора, принимающего решения по результатам голосования нескольких однотипных классификаторов (как правило, <<слабых>>, т. е. неспособных формировать сложные границы принятия решений). Слабые классификаторы $c_i$ внутри одного сильного классификатора различаются весами $\alpha_i$, которые их решениям присваиваются при голосовании, и весами выборки $\vec{w}_i$ (каждый слабый классификатор должен уметь работать со взвешенной выборкой).

Алгоритм вначале присваивает элементам выборки одинаковые веса, обучает первый слабый классификатор и вычисляет его вес $\alpha_0$: чем больше неверных ответов, тем вес выше. Если доля ошибочных решений более $0.5$, вес отрицательный. Затем в цикле веса пересчитываются -- примерам, на которых ошибка выше, присуждается больший вес; строится новый классификатор и вычисляется его вес $\alpha_i$. Можно показать, что этот процесс -- градиентный спуск в пространстве всевозможных функций, которые можно получить подобными комбинацями \cite{murphy}. Алгоритм направлен на минимизацию по $f$ функционала \eqref{eq:boosting_objective}:

\begin{equation} \label{eq:boosting_objective}
	\sum_{i=1}^{N} L(y_i, f(x_i))
\end{equation}
Здесь $N$ -- число элементов в тренировочной выборке $\{(x_i, y_i)\}$, $L(...)$ -- некоторый разумный функционал риска. В методе \texttt{AdaBoost} $L = \exp(-y_i f(x_i))$.

Отличительная особенность бустинга -- усточивость к переобучению: при увеличении числа итераций, как правило, ошибка на тестовой выборке не возрастает. Это результат использования весов, которые назначаются классификаторам при голосовании (аналог регуляризации для пространства функций). 

\subsection{Реализация}

С помощью библиотеки \texttt{adabag} алгоритм AdaBoost был применён к данным Vehicle несколько раз с разным числом итераций. На рис. \ref{fig:adaboost_trees} показаны резульаты. 

\begin{figure}[H]
	\centering \includegraphics[width=\myPictWidth]{adaboost_trees}
	\caption{График зависимости ошибки на тестовых и тренировочных данных для классификатора, построенного из деревьев решений алгоритмом AdaBoost.M1}
	\label{fig:adaboost_trees}
\end{figure}

Видно, что ошибка на тренировочных данных быстро достигает нуля.
Переобучения не происходит (ошибка на тестовой выборке в целом не возрастает с ростом числа итераций), как и ожидается из теории (см. раздел \ref{section:boosting_theory}).

\newpage
\section{Бэггинг}
\subsection{Задание}
Исследовать зависимость ошибки на тестовых данных от количества деревьев в ансамбле для метода bagging на наборе данных Glass из пакета \texttt{mlbench} (тестовая выборка -- 30\% данных). Построить график зависимости тестовой ошибки при числе деревьев в диапазоне от 1 до 301 с шагом 10. Дать интерпретацию результатам.


\subsection{Теория}
Бэггинг (англ. Bagging, сокращение от Bootstrap Aggregating) -- ещё один метод составления классификатора из композиции однотипных слабых. Его работа проста: из набора тренировочных данных случайным образом выбираются подмножества, на каждом из которых обучается слабый классификатор. Результатом классификации будет тот класс, который чаще всего встретится среди ответов.

Недостаток метода заключается в том, что случайно выбранные подмножества исходного набора обучающих данных могут оказаться сильно коррелированными. Как следствие, результаты слабых классификаторов также будут коррелировать, и ошибка будет сравнительно велика и на тренировочной, и на тестовой выборке.

\subsection{Реализация}

Рис. \ref{fig:bagging} демонстрирует результат обучения Bagging'а на данных из набора Glass.
Как и бустинг, бэггинг не приводит к переобучению даже при большом числе итераций, что интуитивно понятно: если в тестовом наборе точка расположена <<близко>> к области, где сосредоточены точки некоторого класса, вероятно, многие слабые классификаторы присвоят ей значение именно этого класса, и наоборот, когда точка <<далеко>>, большинство классификаторов дадут какой-то другой ответ.

\begin{figure}[H]
	\centering \includegraphics[width=\myPictWidth]{bagging}
	\caption{Bagging. Кривая ошибки на тестовых и тренировочных данных}
	\label{fig:bagging}
\end{figure}

\newpage
\section{Бустинг (слабые классификаторы - k ближайших соседей)}
\subsection{Задание}
Реализовать бустинг, используя в качестве слабого классификатора метод k ближайших соседей. Построить классификаторы для данных Vehicle и Glass; сравнить ошибку классификатора с ошибкой ещиничных деревьев, обученных на тех же данных.

\subsection{Теория}
Алгоритм бустинга (и, в частности, используемая здесь модификация \texttt{Ada\-Boost.M1}) описаны в секции \ref{section:boosting_theory}. В библиотеке \texttt{adabag} алгоритм использует в качестве базового классификатора деревья решений, которые можно считать <<слабыми>>. Метод k ближайших соседей может формировать сложные границы; чтобы его можно было считать слабым классификатором, необходимо брать большое значение $k$ (в данной работе использованы $k=50$ и $k=10$ при объёмах выборки 948 и 215 соответственно).

\subsection{Реализация}
При реализации неожиданно возникла проблема: после двух-трёх итераций вес каждого следующего классификатора вес $\alpha_i$ слабого классификатора падает почти до нуля. Это означает, что доля ошибочных решений классификатора всё время остаётся на уровне, приближённом к 0.5. В итоге ошибка на тестовых данных для набора Glass $\approx 0.46$, для Vehicle $\approx 0.33$ при ошибках одного случайного дерева $0.28$ и $0.26$ соответственно.

Наблюдаемый плохой результат может быть объяснён следующим образом. Обычно слабый классификатор ошибается на некоторой доле примеров, которая либо выше 0.5, либо ниже. Иллюстрация подобного случая для задачи 1 приведена на рис. \ref{fig:boosting_k2}. В таком случае вес классификатора, заметно отличающийся от нуля, и ошибки на данной итерации приводят к тому, что на следующей итерации веса набора данных существенно изменяются. Но если в какой-то момент слабый классификатор ошибётся примерно в половине случаев, то его вес станет почти равным нулю, веса данных почти не изменятся и на следующей итерации будет построен такой же классификатор (рис. \ref{fig:boosting_stuck}). Ситуация усугубляется тем, что метод k ближайших соседей детерминирован и выдаёт всегда одинаковый ответ при заданном наборе данных и значении k, в отличие от дерева решений.

\begin{figure}[H]
	\centering \includegraphics[width=\myPictWidth]{knn_k5}
	\caption{Boosting + kNN. Иллюстрация для тренировочного набора данных размерности 1 (Сверху вниз - тренировочные данные в пяти итерациях слабого классификатора; Каждый ряд показывает состояние слабого классификатора в соответствующий момент: размер точки пропорционален весу, форма - классу, цвет - ответу слабого классификатора. Подписан вес классификатора $w$ на каждой итерации)}
	\label{fig:boosting_k2}
\end{figure}

\begin{figure}[H]
	\centering \includegraphics[width=\myPictWidth]{knn_stuck}
	\caption{Boosting + kNN. Попадание в стационарную точку}
	\label{fig:boosting_stuck}
\end{figure}

\section{Заключение}
Алгоритмы композиции могут строить хорошие, устойчивые к переобучению классификаторы для табличных данных. Впрочем, обучение и вычисление результата обученного классификатора занимает много времени.

При попытке применить метод k ближайших соседей в качестве базового алгоритма для AdaBoost.M1 оказалось, что слабый классификатор почти сразу приходит к конфигурации, при которой доля ошибки близка к $0.5$, отчего веса данных в следующих итерациях практически перестают пересчитываться.

Исходный код \href{https://github.com/zuevval/source/tree/master/r/ml/boosting}{доступен в GitHub}.


\begin{thebibliography}{99}
	\bibitem{murphy} Murphy, Kevin P. Machine learning: a probabilistic perspective.: Kevin P. Murphy. -- Cambridge, Massachusetts: The MIT Press, 2012.
\end{thebibliography}	
	
\end{document}