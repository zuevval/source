% compile with XeLaTeX or LuaLaTeX

\documentclass[a4paper,12pt]{article} %14pt - extarticle
\usepackage[utf8]{inputenc} % russian, do not change
\usepackage[T2A, T1]{fontenc} % russian, do not change
\usepackage[english, russian]{babel} % russian, do not change

% fonts
\usepackage{fontspec} % different fonts
\setmainfont{Times New Roman}
\usepackage{setspace,amsmath}
\usepackage{amssymb} %common math symbols
\usepackage{dsfont}

% utilities
\usepackage{subfiles}
\usepackage{hyperref}
\hypersetup{pdfstartview=FitH,  linkcolor=blue, urlcolor=blue, colorlinks=true}
\usepackage{graphicx}
\usepackage{caption}
\usepackage{subcaption} % captions for subfigures
\usepackage{listings}
\usepackage{color}

\definecolor{dkgreen}{rgb}{0,0.6,0}
\definecolor{gray}{rgb}{0.5,0.5,0.5}
\definecolor{mauve}{rgb}{0.58,0,0.82}

\lstset{language=R,
	basicstyle=\small\ttfamily,
	stringstyle=\color{dkgreen},
	otherkeywords={0,1,2,3,4,5,6,7,8,9},
	morekeywords={TRUE,FALSE},
	deletekeywords={data,frame,length,as,character},
	keywordstyle=\color{blue},
	commentstyle=\color{dkgreen},
	tabsize=2,
	breaklines=true,
	columns=fullflexible
}

% styling
\usepackage{indentfirst} % indent first line of section
\usepackage{float} % force pictures position
\floatstyle{plaintop} % force caption on top

% misc
\graphicspath{{./img/}}
\newcommand{\myPictWidth}{.95\textwidth}

\begin{document}
\subfile{titlepage}

\section{Бустинг (слабые классификаторы - деревья решений)}
\subsection{Задание}
Исследовать зависимость тестовой ошибки от количества деревьев в ансамбле для алгоритма AdaBoost.M1 на наборе данных Vehicle из пакета \texttt{mlbench} (поделить выборку на обучающие и тестовые данные в отношении 7:3). Построить график зависимости тестовой ошибки при числе деревьев, равном 1, 11, 21, ..., 301. Объяснить результаты.

\subsection{Теория}\label{section:boosting_theory}

\textit{Бустинг} -- алгоритм построения классификатора, принимающего решения по результатам голосования нескольких однотипных классификаторов (как правило, <<слабых>>, т. е. неспособных формировать сложные границы принятия решений). Слабые классификаторы $c_i$ внутри одного сильного классификатора различаются весами $\alpha_i$, которые их решениям присваиваются при голосовании, и весами выборки $\vec{w}_i$ (каждый слабый классификатор должен уметь работать со взвешенной выборкой).

Алгоритм вначале присваивает элементам выборки одинаковые веса, обучает первый слабый классификатор и вычисляет его вес $\alpha_0$: чем больше неверных ответов, тем вес выше. Если доля ошибочных решений более $0.5$, вес отрицательный. Затем в цикле веса пересчитываются -- примерам, на которых ошибка выше, присуждается больший вес; строится новый классификатор и вычисляется его вес $\alpha_i$. Можно показать, что этот процесс -- градиентный спуск в пространстве всевозможных функций, которые можно получить подобными комбинацями \cite{murphy}. Алгоритм направлен на минимизацию по $f$ функционала \eqref{eq:boosting_objective}:

\begin{equation} \label{eq:boosting_objective}
	\sum_{i=1}^{N} L(y_i, f(x_i))
\end{equation}
Здесь $N$ -- число элементов в тренировочной выборке $\{(x_i, y_i)\}$, $L(...)$ -- некоторый разумный функционал риска. В методе \texttt{AdaBoost} $L = \exp(-y_i f(x_i))$.

Отличительная особенность бустинга -- усточивость к переобучению: при увеличении числа итераций, как правило, ошибка на тестовой выборке не возрастает. Это результат использования весов, которые назначаются классификаторам при голосовании (аналог регуляризации для пространства функций). 

\subsection{Реализация}

С помощью библиотеки \texttt{adabag} алгоритм AdaBoost был применён к данным Vehicle несколько раз с разным числом итераций. На рис. \ref{fig:adaboost_trees} показаны резульаты. 

\begin{figure}[H]
	\centering \includegraphics[width=\myPictWidth]{adaboost_trees}
	\caption{График зависимости ошибки на тестовых и тренировочных данных для классификатора, построенного из деревьев решений алгоритмом AdaBoost.M1}
	\label{fig:adaboost_trees}
\end{figure}

Видно, что ошибка на тренировочных данных быстро достигает нуля.
Переобучения не происходит (ошибка на тестовой выборке в целом не возрастает с ростом числа итераций), как и ожидается из теории (см. раздел \ref{section:boosting_theory}).

\newpage
\section{Бэггинг}
\subsection{Задание}
Исследовать зависимость ошибки на тестовых данных от количества деревьев в ансамбле для метода bagging на наборе данных Glass из пакета \texttt{mlbench} (тестовая выборка -- 30\% данных). Построить график зависимости тестовой ошибки при числе деревьев в диапазоне от 1 до 301 с шагом 10. Дать интерпретацию результатам.


\subsection{Теория}
Бэггинг (англ. Bagging, сокращение от Bootstrap Aggregating) -- ещё один метод составления классификатора из композиции однотипных слабых. Его работа проста: из набора тренировочных данных случайным образом выбираются подмножества, на каждом из которых обучается слабый классификатор. Результатом классификации будет тот класс, который чаще всего встретится среди ответов.

Недостаток метода заключается в том, что случайно выбранные подмножества исходного набора обучающих данных могут оказаться сильно коррелированными. Как следствие, результаты слабых классификаторов также будут коррелировать, и ошибка будет сравнительно велика и на тренировочной, и на тестовой выборке.

\subsection{Реализация}

Рис. \ref{fig:bagging} демонстрирует результат обучения Bagging'а на данных из набора Glass.
Как и бустинг, бэггинг не приводит к переобучению даже при большом числе итераций, что интуитивно понятно: если в тестовом наборе точка расположена <<близко>> к области, где сосредоточены точки некоторого класса, вероятно, многие слабые классификаторы присвоят ей значение именно этого класса, и наоборот, когда точка <<далеко>>, большинство классификаторов дадут какой-то другой ответ.

\begin{figure}[H]
	\centering \includegraphics[width=\myPictWidth]{bagging}
	\caption{Bagging. Кривая ошибки на тестовых и тренировочных данных}
	\label{fig:bagging}
\end{figure}

\newpage
\section{Бустинг (слабые классификаторы - k ближайших соседей)}
\subsection{Задание}
Реализовать бустинг, используя в качестве слабого классификатора метод k ближайших соседей. Построить классификаторы для данных Vehicle и Glass; сравнить ошибку классификатора с ошибкой ещиничных деревьев, обученных на тех же данных.

\subsection{Теория}
Алгоритм бустинга (и, в частности, используемая здесь модификация \texttt{Ada\-Boost.M1}) описаны в секции \ref{section:boosting_theory}. В библиотеке \texttt{adabag} алгоритм использует в качестве базового классификатора деревья решений, которые можно считать <<слабыми>>. Метод k ближайших соседей может формировать сложные границы; чтобы его можно было считать слабым классификатором, необходимо брать большое значение $k$ (в данной работе использованы $k=50$ и $k=10$ при объёмах выборки 948 и 215 соответственно).

\subsection{Реализация}
При реализации неожиданно возникла проблема: после двух-трёх итераций вес каждого следующего классификатора вес $\alpha_i$ слабого классификатора падает почти до нуля. Это означает, что доля ошибочных решений классификатора всё время остаётся на уровне, приближённом к 0.5. В итоге ошибка на тестовых данных для набора Glass $\approx 0.46$, для Vehicle $\approx 0.33$ при ошибках одного случайного дерева $0.28$ и $0.26$ соответственно.

Наблюдаемый плохой результат может быть объяснён следующим образом. Обычно слабый классификатор ошибается на некоторой доле примеров, которая либо выше 0.5, либо ниже. Иллюстрация подобного случая для задачи 1 приведена на рис. \ref{fig:boosting_k2}. В таком случае вес классификатора, заметно отличающийся от нуля, и ошибки на данной итерации приводят к тому, что на следующей итерации веса набора данных существенно изменяются. Но если в какой-то момент слабый классификатор ошибётся примерно в половине случаев, то его вес станет почти равным нулю, веса данных почти не изменятся и на следующей итерации будет построен такой же классификатор (рис. \ref{fig:boosting_stuck}). Ситуация усугубляется тем, что метод k ближайших соседей детерминирован и выдаёт всегда одинаковый ответ при заданном наборе данных и значении k, в отличие от дерева решений.

\begin{figure}[H]
	\centering \includegraphics[width=\myPictWidth]{knn_k5}
	\caption{Boosting + kNN. Иллюстрация для тренировочного набора данных размерности 1 (Сверху вниз - тренировочные данные в пяти итерациях слабого классификатора; Каждый ряд показывает состояние слабого классификатора в соответствующий момент: размер точки пропорционален весу, форма - классу, цвет - ответу слабого классификатора. Подписан вес классификатора $w$ на каждой итерации)}
	\label{fig:boosting_k2}
\end{figure}

\begin{figure}[H]
	\centering \includegraphics[width=\myPictWidth]{knn_stuck}
	\caption{Boosting + kNN. Попадание в стационарную точку}
	\label{fig:boosting_stuck}
\end{figure}

\newpage
\subsection{Код программы, реализующей бустинг с методом k ближайших соседей}
\begin{lstlisting}
	library(types) # type hints
	library(dplyr) # working with dataframes
	library(pryr) # for `partial` (~ functools.partial in Python)
	
	#' Weighted k nearest neighbors multiclass classifier
	#' @param x input point (a list of real numbers)
	#' @param train_data data.frame with train samples:
	#' train_data.x - input vectors (each is a list of the same length as `x`)
	#' train_data.y - class labels ( each is a factor)
	#' train_data.w - weights (each is a real number, typically between 0 and 1)
	#' @param k tuning parameter (number of nearest points to take into account)
	#' @returns A result of a weighted kNN classifier - a class label for `x`
	#' @examples
	#' knn_w(
	#'    x = c(0, 1),
	#'    train_data = data.frame(
	#'      x = I(list(c(-1, 1), c(1, 1))),
	#'      y = c(-1, 1),
	#'      w = c(.5, .5)),
	#'    k = 2
	#' )
	knn_w <- function(x = ? list, train_data = ? data.frame, k = ? integer) {
		train_data$remoteness <- train_data$x %>% # `remoteness` = eucledian distance from `x`
		lapply(function(x1 = ? list) dist(rbind(x1, x))[[1]]) %>%
		unlist
		closest_points <- train_data %>% slice_min(remoteness, n = k) # select `k` rows with lowest `remoteness`
		possible_classes <- unique(closest_points$y)
		classes_probabilities <- lapply(possible_classes, function(class) {
			closest_points %>%
			filter(y == class) %>%
			select(w) %>%
			sum # summarize all contributions of all points with appropriate class label
		})
		return(possible_classes[which.max(classes_probabilities)])
	}
	
	#' AdaBoost M1 for k nearest neighbors (multi-class classification)
	#' @param train_data data.frame with samples:
	#' train_data.x - each cell is a list (feature vector)
	#' train_data.y - corresponding class labels (factors)
	#' @param k tuning parameter for kNN (see reference for `knn_w`)
	#' @param m tuning parameter for AdaBoost (number of iterations)
	#' @returns function(x = ? list) - a trained classifier. `x` is a vector of the same length as each of `train_data.x`
	knn_adaboost <- function(train_data = ? data.frame, k = ? integer, m = ? integer, extended_return = F) {
		train_data$w <- 1 / nrow(train_data) # initialize weights
		classifiers <- list()
		classifiers_weights <- list()
		classifiers_params <- train_data
		err_threshold <- 1e-5
		for (i in 1:m) {
			weak_classifier <- partial(knn_w, train_data = train_data, k = k, .lazy = F)
			train_data$prediction <- lapply(seq_len(nrow(train_data)), function(i) weak_classifier(unlist(train_data$x[i]))) %>% unlist
			train_data <- train_data %>%
			mutate(contrib = (y != prediction) * w)
			err <- sum(train_data$contrib) / sum(train_data$w)
			alpha <- log((1 - err) / err)
			
			classifiers <- classifiers %>% append(weak_classifier)
			classifiers_weights <- classifiers_weights %>% append(alpha)
			train_data <- train_data %>% mutate(w = w * exp(alpha * (y != prediction))) # recalculating data weights
			train_data <- train_data %>% mutate(w = w / sum(w))
		}
		classifiers <- unlist(classifiers)
		classifiers_weights <- unlist(classifiers_weights)
		
		function(x = ? list) {
			classifiers_answers <- lapply(classifiers, function(classifier) classifier(x))
			classes <- unique(classifiers_answers)
			classes_weights <- lapply(classes, function(class) {
				classifiers_weights[classifiers_answers == class] %>% sum
			})
			return(classifiers_answers[[which.max(classes_weights)]])
		}
	}
\end{lstlisting}

\section{Заключение}
Алгоритмы композиции могут строить хорошие, устойчивые к переобучению классификаторы для табличных данных. Впрочем, обучение и вычисление результата обученного классификатора занимает много времени.

При попытке применить метод k ближайших соседей в качестве базового алгоритма для AdaBoost.M1 оказалось, что слабый классификатор почти сразу приходит к конфигурации, при которой доля ошибки близка к $0.5$, отчего веса данных в следующих итерациях практически перестают пересчитываться.

Исходный код \href{https://github.com/zuevval/source/tree/master/r/ml/boosting}{доступен в GitHub}.


\begin{thebibliography}{99}
	\bibitem{murphy} Murphy, Kevin P. Machine learning: a probabilistic perspective.: Kevin P. Murphy. -- Cambridge, Massachusetts: The MIT Press, 2012.
\end{thebibliography}	
	
\end{document}