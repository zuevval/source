\documentclass[main.tex]{subfiles}
\begin{document}

\section{Лекция 3. Дискриминантный анализ. Самостоятельная работа}

Матрица наблюдений $ X = \begin{pmatrix} x_1 \\ x_2 \\ \vdots \\ x_n \end{pmatrix}
= \begin{pmatrix}
x_{11} & \dots & x_{1p} \\
\vdots & \ddots & \vdots \\
x_{n1} & \dots & x_{np}
\end{pmatrix} $

$ x_l \in \mathcal{X} \in \mathds{R}^p, l \in \{1;p\} $

Задача -- разбить множество векторов на однородные группы (классы).

\subsection{Классификация при наличии обучающей выборки}

Пусть наблюдениям соответствуют классы $ D_i, i \in \{1;k\} $.
Тогда объекты каждого класса образуют в $ \mathds{R}^p $ <<облако>> с плотностью $ f_i(x) $ (в общем случае $ f_i(x) $ неизвестна).

Для качественной классификации нужно, чтобы элементы $ D_i $ были сконцентрированы в обособленной области $ R_i \in \mathds{R}^p $, где плотность остальных классов низка.
Тогда правило классификации: $ x \in R_i \Rightarrow x \to D_i$; $ P(i|j) = P\{x \in R_i | x \in D_j \} $ -- вероятность неправильного решения.


\subsubsection{Бинарная классификация}

\emph{(А) Случай, когда $f_i(x)$ известны:}

Пусть $ \{D_i\} = \{D_1, D_2\}, R_1 : P_1 \sim \mathcal{N}\left(\mu_1, \Sigma \right), P_2 \sim \mathcal{N}\left(\mu_1, \Sigma \right) $ (два нормальных распределения с одинаковыми ковариационными матрицами).

Строим \emph{дискриминантную функцию} $ z = \sum_{i=1}^{n} \alpha_i x^{(i)} $.
Классификация: $ z \glq c $

Величина $ z $ такова, что классы распределены по ней согласно двум нормальным законам: $ \mathcal{N}(\xi_1, \sigma_z), \mathcal{N}(\xi_2, \sigma_z) $.
Подбираем $ \alpha_i $ так, чтобы максимизировать \emph{расстояние Махаланобиса} $ \Delta^2 = \frac{(\xi_1 - \xi_2)^2}{\sigma_z^2} $

Приходим к системе уравнений:

\[
\begin{cases}
\alpha_1 \sigma_{11} + ... + \alpha_n \sigma_{1p} = \mu_{11} - \mu_{21}, \\
...,\\
\alpha_1 \sigma_{1p} + ... + \alpha_n \sigma_{pp} = \mu_{1p} - \mu_{2p}
\end{cases}
\] 

\subsubsection{Байесовская классификация}

\begin{itemize}[noitemsep]
	\item Байесовский классификатор
	\item Усовершенствованный байесовский классификатор: известны веса ошибок попадания в класс
\end{itemize}

\emph{(Б) Случай, когда параметры неизвестны} (но известно, что выборки приходят из двух нормальных распределений):

$ \mu_i \to \hat \mu_i, \Sigma \to S, S = \frac{(n_1 - 1) S_1 + (n_2 - 1) S_2}{n_1 + n_2 - 2} $

Вектор $ \alpha $ заменяем его оценкой, расстояние Махаланобиса $\Delta$ тоже:

$$ \Delta^2 \to D^2 $$

Несмещённая оценка расстояния Махаланобиса

$$ D^2_H = \frac{n_1 + n_2 - p - 3}{n_1 + n_2 - 2} + p \cdot \left(\frac{1}{n_1} + \frac{1}{n_2}\right) $$

\subsection{Случай многих классов}

"One-vs-all"

\section{Лекция 4. Снижение размерности многомерного признака}

Пусть $ x \in \mathds{R}^p $ -- случайная величина, определяемая матожиданием и дисперсией.
Выборка этой с. в. представлена матрицей <<объект-свойство>> $ X $.

Естественно стремление представить вектор $ x $ в пространстве меньшей размерности, поскольку хочется визуализировать данные, или сжать объём хранимых данных.

Новые признаки $ z \in \mathds{R}^{p'} $ можно выбирать из числа исходных или рассчитывать по какому-либо правилу.
В задаче классификации разумно выбирать признаки, имеющие наибольшую вариабельность.

Задача в общем виде: по результатам наблюдений $ X = \{x | x \in \mathds{R}^p\}, p \in \mathds{N} $ найти преобразование признаков, которое доставляет максимум некоторому критерию информативности $I_{p'}$.

\subsection{Метод главных компонент}

Строим агрегированные признаки.

В качестве класса $f(x)$ допустимых преобразований признаков берём класс линейных ортонормированных преобразований координат.

$ F $

% TODO пропустил немного

Будем считать, что вектор $x$ имеет нулевое матожидание.
Метод главных компонент применим к центрированным данным!
Это обязательное условие.

\subsubsection{Вычисление главных компонент}

$ (5) \to z_1 = l_1 x $

Здесь $ l_1 $ -- первая строка матрицы преобразования.

Матрица ковариации $ \Sigma := \Sigma_x = ... = M[xx^T] $

Задача оптимизации: максимизировать дисперсию первой компоненты при условии: вектор $l_1$ должен быть нормированный.

Строим лагранжиан, вычисляем производную по параметру лагранжиана:

$$ (\Sigma - \lambda I) l_1^T = 0 \thickspace (10) $$ % TODO equations labels

$$ l_1^T \ne 0 \Rightarrow det(\Sigma - \lambda I) = 0 \thickspace (11) $$

Т. о. мы пришли к задаче поиска собственных значений матрицы ковариаций $ \Sigma $.

\subsubsection{Основные числовые характеристики и свойства главных компонент}

\begin{enumerate}[noitemsep]
	\item Главные компоненты также центрированные: матожидание $ M[z] = 0 $
	\item Новые признаки некоррелированы: $ Sigma_z = M[zz^T] = M[Lxx^TL^T] = LM[xx^T]L^T = L\Sigma L^T  \Rightarrow $
	$ \overset{}= $ % TODO equations labels
	\item Условие (8) выполнено % TODO equations labels
\end{enumerate}

\subsubsection{Выбор новой размерности. Критерий информативности}

$$ I_{p'} = \frac{\sum_{j=1}^{p'} \lambda_j}{\sum_{j=1}^p D[x_j]}, p' < p \thickspace (13) $$ % TODO equations labels

Если все признаки нормированы по дисперсии ($ D[x_i] = 1 $), знаменатель -- просто $ p $

\subsubsection{Матрица нагрузок}

Здесь считаем, что выборка центрирована и признаки нормированы по дисперсии (выборка \emph{стандартизована}).

Напомним, $ \Sigma_z = diag\{\lambda_i, i \in \{1;p\}} $.

Матрица нагрузок $ A^{df} = L^T \Lambda^{\frac{1}{2}} $, $ \Lambda := \Sigma_z $

\textbf{Нормированные главные компоненты:} $ z_H = \Lambda^{-frac{1}{2}}z $

\subsubsection{Свойства матрицы нагрузок}
\begin{enumerate}[noitemsep]
	\item $a_{ij}$ -- удельный вес влияния $i$-ой нормированной главной компоненты на $j$-й исходный признак
	\item % TODO
	\item % TODO 
\end{enumerate}

\subsubsection{Геометрический смысл главных компонент}

Если два признака и они линейно зависимы, то главная компонента только одна, она объясняет всю дисперсию.

Если признаки не коррелированы, главных компонент нет (эллипс вырождается в окружность, преимущества в выборе компонент нет). \\

Понятие главных осей относится не только к нормальному распределению.
Главная ось для произвольного распределения задаётся прямой, для которой минимальна сумма квадратов расстояний от всех исходных точек до прямой.

Дисперсия в направлении первых главных компонент максимальна.

\subsubsection{Статистические свойства главных компонент}

Предполагаем, что исходная матрица <<объект-свойство>> центрированная и $x^{(k)}$ взаимно независимы.

Конечно, по выборке мы получаем не сами главные компоненты, а их оценки.
Насколько им можно доверять?

Если все собственные числа $\lambda_j$ матрицы $\Sigma$, то оценки $\hat \lambda_j, \hat l_j $ асимптотически нормальные (т. к. это МП-оценки), асимптотически несмещённые и асимптотически эффективные.
Притом $ \hat \lambda_j  $ независимы от $ \lambda_i, i \ne j $

\subsubsection{Проверка статистических гипотез}

Можем проверить гипотезу о некоррелированности исходных признаков.
В условии гипотезы некоррелированности $ H_0 : cov(x_i, x_j)=0, i \ne j; cov(x_i, x_i) = \sigma_i^2; i,j = 1, ..., p $

Оказывается, для сферического $p$-мерного распределения существует единственное собственное число...

Оценку ... можно использовать для расчёта доверительных интервалов выборочной дисперсии.

% TODO оценка вероятности сферичности 

Прежде, чем переходить к главным компонентам, имеет смысл проверить гипотезу о сферичности признаков.
Если гипотеза не отвергается, возможно, лучше воспользоваться иным 

\subsection{Работа №2}

Готовим модельные данные; после того, как убедились, что дискриминантный анализ проводится верно, пытаемся с использованием написанного кода классифицировать данные из репозитория.
Аккуратно готовим обучающую выборку: данные несбалансированные, гораздо больше данных, отнесённых к первому классу.

Потом делаем метод главных компонент, с ним тоже

Попробовать только центрировать, но не нормировать исходные данные. 
Тогда первые три главные компоненты будут объяснять более $ 80\% $ дисперсии.
Также просьба посмотреть только одну, первую главную компоненту. 

\subsection{Оценивание вероятностей ошибочной классификации}

Теоретическая вероятность: через функцию Лапласа $ \Phi $

Можно построить четырёхпольную таблицу сопряжённости (матрицу соответствий): true/false positive/negative.
Она показывает эмпирическую вероятность классификации (долю неверно классифицированных векторов).

\subsubsection{Процедура скользящего экзамена}

Оценки, построенные с использованием этого метода, являются почти несмещёнными (в каком смысле, мы здесь не говорим).

Удаляем из выборки один элемент, принадлежащий первому классу (первой популяции), строим новую дискриминантную функцию и рассчитываем эмпирическую вероятность ошибки.
Так повторяем для каждого элемента из первой выборки, потом из второй... % TODO

\end{document}
