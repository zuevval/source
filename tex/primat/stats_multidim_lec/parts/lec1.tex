\documentclass[main.tex]{subfiles}
\begin{document}

\section{Лекция 1. Статистические гипотезы и статистические критерии}

2 февраля 2021 года

Немного возвращаемся к одномерной статистике. Как мы знаем, проверка статистических гипотез -- одна из важнейших моделей.

Проводятся повторные независимые наблюдения над случайной величиной $ \xi $. Задача матстатистики -- каким-то образом уменьшить эту неизвестность. Напоминмем, множество значений $ \xi $ -- \textit{генеральная совокупность} (с функцией распределения ). Элементы выборки. Важно, что повторные наблюдения независимы. $X_i$ можно считать многомерной

Выборка -- понятие абстрактное. Реальный набор данных есть реализация выборки; будем обозначать выборку $ X = (X_1, X_2, ..., X_n) $, её реализацию $ x = (x_1, x_2, ..., x_n) $.

Модель $ \mathcal{F} = \{F(t;\Theta)\} $

\emph{Статистическаая гипотеза} -- любое утверждение о виде или свойствах распределения наблюдаемых в эксперименте случайных величин.

Например, в эксперименте по измерению физической величины $ a $ измеренное значение $ X_i $ -- случайная величина. Можно предположить, что ошибка в измерении аддитивна и нормально распределена: $ X_i = a + \varepsilon_i $. Это будет наша основная (нулевая) гипотеза $ H_0 $.

Основная гипотеза проверяется против альтернативной гипотезы $ H_1 $.

\emph{Статистический критерий} проверки гипотезы -- правило, согласно которому гипотеза принимается (при проверке против альтернативной) или отвергается.


% TODO

Проверить принадлежность выборки множеству выборок, при которых мы должны отвергать гипотезу -- какая-то странная задача.

От выборочного пространства переходим к множеству значений статистики $ T(X) $.

% TODO

Ошибка 1 рода: принятие альтернативной гипотезы в условиях, когда она является ложной. Её вероятность будем обозначать $ P(H_1 |H_2) $.

Запишем определения ошибки 1 и $ II $ рода ч

Мощность критерия -- вероятность попадания выборки в ...

Вероятность ошибки 1 рода (вероятность отклонить верную гипотезу $ H_0 $) = вероятность попадания выборки в критическую область при условии $ H_0 $ = мощности % TODO

$ P(H_1 | H_1) $ -- вероятность выявить отклонение от основной гипотезы. $ P(H_0 | H_1) = 1 - P(H_1 | H_1) $ -- вероятность ошибки 2 рода.

Как выбирать критическую область? Варьируя её размер, можно добиться сколь угодно малой и даже нулевой вероятности ошибки 1 рода или 2 рода, но одна из двух при этом будет возрастать вплоть до единицы.

Выборка, как правило, имеет конечный объём. Рациональный принцип выбора критической области такой:

Ограничиваем вероятность ошибки 1 рода величиной $ \alpha \in (0;1) $:
функция мощности $ W(F) \le \alpha \forall F \in \mathcal{F}_0 $

Второе условие: 
\begin{equation} \label{eq:stat_crit2}
	1 - W(F) \to min_{\mathfrak{X}_1}
\end{equation} 

Т. о. переходим от критической области $ \mathfrak{X}_1 $ к новой $ \mathfrak{X}_{1\alpha} $

Основное назначение критерия -- улавливать отклонение от основной гипотезы. Характер отклонения может быть разнообразным, поэтому и критерии должны быть разными.
В одном случае это должны быть критерии общего толка (например, критерии согласия), в других случаях должны быть узконаправленные (например, критерии для проверки однородности выборки).

% TODO

Ключевой момент для расчёта критерия: нужно знать функцию распределения статистики критерия в условиях, когда гипотеза $ H_0 $ верна $ F_{T(X)} | H_0 $.

Задачу \ref{eq:stat_crit2}, как правило, не удаётся решений, и вместо этого допустимыми критериями считаются те, которые обладают свойствами \emph{состоятельности} и \emph{несмещённости}

\begin{enumerate}[noitemsep]
	\item Несмещённость
	\item Состоятельность: при $ n \to \infty $ вероятность уловить отклонение от $ H_0 $ $ \to 1 $.
\end{enumerate}

\subsection{Параметрические гипотезы}
Пусть у нас есть модель с параметрами: $ \mathcal{F} = \{F(t;\theta), \theta \in \Theta \in \mathds{R}^r\} $
Знание параметра $\theta$ полностью определяет функцию распределения, поэтому можно записать 

(в файле лекции)

Уроверь значимости $ \alpha $ обычно выбирают небольшим, стандартное значение $ \alpha = 0.05 $. Чем меньше $ \alpha $, тем ниже вероятность ошибки 1 рода. Но нужно, чтобы критерий мог улавливать альтернативы, поэтому в зависимости от задачи может подходить даже $ \alpha = 0.25 $.

\subsection{Равномерно более мощный критерий}

Пусть выбран уровень значимости и построены два критерия: $ \mathfrak{X}_{1 \alpha}, \mathfrak{X}_{1 \alpha}^* $.
Пусть также вероятность ошибки 1 рода $ W(\mathfrak{X}_{1 \alpha}; \theta) \forall \theta $ ... и ... % TODO

% TODO ask: что значит "для всех критериев"?

Гипотеза является \emph{простой}, если включает только одну модель. Если гипотеза допускает несколько моделей, она называется сложной.

Если проверяется простая альтернатива, такой критерий называется просто "наиболее мощным". Пример: критерий Неймана-Пирсона % TODO

\subsection{Критерий отношения правдоподобия}

Критерий отн

Функция правдоподобия, напомним, есть совместная плотность вероятности элементов выборки. Элементы выборки -- независимые случайные величины, поэтому совместная вероятность можно выразить через произведение.

Статистика $ \lambda_n = \frac{\sup_{\theta \in \Theta_0} L(X;\theta)}{\sup_{\theta \in \Theta} L(X;\theta)} $

Критическая область $ \mathfrak{X}_1 = \{x: \lambda_n = \lambda_n(x; \Theta_0) \le c\} $

Будем считать, что модель $ \mathcal{F} $ \emph{регулярная}, то есть % TODO
Тогда для больших выборок критерий отношения правдоподобия таков ($ r $ -- число параметров):
\begin{enumerate}
	\item $ H_0 : \theta=\theta_0 $, $ \theta_0 $ -- фиксированная внутренняя точка множества $ \Theta $ при $ n \to \infty $. Тогда $ \mathfrak{X}_{1 \alpha} = \{x: - 2 \ln \lambda_n(x;\theta_0) \ge \xi{1 - \alpha r}^2\} $
	\item % TODO
\end{enumerate}

\subsection{p-value}

Есть противники и сторонники использования $ p-value $. В фишеровской статистике гипотезам вероятности не назначаются (другое дело -- в байесовской статистике).

Пусть есть гипотеза и статистика критерия $ T(X)$; обозначим $ T^* = T(x) $ (значение статистики критерия при данной реализации выборки).

% TODO

В биоинформатике любят пользоваться $ p-value $: выбрать уровень значимости, провести статистический тест и смотреть, 

\section{Практика 1. Проверка гипотез о виде распределения}

Проверяем простую гипотезу $H_0$ о том, что функция распределения равна заданной ($ F_\xi(t) $ полностью задана).
Альтернативная гипотеза $ H_1 $ сложная: $  $

Критерии, проверяющие гипотезы о виде распределения, называются \emph{критериями согласия}.

\textbf{Критерий согласия Колмогорова}: $ D_n(X) = \sup_t |F_n(t) - F(t)| $

Эмпирическая функция распределения $ F_n(t) $ -- ступенчатая фукнция со скачками в точках-элементах выборки, которую строим по реализации выборки.

Теорема Гнеденко (Гнеденко-Кантелли): эмпирическая фукнция распределения является состоятельной оценкой функции распределения.
Это следствие теоремы Бернулли.

Что хорошо, уже начиная с $ n \approx 20 $ значение $ \sqrt{n} D_n $ практически не зависит от $ n $ и подчиняется распределению Колмогорова.

Критическое значение $ \sqrt n D_n $ определяется как квантиль уровня $ 1 - \alpha $ распределения Колмогорова.

\subsection{Критерий согласия Пирсона}

Критерий согласия Пирсона можно использовать только для простых гипотез!

Тест сводится к проверке гипотезы о том, что вектор частот распределён по полиномиальному закону:

$$ P\{\nu_1 = m_1, ..., \nu_N = m_N\} = \frac{n!}{m_1! \cdot ... \cdot m_N!} (p_1^0)^{m_1} \cdot ... \cdot (p_N^0)^{m_N} $$

В случае конечного объёма выборки для статистики хи-квадрат не удаётся найти распределение $ F_{T(X)} | H_0 $, но Карл Пирсон доказал состоятельность оценки.

% TODO

% TODO figure out on my own: 
Нужно проверить: если $ t_\alpha $

\subsection{Критерий согласия хи-квадрат для сложной гипотезы}

Ограничимся случаем, когда класс гипотетических функций -- параметрическое семейство.

Если бы пользовались критерием Пирсона, надо было бы построить $ X_n^2 = $ % TODO
но параметры $p_j^0$ нам неизвестны.

$ \theta \to \hat \theta $  согласно теореме Фишера

% TODO

Мультиномиальные оценки чуть сложнее находить, чем ММП, но на самом деле их можно выбирать как минимум некоторой функции, зависящей от параметров $ \theta $

Критическая область находится аналогично критической области критической области статистики хи-квадрат Пирсона.


Наиболее информативные выборачные статистики -- выборочный коэффициент асимметрии и эксцесса.
Если они близки к нулю, можно предположить нормальность; иначе предполагаем другое распределение: вариации гамма-распределения, ...

Гипотезу выдвигаем параметрическую и проверяем, используя критерий Фишера.
Смотрим, отвергается гипотеза или нет.

Смотрим, согласуется ли теоретическая кривая с эмпирической.

Делаем выводы.
Хорошее согласование или плохое?
Если не отвергаются по крайней мере две гипотезы, какую нужно выбрать в качестве рабочей (ту, которой соответствует наименьшее значение статистики критерия).

\end{document}