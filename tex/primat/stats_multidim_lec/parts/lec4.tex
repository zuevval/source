\documentclass[main.tex]{subfiles}
\begin{document}

\section{Лекция 4}
2 марта 2021 г.

Литература: Демиденко, <<Линейная и нелинейная регрессия>>

\subsection{Классияеская линейная регрессия: обозначения и основные предположения}

К матрице <<объект-свойство>> $ X = \begin{pmatrix}
    x_{11}&  ... & x_{1m} \\
    \vdots & & \vdots \\
    x_{n1}&  ... & x_{nm} \\
\end{pmatrix} $ в регрессии добавляется вектор $ y = \begin{pmatrix}
y_1 \\ \vdots \\ y_n
\end{pmatrix} $

Классическая линейная регрессия: предполагается, что матрица $ X $ детерминированная.

$y_i$ называют зависимыми (выходными) переменными.
Предположим, что
\begin{equation} \label{eq:lin_reg}
    y_t = \sum_t \alpha_t x_{tj} + \varepsilon_t
\end{equation}
 где $\varepsilon = \begin{pmatrix}
    \varepsilon_1 \\ \vdots \\ \varepsilon_n
\end{pmatrix}$ -- \emph{вектор случайных отклонений}.

Выражение \eqref{eq:lin_reg} и есть линейная регрессия.
Индекс принято называть $t$, т. к. часто это время.

Матричная запись \eqref{eq:lin_reg}:

\begin{equation} \label{eq:lin_reg_mtx}
    y = X \alpha + \varepsilon
\end{equation}

Нужно определить неизвестный вектор $ \alpha $.

В классической модели используются следующие предположения:

\begin{enumerate}[noitemsep]
    \item На вектор неизвестных параметров $ \alpha $ не накладывается никаких ограничений: $ Dom(\alpha) = \mathds{R} $ \label{item:no_limitations}
    \item Вектор $ \varepsilon $ случайный, следовательно, $ y $ -- тоже случайный вектор
    \item $ M [ \varepsilon ] = 0 $ \label{item:expect0}
    \item $ \forall t_k \ne t_s M[\varepsilon_{t_k}, \varepsilon_{t_s}] = 0; M[\varepsilon_t^2] = \sigma^2 $ (дисперсия каждой ошибки одна и та же, и ошибки не коррелированы). Отсюда
    $$ cov(\varepsilon) = \sigma^2 I_{n \times n} $$ \label{item:normal_err}

    \item $ X $ -- детерминированная матрица

    \item $ rank(X) = m $ \label{item:rank}
\end{enumerate}

Выдвинутые предположения могут быть ослаблены, но каждое ослабление требует дополнительных исследований.

На основе предположения \ref{item:expect0} можно заключить, что вид рассматриваемой регрессии есть

$$ My = X \alpha $$

Будем строить статистическую оценку $ a = \hat \alpha = T(y) $ вектора оценок $ \alpha $.
Тогда оценка $ \hat y := Xa \Rightarrow $ можно ввести вектор остатков (оценённых отклонений) $ e := y - \hat y $, т. о. $ y = \hat y + e $

Замечание: обычно в модели регрессии вводят дополнительный фактор (признак), который всегда представлен столбцом из единиц.
В наших обозначениях это будет последний столбец $ x_{tm} $.
Ему будет соответствовать свободный член $ \alpha_m $.
Такой фактор служит для того, чтобы ослабить требование \ref{item:expect0}, которое трудно соблюсти.

\subsection{МНК-оценки параметров регрессии}

Минимизируем сумму квадратов отклонений от модели (метод наименьших квадратов).

функционал качества $ Q(\alpha) = \sum_{t=1}^{n} (\hat y_t - y_t)^2 = ... $

\begin{equation} \label{eq:least_sq}
    \hat alpha = a = (X^T X)^{-1} X^T y
\end{equation}

Если выполнено \ref{item:rank}, МНК-оценка \eqref{eq:least_sq} будет единственная.

\subsubsection{Общие статистические свойства МНК-оценок и статистики s2}

Обозначим $ G := (X^T X)^{-1} X^T $.
Найдём матожидание $a$. $ G $ -- детерминированная матрица $ \Rightarrow $
$$ Ma = (X^T X)^{-1} X^TMy = (X^T X)^{-1} X^T X \alpha = \alpha $$
$ Ma = \alpha \Rightarrow $ ура, $ a $ -- несмещённая оценка $ \alpha $.

Оценим \emph{матрицу ковариации} случайного вектора $a$ $ cov(a) = M[(a-Ma)(a-Ma)^T] $:

$$ (a-Ma) - (a-\alpha) = ... (\text{см. лекцию}) = (X^T X)^{-1} X^T \varepsilon $$
$$ cov(a) = ... = \sigma^2(X^T X)^{-1} $$

Матрицу ковариации нельзя считать статистикой в рамках нашей модели, поскольку матрица $ X $ детерминированная, а выборка включает ещё и зависимый параметр $ y $.

\subsubsection{Теорема Гаусса-Маркова}

\begin{theorem}
В указанных выше предположениях МНК-оценка параметров $ \alpha $

\begin{enumerate}[noitemsep]
    \item Несмещённая (это мы уже доказали)
    \item Эффективная в классе несмещённых оценок, линейных по $y$ (т. е. её дисперсия наименьшая среди всех таких оценок).
\end{enumerate}
\end{theorem}
\begin{proof}
Пусть существует ещё одна оценка $ b = Hy: Mb = \alpha, b \ne a $

$$ M[b]= ... =HX_{\alpha} $$

...

$ CC^T $ является неотрицательно определоённой $ \Rightarrow $ матрицы ковариации векотров $ b $ и $ a $ связаны соотношением: $ cov(b) \ge cov(a) $.
При этом $ C \ne 0 \Rightarrow $ существует координата $ k: \sigma^2_{bk} > \sigma^2{ak} $

\end{proof}

\subsubsection{Оценка дисперсии отклонений регрессионной модели и матрицы ковариаций}

Дисперсия отклонений $ \sigma^2 = D[\varepsilon_t], t = 1, ..., n $.

Её оценка $ s^2 = \frac{\sum_t e_t^2}{ n-m } = \frac{e^T e}{n - m} $ является несмещённой и состоятельной, причём состоятельность не зависит от матрицы $ X $.

Тогда можно получить оценку для матрицы ковариации вектора $ a $:

$$ cov(a) = \sigma^2 (X^T X)^{-1} $$

Пользоваться ковариационной матрицей не очень удобно (зависит от единиц измерения данных), поэтому вводят безразмерную \emph{корреляционную матрицу} --  делим на среднеквадратичные выборочные отклонения:

$$ corr_{ij}(a) = \frac{(X^T X)_{ij}^{-1}}{\sqrt{(X^T X)_{ii}^{-1} (X^T X)_{jj}^{-1}}} $$

Это -- точные, истинные оценки коэффициентов корреляции вектора $ a $.

\subsection{Задание}

После сегодняшней лекции можно выполнять уже всё то, что в первом абзаце задания.

Гистограмма остатков под вопросом, т. к. выборка мала и неизвестно, получится ли красивое распределение с нулевым матожиданием.

\end{document}
