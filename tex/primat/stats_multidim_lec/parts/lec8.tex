\documentclass[main.tex]{subfiles}
\begin{document}

\section{Лекция 8}
30 марта 2021 г. \\

\subsection{Перебор и недобор факторов в регрессии. Выбор независимых переменных (спецификация модели)}

Пусть модель регрессии нам неизвестна (не знаем вид матрицы $ X $).
Можно совершить две ошибки: перебор факторов и недобор факторов.

\subsubsection{ Перебор факторов }
Пусть истинная модель содержит $ m $ факторов, а предполагаемая модель -- $ m + k $ факторов.
Пусть также в нашей выборке $ n $ экземпляров.

Предполагаемая модель в матричной форме:
\[ y = Z \beta + \xi, Z = [ X P ] \]
$ P $ -- добавленные по ошибке факторы.
\[ \beta^T = ( \delta^T, \phi^T ) \]
$ \delta $ -- оценки реально существующих параметров, $ \phi $ -- добавленные нами по ошибке.

\begin{enumerate}[noitemsep]
    \item $ b $ -- МНК-оценка $ \beta $ -- несмещённо оценивает величину в следующем смысле:
    \[ ] b^T = (d^T, f^T) \Rightarrow M[d] = \alpha, M[f] = 0, d \in \mathds{R}^m, f \in \mathds{R}^k \]
    \item В условии сильной регулярности $ Z $ оценка состоятельная
    \item $ cov(d) = cov(a) + \sigma^2 G $, где матрица $ G $ симметричная и положительно определённая.
\end{enumerate}

Замечание:

\begin{enumerate}[noitemsep]
    \item идеальный вариант -- когда добавленные нами переменные ортогональны истинным: $ X^T P = 0 \Rightarrow cov(d) = cov(a) $
    \item Оценка дисперсии $ s^2_b $ является состоятельной и несмещённой.
\end{enumerate}

\subsubsection{ Недобор факторов }

Пусть предполагаемая модель содержит $ k < m $ параметров.

\[  y = W \gamma + \xi  \]
$ W $ -- подматрица истинной матрицы  $ X $: $ X = [W V] $

МНК-оценка $ \gamma $: $ g = (W^T W)^{-1} W^T y $

Свойства оценки $ g $:

\begin{enumerate}[noitemsep]
    \item В общем случае оценка смещённая: $ M[g] = \alpha^{(1)} + (W^T W)^{-1} W^T V \alpha^{(2)} $

    Если второе слагаемое равно нулю ($ W^T V = 0 $)

    \item Оценка несостоятельная
\end{enumerate}

Вывод: как правило, перебор параметров лучше, чем недобор.

\subsubsection{ Статистика s2 при выборе независимых переменных }

Оценка дисперсии, полученная по истинной модели, не превышает полученную по ошибочной модели.

\subsubsection{ Оценка Эйткена (обобщённый МНК) }

Пусть в нашем наборе предположений о линейной регрессии четвёртое предположение ослабляется: не $ M[ \varepsilon_{t_i} \varepsilon_{t_j} ] \underset{i \ne j}= 0 $ и $ cov(\varepsilon) = \sigma^2 I_n $,
а $ cov(\varepsilon) \sigma^2 \Omega $, $ \Omega \in \mathds{R}^{n \times n} $, $ \Omega $ -- положительно определённая матрица.

$ \hat \alpha^{\text{э}} $ -- так обозначим оценку Эйткена.

\[ \hat \alpha^{\text{э}} = [ X^T \Omega^2 X ]^{-1} X^T \Omega^{-1} y \]

\begin{theorem}
    (обобщённая теорема Гаусса-Маркова): % TODO
\end{theorem}

\begin{theorem}
    Если вектор ошибки $ \varepsilon $ распределён по стандартному нормальному закону: $ \varepsilon \sim \mathrm{N}(0, \sigma^2 I_n) $, то % TODO
\end{theorem}

\begin{theorem}
    Оценка Эйткена состоятельна, если в пределе при $ n \to \infty $ выполнено условие Эйкера ( $ \lambda_{min}(X^T_nX_n) \to 0 $ ) и максимальное собственное число $ \Omega_n $ конечное.
\end{theorem}

Частный случай -- гетероскедастичная модель:

\begin{leftbar}
    \emph{Гомоскедастичная модель} -- все дисперсии признаков одинаковые.
   \emph{Гетероскедастичная модель} -- дисперсии разных признаков могут быть разные.
\end{leftbar}

\subsection{Заключение}

\subsubsection{Робастные оценки}
МНК обладает многими замечательными свойствами, но он не является \emph{робастным}: при возникновении серьёзных выбросов МНК-оценки могут <<поехать>>.

Можно пользоваться МНК, изначально проведя анализ исходных данных на наличие выбросов.
Существуют, однако, в регрессионном анализе и робастные оценки:

\begin{enumerate}[noitemsep]
    \item $ L_\nu $-оценка: вместо суммы квадратов минимизируется $ \sum_t |y_t - \vartheta|^\nu $, $ 0 < \nu < 2 $.

    Если $ \nu \ge 1 $,

    Желательно пользоваться $ \nu $ не меньше единицы.
    \item Оценка Хюбера

    $ \rho $ -- выпуклая функция, на неё накладываются определённые ограничения.
\end{enumerate}

\subsubsection{ Полиномиальная регрессия }
Познакомившись с линейной регрессией, мы умеем строить и \emph{полиномиальную} регрессию вида
\[ y_t = \alpha_1 z_t + \alpha_2 z_t^2 + ... + \alpha_m z_m^2 + \varepsilon_t \]
Эта задача сводится к линейной регрессии введением факторов $ z_t, z_t^2 $ и так далее.

\subsubsection{ Нелинейная регрессия }

В случае нелинейной модели МНК-оценка может не существовать.

Самое сложное в нелинейной модели -- подбор самой модели $ \phi $.
Есть современные методы подбора.
Например, метод опорных векторов позвлояет успешно восстановить модель по экспериментальным

\subsubsection{ Регрессия, линейная в логарифмах }

Можно попытаться линеаризовать нелинейную модель.

Должно выполняться условие: $ g(f_t( \alpha ) + \varepsilon_t) \approx g(f_t(\alpha)) + \xi_t $

Если $ f_t(\alpha) = \exp( \alpha_1 x_1 + ... + \alpha_m x_m ) $, в качестве  $ g $ может подходить логарифм. \\

Например, такое предположение можно сделать при решении третьей лабораторной.

\begin{enumerate}[noitemsep]
    \item
    \item
\end{enumerate}

\end{document}

