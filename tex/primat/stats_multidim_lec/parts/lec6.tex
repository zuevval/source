\documentclass[main.tex]{subfiles}
\begin{document}

\section{Лекция 6}
16 марта 2021 г.

\subsubsection{Коэффициент детерминации в регрессионном анализе и его интерпретация}

Линейная регрессия в математической статистике вводится немного иначе, чем в регрессионном анализе.
Факторы тоже считаются случайными величинами, как и значения отклика $ y $.

Регрессией в мат. статистике называют условное математическое ожидание $ M[y|x_1,x_] $

\textbf{Коэффициент детерминации в мат. статистике} вводится через отношение условной дисперсии к безусловной.

$$ r^2 = 1 -  \frac{\sigma^2(y|x_1, ..., x_m)}{\sigma^2(y)} $$

Мы считаем, что $ \sigma^2(y|x_1,...) $ -- величина постоянная, равная $ \sigma^2 $, поэтому

$$ r^2 = 1 - \frac{\sigma^2}{\sigma^2(y)} $$

$ r^2 $ -- показатель адекватности введённой модели.
Т. о. $ r^2 $ -- доля объяснённой дисперсии, $ 1 - r^2 $ -- доля дисперсии, которая не смогла быть объяснена.

Эту интерпретацию часто неверно переносят на коэффициент детерминации в линейной регрессии.
Но \textbf{коэффициент детерминации в классическом регрессионном анализе}  $ R^2 $ вводится иначе.

Напомним,
\begin{equation} \label{eq:linreg}
    y_t = \alpha_1 x_{t1} + ... + \alpha_{m-1}x_{tm-1} + \varepsilon_t, t = 1,...,n
\end{equation}

...

% TODO ask: почему нужно везде домножать на единицу?

Сумма разбросов объясняемой переменной вокруг среднего объясняется двумя суммами:

\begin{equation}\label{eq:linreg_deviances}
    \sum_t (y_t - \bar y)^2 = \sum_t (\hat y_t - \bar y)^2 + \sum_t e_t^2
\end{equation}

\begin{equation}
    R^2 = TODO =  1 - \frac{\sum_t e_t^2}{\sum_t (y_t - \bar y)^2 = 1 - \frac{\frac{\sigma_t e_t^2}{n}}{\frac{\sigma_t (y - \bar y)^2}{n}}}
\end{equation}

Т. о. в модели с детерминированными факторами коэффициент детерминации $ R^2 $ показывает, насколько модель нашей регрессии \eqref{eq:linreg} лучше, чем модель среднего.

Модель среднего:
$$ y_t = \beta_m + \xi_t $$

Чем ближе $ R^2 $ к единице, тем больше дополнительные введённые линейные добавки удачны и тем более обоснованно применение линейной регрессии.

Нельзя забывать, что коэффициент детерминации с добавлением новых переменных не уменьшается.

Иногда удобнее использовать модифицированный коэффициент детерминации -- несмещённый:

$$ R_{Н}^2 = 1 - \frac{\sum_t e_t^2 / (n-m)}{\sum_t (y_t - \bar y)^2 / (n-1)} $$

\subsection{Проверка гипотез в линейной регрессии}
$$ y = X \alpha + \varepsilon $$
МНК-оценка $ \alpha $
$$ a = (X^TX)^{-1}X^Ty $$

Будем обозначать сумму квадратов отклонений
$$ \hat Q = (y-Xa)^T (y-Xa) $$

\subsubsection{Линейная гипотеза общего вида}

$$ H_0: R\alpha=r, R(k\times m), rank R = k \le m, r \in (k \times 1)   $$

Т. о. на коэффициенты накладываются $ k $ независимых ограничений.

Как и в методе главных компонент, строим лагранжиан и, используя метод множителей Лагранжа, можем легко показать, что МНК-оценка вектора $ \alpha $ при наличии ограничений
$$ a_R = a + (X^TX)^{-1}R^TS(r-R\alpha), S = [R(X^TX)^{-1}, S(k\times k)] $$

Чтобы оценить, насколько оценка правильно описывает зависимость, нужно знать распределение $y$. Пусть
$$ y \sim \mathcal{N}(X\alpha, \sigma^2 I_n) $$

Воспользуемся критерием отношения правдоподобия.

\subsubsection{ Критерий отношения правдоподобия для проверки линейной гипотезы }

Вводится статистика $ t $ -- отношение условного максимума к глобальному максимуму.

Квадратичная форма $ f \to max | H_0 $ при $ \hat \alpha = a_R $.
В этом случае
$$ s_R^2 = \frac{1}{n} \left(y - X a_R\right)^T (y - Xa_R) $$

В случае безусловной максимизации оптимальное значение есть МНК-оценка:
$ \hat \alpha = a \Rightarrow f \to \max $;

$ f \to max \Rightarrow \hat \sigma^2 = s_{MaxLikelihood}^2 = \frac{1}{n} \left( y - Xa \right)^T \left( y - Xa \right) = \frac{1}{n} \tilde Q_R $

... преобразование критической области статистики $t$ ...

Как распределена статистика $ t $?

Можно показать, что квадратичные формы в числителе и знаменателе распределены по закону хи-квадрат $ \Rightarrow $ их отношение распределено по закону Фишера, и критическое значение сравниваем с квантилем распределения Фишера.

\subsubsection{применение критерия отношения правдоподобия}

Рассмотрим два частных случая применения критерия отношения правдоподобия.

\begin{enumerate}[noitemsep]
    \item Проверим гипотезу о равенстве одного коэффициента $ \alpha_i $ фиксированному значению $ \beta \in \mathds{R}^1 $.

    $$ k=1; R = R(1\times m) = [0 0 ... 0 1 0 ... 0] $$

    Если эта гипотеза подтверждается,
    Т. о. можно проводить автоматический отбор переменных.

    Здесь ошибка 1 рода будет такова: $ \alpha_i = 0 $, а делается вывод о том, что $ \alpha_i \ne 0  \Rightarrow$ в таком случае будет перебор факторов.

    Ошибка второго рода (принятие неверной гипотезы): в реальности $ \alpha_i \ne 0 $, а мы считаем, что $ \alpha_i = 0 \Rightarrow $ недобор параметров, что хуже избытка.

    Вывод: при проверке такой гипотезы не нужно увлекаться слишком малыми значениями уровня значимости.
    \item % TODO
    \item Проверяем гипотезу однородности двух выборок: можно ли объединить в одну две регрессии $ y_1 = X_1 \alpha_1 + \varepsilon_1, |y_1| = n_1 $; $ y_2 = X_2 \alpha_2 + \varepsilon_2, |y_2| = n_2 $ (в предположении о том, что случайные отклонения нормальные)

    Нужно найти $ \hat Q_R $ -- сумму квадратов отклонений в условиях справедливости гипотезы $ H_0 $ и $ \hat Q $ -- сумму квадратов отклонений в случае двух разных регрессий.

    В условиях $ H_0 $ находим $ \alpha = \alpha_1 c= \alpha_2 $ с помощью МНК: введём

    $$ X := \begin{pmatrix} X_1 \\ X_2 \end{pmatrix}, y := \begin{pmatrix} y_1 \\ y_2 \end{pmatrix}, \varepsilon := \begin{pmatrix} \varepsilon_1 \\ \varepsilon_2 \end{pmatrix} \Rightarrow y = X \alpha + \varepsilon $$

\end{enumerate}

\begin{enumerate}[noitemsep]
    \item
    \item
\end{enumerate}

\end{document}
