\documentclass[main.tex]{subfiles}
\begin{document}

\section{Лекция 5}
9 марта 2021 г.

Линейная регрессия. Продолжение. МНК-оценки.

Напомним, доказывали теорему Гаусса-Маркова: МНК-оценка параметров линейной регрессии является несмещённой и эффективной в классе несмещённых оценок, линейных по $y$.

Сегодня изучим свойства, которые не всегда выполняются для МНК-оценок.
рассмотрим, как ведёт себя МНК-оценка при увеличении объёма выборки $n$.

\subsubsection{Виды состоятельности одномерной статистики}

\begin{enumerate}[noitemsep]
    \item \emph{Слабая состоятельность:} определяется через сходимость по вероятности. \label{item:consistency_weak}
    \item \emph{Сильная состоятельность:} опирается на сходимость с вероятностью 1 \label{item:consistency_strong}
    \item \emph{Состоятельность в среднем квадратическом:} матожидание отклонения стремится к нулю. \label{item:consistency_msq}
\end{enumerate}

$ \ref{item:consistency_msq} \Rightarrow \ref{item:consistency_weak}, \ref{item:consistency_strong} \Rightarrow \ref{item:consistency_weak} $.
В статистике чаще используют слабую состоятельность, но теоремы проще доказывать для состоятельности в среднем квадратичном.

\subsubsection{Условия состоятельности МНК-оценки}

Матрица $ X_n $ \emph{сильно регулярная} (далее с/р), если
\begin{equation} \label{eq:strong_reg}
    \lim\limits_{n \to \infty} \frac{1}{n} X_n^T X_n = A
\end{equation}
где $ A $ -- невырожденная матрица.

\begin{theorem}
    \ref{item:no_limitations} -- \ref{item:rank}, \eqref{eq:strong_reg} $\Rightarrow$ МНК-оценка параметров линейной регрессии состоятельна
\end{theorem}

Не всегда удобно проверять сильную регулярность.

% TODO a bit

Матрица сопряжённости похожа на матрицу корреляции, но величины не центрированы.

Теорема: если сумма квадратов элементов столбцов стремится к бесконечности, а матрица сопряжённости

Без доказательства.
Доказательство есть в Демиденко.

Пример.
$$ y_t = \alpha_1 t + \alpha_2 + \varepsilon_t, \thickspace t = 1, ..., n $$
$$ X = \begin{pmatrix}
    1 & 1 \\ 2 & 1 \\ \vdots & \vdots \\ n & 1
\end{pmatrix} $$
...

Подчеркнём, что МНК-оценка дисперсии $ S^2 $ состоятельна всегда, без дополнительных условий.

\subsubsection{Асимптотическая нормальность МНК-оценки}

Позволяет считать распределение отклонения от оценки приблизительно нормальным.
Если это выполняется, можно использовать для построения доверительных интервалов.

Последовательность оценок $ \vartheta_1, \vartheta_2, ... $ \emph{асимптотически нормальна}, если $ \exists \alpha, \beta : $ % TODO

Напомним, МНК-оценки параметров регрессии
$ a = (X^TX)^{-1}X^Ty $ % TODO

\begin{theorem}
    (достаточный критерий асимптотической нормальности Андерсона):

    Если случайные величины $ \{\varepsilon_t\} $ независимы и одинаково распределены при $ n \to \infty $, а также максимум нормированного квадрата элемента в каждом столбце при $ n \to \infty $ стремится к нулю и матрица сопряжённости $ R_n \to R : |R| \ne 0 $,  МНК-оценка асимптотически нормальна.
\end{theorem}


% TODO ask: почему Вы говорите, что должны быть выполнены условия, определяемые матрицей А?

\subsubsection{Свойства МНК-оценки при нормальных отклонениях}

Предполагаем (\ref{item:normal_err}) $ \varepsilon \sim \mathcal{N}(0, \sigma^2 I_n) \Rightarrow y \sim \mathcal{N}(X\alpha, \sigma^2 I_n) $

\begin{theorem}
    При выполнении условия \ref{item:normal_err} МНК-оценка совпадает с МП-оценкой:
    $$ \hat \alpha^{\text{МНК}} = \hat \alpha^{\text{МП}} $$
\end{theorem}

Ещё есть несколько теорем, последняя из которых даёт ценный результат: если ошибки нормально распределены, одинаковы и независимы, то МНК-оценка является состоятельной в классе вообще всех несмещённых оценок, и формула для матрицы ковариации такая же, конечно, как в теореме Гаусса-Маркова.

\end{document}
