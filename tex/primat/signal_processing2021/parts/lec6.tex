\documentclass[main.tex]{subfiles}
\begin{document}

\section{ Лекция 6. Обучение без учителя. Кластеризация и понижение размерности / TODO }
11 марта 2021 г.

\subsubsection{Искусственный интеллект}

Искусственный интелект в широком смысле слова -- качественный подбор модели.

Когда параметров не очень много, мы называем это машинным обучением.
Чёткой грани между машинным обучением и подбором статистической модели нет.

Если есть размеченные данные: понимаем, какой ответ хороший и какой не очень хороший.
Но бывает, что меток нет (например, есть набор данных о транзакциях, но нет информации, какие из них мошеннические).
Это приводит нас к задачам обучения без учителя.

Если мы научимся использовать неразмеченные данные, это во многих случаях даст возможность использовать большие датасеты, которые сложно разметить.
Зато не очень понятно, как оценивать качество.

Как можно использовать неразмеченные данные?
Можно понижать размерность, визуализировать, кластеризовать данные.

Пример: есть данные о покупках и операциях клиентов; хотим понять, что ему понравится (какие ещё продукты ему имеет смысл рекламировать).

\subsection{Снижение размерности }
Имеем множество признаков.

Признаки могут принимать множество значений или малый диапазон; так или иначе, если признаков много, сложно манипулировать данными и ещё сложнее искать зависимости.

Было бы интересно данные из, скажем, 1000 признаков (возможно, зависимых) свернуть, например, в 10 наиболее показательных.

\subsubsection{PCA -- Principal Component Analysis}

Классический метод.

Считаем, что исходные данные -- приближённо эллипсоид в $n$-мерном пространстве.
Пытаемся найти подпространство, в котором эл

Ищем коэффициенты ковариации (они показывают, насколько оси=признаки похожи друг на друга, точнее, насколько хорошо их зависимость описывается линейной функцией).
Пример: рассмотрим признаки $ i,j $; у нас есть набор точек в $n$-мерном пространстве $ X $.
$ cov(X_i, X_j) = E[(X_i - \mu_i)(X_j - \mu_j)] $

Ищем оси с наибольшей дисперсией и поворачиваем наше пространство признаков так, чтобы эллипс был наиболее вытянут вдоль осей.

Снижаем размерность, оставляя только те новые признаки, которые

\subsubsection{ PCA. Подбор числа компонент }

% TODO

Много идей, которые можно использовать.

\subsubsection{t-SNE}

Как-то умеем сокращать размерность, но далеко не идеально.
Хотим придумать что-то кроме максимизации дисперсии.

Критерий, который кажется разумным: попытаться по возможности сохранить пропорции расстояний между точками.

Для этого нужны оценки расстояний.
Введём $ p(x_i, x_j) $ -- верятность встретить $x_j$ в окрестности $x_i$; пусть есть соотношение
$$ p(x_i, x_j) = \alpha p(x_i, x_k) $$
Хотим при переходе в новое пространство сохранить вероятность:
  % TODO ask: при условии? Или совместная
$$ p(\bar x_i, \bar x_j) = \alpha p(\bar x_i, \bar x_k) $$

Считаем, что расстояния между точками в пределах одного кластера распределены по нормальному закону.
Поэтому можем использовать в качестве расстояния \emph{дивергенцию Кульбака-Лейблера}.

В формуле для расстояния есть многомерный параметр $ \vec \sigma = \{ \sigma_i \}, \sigma_i  $ соответствуют элементам данных $x_i$.

\subsubsection{ SNE. Подбор сигм в оценке расстояния }

$$ p_{j|i} = \frac{\exp(- || x_i - x_j ||^2)}{den} $$ % TODO denominator

Хотим взять разные сигмы для разных фрагментов (разных кластеров).
Можно их обучить.
Можно подбирать сигмы так, чтобы они были меньше в областях с большей концентрацией данных; или задаём некоторую характеристику -- перплексию -- в каждой точке, минимизируя которую, подбираем сигму.

\subsubsection{t-SNE. Улучшение SNE}

Идеи:

\begin{enumerate}[noitemsep]
	\item Проблема распределения Гаусса: то, что находится в хвостах распределения вокруг точи, почти не учитывается.
	Можно взять \emph{распределение Стьюдента}, хвосты которого не так быстро убывают.
	\item Переходим от вероятностей $ p_{i|j}, p_{j|i} $ к симметричным верояностям $ p_{ij} = \frac{p_{i|j} + p_{j|i}}{2n} $.
	Это позволяет
	\item <<Ранняя компрессия>>: начинаем градиентный спуск в пространстве параметров с близких друг к другу точек.
\end{enumerate}

Алгоритм t-SNE -- просто градиентный спуск.

\subsubsection{карты данных}

Есть множество алгоритмов визуализации, похожих на t-SNE, в которых мы задаём какой-то порядок, какую-то структуру (например, сетку) и трансформируем её, передвигая точки.
Их называют \emph{картами данных} (к примеру, есть карты Кохонен).

Передвигаем узлы струкутры так, чтобы минимизировать и расстояния, и деформации сетки. (как правило, градиентный спуск).

\subsection{Кластеризация}

Кластеризация редко помогает построить завершённый алгоритм машинного обучения.
Способов кластеризации множество (делим по близости точек, по связности и так далее).
Какой из них выбрать -- зависит от задачи.

\subsubsection{Этапы кластерного анализа}

\begin{enumerate}[noitemsep]
	\item Нужны переменные (возможно, их как-то необходимо нормализовать).
	\item Нужна мера -- критерий похожести (очень важно).
	\item Далее применяем алгоритм кластеризации, смотрим на результаты и делаем выводы.
\end{enumerate}

\subsubsection{K-means / K-medians / ...}

Самый <<дубовый>>, простой алгоритм.
Работаем только в $n$-мерном пространстве, для графов не подходит.

Алгоритм:

\begin{enumerate}[noitemsep]
	\item Выбираем число кластеров
	\item Случайным образом выбираем точки в $n$-мерном пространстве -- центроиды кластеров (или медоиды)
	\item Смотрим, какие точки ближе к какому центроиду
	\item Пересчитываем центроиды (или медоиды), затем повторяем предыдущий шаг
\end{enumerate}

Вопросы:

\begin{enumerate}[noitemsep]
	\item Как выбирать $k$?
	\item Как выбирать критерий качества?
	\item Когда останавливаться в пересчёте центроидов?
\end{enumerate}

Если мы не ограничим число кластеров $k$, лучшее разбиение получится при $k=n$ (каждая точка в своём кластере).

Можно выбирать $k$, подбирая его постепенным увеличением и останавливаться в тот момент, когда критерий качества перестаёт падать сильно.

\subsubsection{Классификация кластеризаций}
% TODO a bit

Каждый алгоритм кластеризации задаёт свою гипотезу того, как организованы исходные данные.

\subsubsection{ Иерархическая кластеризация. Агломеративный алгоритм }

\begin{enumerate}[noitemsep]
	\item Вначале каждая точка
	\item % TODO
\end{enumerate}

Похоже на алгоритм построения по графу минимального остовного поддерева.

Полученное дерево можно отсечь на каком-то уровне.

\subsubsection{C-means}

Похоже на $k-means$.

Вначале выбираем число кластеров и случайным образом
Затем рассчитываем принадлежность точек кластерам (но не единственный кластер для точки, а вес каждого кластера).
Потом пересчитываем принадлежности точек кластерам (то есть пересчитываем веса).

\subsubsection{ Affinity Propagation }

Этот алгоритм довольно хорош, но мы его не будем рассматривать.
Можно почитать статьи.

\subsubsection{DBScan}

Считаем, что кластеры -- это группы объектов, расположенные кучно.



Вначале все объекты нерассмотренные.
Пока не рассмотрим все объекты, выбираем произвольный

Микропроблема: если
Но мы можем как-то пытаться подбирать радиус окрестности в зависимости от плотности точек вокруг данной (адаптивный радиус).

\subsubsection{} % TODO title slide

Что, кроме близости и связности, может характеризовать качество разбиения графа?

\begin{enumerate}[noitemsep]
	\item число соседних вершин, оказавшихся в разбиении
	\item % TODO slide
\end{enumerate}

Делаем первоначальное разбиение.
Затем несколько раз пробегаемся по всем имеющимся вершинам и перемещаем объекты так, чтобы минимизировалась целевая функция.
Иногда можно изменять число кластеров (добавлять новые или удалять имеющиеся).

\subsubsection{MCL. Идеи}

Можем случайно гулять по графу (почти как в марковской модели).
Вероятнее всего, мы во время прогулки будем находиться всё время внутри одного кластера, пока не обойдём все его точки.
Края между кластерами, вероятно, будут на многих кратчайших путях между точками.

Если умножить матрицу смежности $A$ на саму себя, получим все возможные пути длины 2 в данном графе.
Матрица $A^k$ -- веса путей длины $k$.

\subsubsection{MCL. Проблема self-loops}

Что ставить на диагоналях матрицы смежности $A$?

\begin{enumerate}[noitemsep]
	\item Нули нелогично: возникает

	\item Бесконечность: если так, то мы никогда из вершины не уйдём.

	\item Единицы: идеально для невзвешенных графов.
	\item Для взвешенных графов не так просто.
\end{enumerate}


Веса self-loops пропорциональны вероятностям, с которыми мы останемся в том

Проблема: при возведении матрицы $A$ в большую степень уходим в область больших чисел.
Можно нормировать: перейти от матрицы смежности $A$ к матрице вероятностей переходов $T$ (где сумма вероятностей перехода из данного узла во все остальные равна единице).
Это уже совсем похоже на марковскую модель.

Идея: нормировать с увеличением контраста (возвести каждый вес в степень $r$ -- т. н. inflation, а затем нормировать).

\subsubsection{MCL. Алгоритм}

Вход: матрица смежности $A$, параметры $e$ (длина пути, точнее, инкремент длины на каждом шаге расширения кластеров), $r$ -- коэффициент инфляции.

На первом шаге нормализуем матрицу $A$ (переходим от весов к вероятностям).

Пока не выполено условие останова (например, $ T \ne T^2 $, но этого не всегда удаётся достичь),
возводим $T$ в степень $e$: $ T \leftarrow T^e $, а затем делаем инфляцию: $ T \leftarrow inflate(T, r) $. \\

Кроме параметров $e$, $r$ может быть ещё $ \varepsilon $: если вес элемента меньше $\varepsilon$, вместо веса ставим ноль.
Тогда в качестве критерия останова можно брать число нулей в матрице.

\subsubsection{RRW (Repeated Random Walk)}

% TODO a bit

\subsubsection{Выбор расстояния}

Простой случай: есть несколько независимых величин, масштаб не играет роли.
Тогда не очень важны метрики расстояния.

В общем случае надо поместить точки в какое-то пространство или в какой-то граф.
Граф может быть полным, если все расстояния известны, или неполным.

Нужно учитывать связи между данными (например, один и тот же день недели).

\subsubsection{Случай временного ряда}

Временой ряд: есть последовательности чисел -- отчёты в моменты времени.
Как разделить их?

Первая идея: посчитать какие-то статистики (моменты, квантили и т. д.) и по ним разделить.
Но это не позволяет нам заметить возрастания, убывания...
(не учитываем упорядоченность во времени)

Можно попробовать приближать отдельные отрезки, например, линейной регрессией.

Иногда имеет смысл квантовать временные ряды в атомы (например, в речи есть отдельные звуки).

\subsubsection{Динамическая трансформация временной шкалы}

Хотим найти парные пики и спады.

Алгоритм DTW:

% TODO

Есть много аналогов этого алгоритма.

\subsubsection{EM-алгоритм}

Мы его не рассматриваем.

\end{document}
