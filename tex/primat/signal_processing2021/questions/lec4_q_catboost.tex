% compile with XeLaTeX or LuaLaTeX

\documentclass[a4paper,12pt]{article}
\usepackage[utf8]{inputenc} % russian, do not change
\usepackage[T2A, T1]{fontenc} % russian, do not change
\usepackage[english, russian]{babel} % russian, do not change

% fonts
\usepackage{fontspec} % different fonts
\setmainfont{Times New Roman}
\usepackage{setspace,amsmath}
\usepackage{amssymb} %common math symbols
\usepackage{dsfont}

% utilities
\usepackage{amsthm} % theorems with proofs
\usepackage{systeme} % systems of equations
\usepackage{mathtools} % xRightarrow, xrightleftharpoons, etc
\usepackage{array} % utils for tables
\usepackage{makecell} % multirow for tables
\usepackage{subfiles}
\usepackage{hyperref}
\hypersetup{pdfstartview=FitH,  linkcolor=blue, urlcolor=blue, colorlinks=true}
\usepackage{framed} % advanced frames, boxes
\usepackage{graphicx}
\usepackage{caption}
\usepackage{subcaption} % captions for subfigures
\usepackage{color}
\usepackage{listings} % code chunks
\definecolor{dkgreen}{rgb}{0,0.6,0}
\lstset{language=Python,
    basicstyle=\small\ttfamily,
    stringstyle=\color{dkgreen},
    keywordstyle=\color{blue},
    commentstyle=\color{dkgreen},
    tabsize=2,
    breaklines=true,
    columns=fullflexible
}

% styling
\usepackage{float} % force pictures position
\floatstyle{plaintop} % force caption on top
\usepackage{enumitem} % itemize and enumerate with [noitemsep]
\setlength{\parindent}{0pt} % no indents!

% misc
\graphicspath{{./img/}}
\newcommand{\myPictWidth}{.95\textwidth}
\newcommand{\phm}{\phantom{-}}
\newtheorem{problem}{Задача}

\begin{document}

\title{CatBoost -- градиентный бустинг от Яндекса. Конспект лекции Анны Вероники Дорогуш}

\author{Валерий Зуев}

\maketitle

\section{Введение}

CatBoost -- библиотека с открытым исходным кодом, выложена в GitHub; 2.5k скачиваний с PyPi, библиотека развивается изнутри и даже кто-то коммитил извне.

\subsection{Дерево решений}
В первой вершине есть вопрос (требуется ответ в форме да/нет: как правило, числовой фактор и сравнение на $\glq$). Если ответ <<да>>, идём в одну сторону, <<нет>> -- в другую.
В следующих вершинах тоже могут быть вопросы, отвечая на них

Градиентный бустинг строит ансамбли деревьев.
Сперва есть одно дерево, затем добавляем ещё одно так, чтобы уменьшилась ошибка на обучающей выборке...
Так строим сотни, тысячи или даже десятки тысяч деревьев, и в итоге уже получается что-то, что хорошо ведёт себя на обучающих данных.

\subsection{Режимы}

\subsubsection{Какие задачи можно решать градиентным бустингом в CatBoost?}

\begin{enumerate}[noitemsep]
    \item Самое простое -- \textbf{регрессия}: есть данные, есть метки и хотим приблизить метки.
    Функция потерь -- MSE (mean square error).
    \item \textbf{Классификация} -- немного посложнее.
    Рассмотрим пример бинарной классификации.
    Хотим, чтобы ответ модели можно было отождествить с вероятностью попадания в класс, то есть ждём число от 0 до 1.

    Как заставить большой ансамбль деревьев давать ответ в отрезке от 0 до 1?
    Максимизируем вероятность того, что все объекты в обучающей выборке распознаны правильно.
    В качестве вероятности используем сигмоиду от значения классификационной формулы.

    \begin{leftbar}
        Если при использовании CatBoost для классификации использовать функцию \texttt{predict}, получится не вероятность, и тогда, к примеру, можно сладывать выходы от разных моделей.
        Если \texttt{predict\_proba} -- будет взята сигма-функция, и на выходе вероятность.
    \end{leftbar}
    С мультиклассификацией почти то же, только в листьях деревьев не 0/1, а несколько чисел (меток классов), они учатся одновременно.
    Соответственно, надо на выходе получить не одну вероятность попадания в позитивный класс, а вероятности всех классов; поэтому вместо сигмоиды берём softmax-функцию (многомерное обобщение сигмоиды).

    \item \textbf{Ранжирование:} есть ранжирование и попарная классификация.
    \textit{Попарная классификация:} есть датасет (набор объектов + для каждого набор признаков). Их нужно поставить в одну очередь. Для каждой пары объектов мы знаем, какой элемент пары лучше бы поставить в очереди прежде другого.
    Т. о. задача алгоритма -- подобрать такую формулу, чтобы ранжировать пары как можно лучше.
    Максимизируем вероятность того, что как можно больше пар ранжированы в правильном порядке: берём сигмоиду от разностей значений формул.

    \textit{Ранжирование:} есть датасет, разбитый по группам.
    Нужно как можно лучше ранжировать каждую группу.

    Как правило, в задаче ранжирования в каждой группе составляют свой набор пар и стараются составить очередь так, чтобы как можно больше пар шли в правильном порядке.

    Замечание: при ранжировании нам совершенно неважно абсолютное значение формулы, т. о. мы не можем, к примеру, сравнивать значение с некоторым допустимым порогом и на основе этого делать выводы.
    Важно только то, как объекты друг с другом сравнивать.
\end{enumerate}

\subsubsection{Оптимизируемая функция и метрики}

Оптимизируемая функция всегда одна, а метрик может быть много, и за ними можно следить во время обучения.
Например, при бинарной классификации можно максимизировать logloss, но при этом ещё смотреть на значение AUC и <<обрезать>> формулу по лучшему значению AUC.
По одним метрикам можно сделать overfitting detector, по другим -- обрезать формулу; в целом можно просто взять много метрик и построить их графики в зависимости от итерации

\subsection{CatBoost Viewer}

Очень полезно пользоваться визуализацией.
\begin{enumerate}[noitemsep]
    \item в Jupyter Notebook (Python) можно в функцию \texttt{fit} передать флаг \texttt{plot=true}, то построится график.
    При этом можно сравнивать несколько моделей.
    \item в R нет встроенной визуализации, но тот же инструмент (CatBoost Viewer) всегда можно использовать в браузере.
    \item в TensorBoard тоже можно смотреть на те же графики.
\end{enumerate}

\section{Обучение модели. Общие сведения}

\subsection{Схема обучения}

Сначала строим дерево, затем считаем значения в листьях (обычно считают значения в листьях при построении, но в CatBoost иначе).

Дерево строится \textit{жадно}: сначала выбираем первую вершину дерева; делаем вид, что дерево состоит только из этой вершины и перебираем все возможные деревья из одной вершины.
Каждому одновершинному дереву присваиваем score и выбираем лучшее.
Затем добавляем вторую вершину и так далее. \\

Краткий обзор алгоритмов добавления вершин в разных библиотеках:
\begin{itemize}[noitemsep]
    \item LightGBM (Light Gradient Boosting Machine) делает так: строит деревья в глубину (сначала одна, потом вторая вершина...).
    Могут получиться чересчур глубокие, <<однобокие>> деревья.
    \item XGBoost строит деревья по слоям: спера первый ярус, затем следующий (впрочем, не обязательно заполняются все вершины, дерево может быть и несимметричным).
    После построения делается pruning (обрезка деревьев).
    \item CatBoost: деревья тоже строятся по слоям, но задано ограничение: во всех узлах на одном уровне производится сравнение по одному и тому же признаку.
\end{itemize}

\subsection{Score разветвления}

Как в CatBoost выбирается лучшее ветвление?

Есть несколько способов оценки разделения.

\begin{enumerate}[noitemsep]
    \item Строим дерево жадным способом; считаем значение в листьях и оцениваем, насколько оно хорошее.
    Например, смотрим, насколько меняется функция ошибки по сравнению с предыдущим вариантом дерева.
    \item Будем строить дерево так, чтобы оно как можно лучше вектора градиента.

    Отступление.
    Как работает градиентный бустинг?
    Есть $ n $ объектов (например, документов), есть функция ошибки от $ n $ переменных, где каждая переменная -- значение ответа ансамбля на соответствующем входе.
    Градиент этой функции есть  $ n $-мерный вектор.

    Листьев в дереве наверняка меньше, чем объектов в обучающей выборке (например, при глубине дерева $ 6 $ будет не более $ 64 $ листа).
    Т. о. мы не можем спускаться прямо по градиенту.
    Зато можем смотреть, насколько вектор значений в листьях похож на вектор градиента (и на основе этого строить score):

    $$ score(split) = \frac{\sum_{doc}leafValue(doc) \cdot gradient(doc) \cdot w(doc)}{\sqrt{\sum_{doc} w(doc) \cdot leafValue(doc)^2}} $$
    $$ leafValue(doc) = \frac{sumWeightedDer}{sumWeights} $$

    где $ w(doc) $ -- вес элемента в обучающей выборке.

    Перебираем все возможные разветвления и смотрим, какое даст наибольший score.

\end{enumerate}

\subsection{Bootstrap}

Можно обучаться не на всей выборке одинаково, а выкинуть часть объектов или назначить вес $w$.

Сейчас в CatBoost реализованы два метода для бутстрапа:

\begin{enumerate}[noitemsep]
    \item Бернулли: каждый элемент выборки берём с вероятностью $ sample\_rate $ (это параметр алгоритма).
    $$ w(doc) = 0 or 1 (p(1) = sample\_rate) $$
    \item Байесовский (мы его так называем):
    на каждой итерации добавка к весу элементов сэмплируется из экспоненциального распределения
    $$ w(doc) *= (-log(rand(0,1)))^{bagging\_temperature} $$

    Этот способ хорош тем, что веса могут меняться во всём диапазоне от $0$ до $+ \infty$, но средний будет равен единице.

    Параметр $sample\_rate$ влияет на интенсивность сэмплирования (и на качество).
    Такой способ
\end{enumerate}

Замечание: бутстрап используется только на этапе выбора структуры дерева.
Когда считаем значения в листьях, используем всю выборку целиком.

В других алгоритмах так не делается.

\subsection{Рандомизация score}

Ещё одна вещь, которая делается только в CatBoost:
при выборе лучшего разветвления к score прибавляются случайные величины, зависящие от номера итерации и длины вектора градиента.

$$ score(f) += random\_strength \cdot \mathcal{N}(0, RndMult \cdot Sko), \thickspace Sko \text{ --  длина вектора градиента} $$

Смысл -- ближе к концу обучения хотим меньше случайных факторов, вначале можно побольше.

\section{Работа с факторами. Бинаризация}
Ещё одна важная вещь -- бинаризация факторов.
\subsection{Бинаризация численных признаков}

Представим, что есть числовой фактор, который может принимать разные значения.
Нужно выбрать какое-то разбиение на два множества (идём направо в узле дерева или идём налево).

В XGBoost есть два способа бинаризации.
Одна просто перебирает возможные пороги (это долго), есть версия с гистограммами.

На каждой итерации в XGBoost выбирается новая сетка порогов, что трудозатратно и бесполезно.
В CatBoost сетка выбирается заранее; эксперименты показали, что это не ухудшает качество. \\

Способы построения сетки:

\begin{enumerate}[noitemsep]
    \item Берём значения признаков в обучающем наборе и равномерно строим сколько-то границ от минимального до максимального значения
    \item Медианная сетка: между двумя соседними границами одинаковое количество точек (например, проходим 5 точек, ставим границу сетки; проходим ещё 5 и ставим вторую и так далее).
    Проблема: если в одну точку попадает множество элементов данных, получим меньше границ, чем запросили.
    \item UniformAndQuantiles: половину границ строим равномерным образом, половину -- из медианной сетки.
    \item MaxSumLog / GreedyLogSum

    Вспомним формулу:
    $$ \sqrt[n]{\prod w_i} \le \frac{\sum w_i}{n} \le \sqrt{\frac{\sum w_i^2}{n}} $$
    где $w_i$ -- то, сколько точек попало в интервал разбиения сеткой.

    Хотим максимизировать равномерность разбиения, иначе, достичь в неравенстве равенства. Для этого можем максимизировать левую часть.
    Это то же, что максимизировать $ \sum \log(w_i) $.

    MaxSumLog строит точное оптимальное приближение; GreedyLogSum -- жадное приближение, работает не всегда оптимально, но быстро (поэтому стоит в CatBoost по умолчанию).
\end{enumerate}

\subsection{Сравнение бинаризаций}

Ещё можно при построении сетки попробовать минимизировать не сумму логарифмов, а сумму квадратов, но возникает проблема -- большие корзины штрафуются сильно, к ним ничего не будет присоединяться.
Например, если есть четыре точки, в них $10, 10, 1000, 1$ элементов выборки соответственно, то будет обязательно поставлен порог между $1000$ и $1$, т. о. в одной из корзин будет единственный элемент, что плохо.

\subsection{Бинаризация счётчиков}

\begin{enumerate}[noitemsep]
    \item на CPU -- равномерная (это быстрее считать)
    \item на GPU -- любая
\end{enumerate}

\subsection{Вычисление значений в листьях}

Почему значения в листьях вычисляются не во время построения дерева, а отдельно?
Когда выбираем split, надо сделать очень много операций (для каждого разбиения вычислить дерево и его качество).
Поэтому стараемся делать это наиболее эффективным способом (градиентный спуск).

Когда считаем значения в листьях, уже больше времени.
Поэтому можем делать шаги по Ньютону (дороже по времени, но лучше приближение).

Внутри одного дерева можно делать несколько шагов по градиенту или методом Ньютона! (посчитали значение градиента, пересчитали значение в листьях, после чего пересчитываем значение формулы -- и так несколько раз, можно шагать по градиенту, можно методом Ньютона). Идея была ещё в MatrixNet.

\subsection{Работа с категориальными факторами}

С числовыми факторами примерно понятно, как делать бинаризацию (сравниваем с пороговым значением).
Если признак категориальный, возможны несколько стратегий.
\begin{enumerate}[noitemsep]
    \item Перенумеровать факторы (дать им номера от $0$ до $k$) и использовать как числовой фактор.
    Это плохо работает.
    \item One-hot encoding: если был фактор, например, <<профессия>> со значениями <<врач>>, <<инженер>> и <<программист>>, то теперь будет три фактора (три ветвления): <<является ли человек врачом>>, <<является ли человек программистом>>, <<является ли инженером>>.
    Хорошо работает, если значений признака мало.
    \item Хэширование в несколько корзин: уменьшение числа значений признаков путём объединения в группы (затем, к примеру, one-hot encoding).

    Тоже никогда не работает!
    Гораздо лучше сделать топ-сколько-то значений признаков и с ними работать.
\end{enumerate}

Это самые простые способы. Есть и более продвинутые, о них дальше идёт речь.

\subsection{Статистики по категориальным признакам}

TODO?

\end{document}