\documentclass[main.tex]{subfiles}
\begin{document}
\section{Постановка задачи}
Найти оценки коэффициентов линейной регрессии $y= a + bx$ для набора данных $\{y_i\} = \{2 + 2x_i + e_i\}$, где $x_i$ -- $20$ точек из интервала $[-1.8;2]$ с шагом $0.2$, $e_i ~ N(0,1)$ двумя способами:
\begin{enumerate}
	\item Используя метод наименьших квадратов (МНК)
	\item Используя метод наименьших модулей (МНМ).
\end{enumerate}
Проделать то же самое для выборки, в которой значения $y_1$, $y_{20}$ претерпевают дополнительные возмущения $10$ и $-10$ соответственно.
\newpage
\section{Теория}
\emph{Метод наименьших квадратов} \cite{sevastianov} строит прямую $f(x) = a + bx$, аппроксимирующую выборку данных $\{(x_i;y_i)\}_{i=1}^{n}$ так, чтобы сумма квадратов отклонений $y_i$ от $f(x_i)$ принимала минимально возможное значение. Из этого условия можно вывести формулы для выражения тангенса угла наклона прямой и свободного члена:
\begin{gather}
	\label{eq:least_sq_b}
	b = \frac{\overline{xy} - \overline{x}\cdot\overline{y}}{\overline{x^2}-(\overline{x})^2}\\
	\label{eq:least_sq_a}
	a = \overline{y}-b\overline{x}
\end{gather}
Оказывается, что при наличии в данных редких, но больших по величине выбросов, данные более качественно описываются линейной регрессией, построенной по критерию минимизации не квадрата, а модуля отклонения (\emph{метод наименьших модулей}) \cite{sevastianov}. Для этого метода не так просто найти аналитические выражения коэффициентов линейной регрессии, но можно составить их оценки:
\begin{gather}
\label{eq:least_mod_b}
b = r_Q \frac{q_y}{q_x}\\
\label{eq:least_mod_a}
a = med(y)-b med(x)
\end{gather}
где $r_Q=\frac{1}{n} \sum_{i=1}^{n} sign(x_i - med(x))sign(y_i-med(y))$, $n$ -- мощность выборки;
\begin{equation}\label{eq:interquartile}
	q_x = \frac{x_{(\lceil\frac{3n}{4}\rceil)} - x_{(\lceil\frac{n}{4}\rceil)}}{k_q(n)}
\end{equation}
$k_q(n)$ -- некоторый коэффициент, зависящий от числа элементов (известно, что $k_q(20)\approx 1.491$).
 
\section{Реализация}
Программа, вычисляющая коэффициенты регрессии и выполняющая построение изображений, написана на языке MATLAB. Массив данных был сгенерирован с использованием встроенной функции генерации нормально распределённых случайных чисел $randn$. Коэффициенты линейной регрессии были вычислены по формулам \eqref{eq:least_sq_b}, \eqref{eq:least_sq_a}, \eqref{eq:least_mod_b} и \eqref{eq:least_mod_a}. Для вычисления \eqref{eq:interquartile} составлена отдельная функция. 

\newpage
\section{Результаты}

\begin{figure}[H]
	\centering \includegraphics[width=\myPictWidth]{normal.jpg}
	\caption{Модели линейной регрессии (нормально распределённая ошибка)}
	\label{img:normal}
\end{figure}
\begin{figure}[H]
	\centering \includegraphics[width=\myPictWidth]{disturbed.jpg}
	\caption{Модели линейной регрессии (возмущения в первом и последнем элементе)}
	\label{img:disturbed}
\end{figure}

Данные, изначальная модель $y=2+2x$ и построенные регрессионные оценки изображены на рис. \ref{img:normal} и \ref{img:disturbed}. В таблице \ref{table} приведены коэффициенты.

\begin{table}[H]
	\centering
	\caption{Коэффициенты линейной регрессии}
	\begin{tabular}{*5r}
		\toprule
		\multirow{2}{*}{модель} & \multicolumn{2}{c}{норм. $e_i$}  & \multicolumn{2}{c}{с возмущ.} \\
		& $a$ & $b$ & $a$ & $b$ \\
		\midrule
		Исходная & $2.0$ & $2.0$ & $2.0$ & $2.0$ \\
		МНК      & $2.2$ & $2.6$ & $0.8$ & $2.8$ \\
		МНМ      & $1.3$ & $3.6$ & $0.9$ & $3.6$ \\
		\bottomrule
	\end{tabular}
	\label{table}
\end{table}

\section{Обсуждение}
Метод наименьших модулей даёт оценки, более устойчивые к редким выбросам (\emph{робастные}), поскольку вклад отдельной точки в минимизируемую целевую функцию зависит от отклонения этой точки линейно, а не квадратично. Это заметно по таблице \ref{table}: тангенс угла наклона прямой, рассчитанный по методу наименьших модулей, остался неизменным при внесении возмущений, а по методу наименьших квадратов -- возрос почти на $10\%$; свободный член тоже меняется не так сильно, как при оценке по методу наименьших квадратов. Впрочем, приближение МНМ более грубое и даже с возмущениями угол наклона дальше от <<истинного>> угла, который использовался при генерации точек, чем угол наклона, полученный МНК.\\
Можно попробовать для каких-то конкретных задач подобрать средний подход между этими двумя, минимизируя сумму отклонений $\sum(|y_i - f(x_i)|)^p$, $1 \le p \le 2$, но здесь уже будет сложнее вывести оценки коэффициентов, и, наверное, целевую функцию придётся минимизировать численными методами.

\end{document}