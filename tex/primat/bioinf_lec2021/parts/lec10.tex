\documentclass[main.tex]{subfiles}
\begin{document}
	
\section{ Лекция 10 (Георгий Мещеряков).  }

1 апр 2021 г.

\subsection{ Конец того, что недорассказали в прошлый раз по Manifold Learning }

Метод ivis -- нейросеть с триплетной функцией цели.

Триплетная функция цели:
Пусть мы классифицируем картинки (абстрагируемся от задачи понижения размерности).
One-Shot Leraning: минимизируем не просто расстояние между предсказанными метками и предсказанными.
\[ max( d(a,p) + d(a, n) + a) \]

$ d $ -- функция расстояния, $ n $ -- негативный и $ p $ -- позитивный пример (любые примеры из позитивного / негативного класса), $ a $ -- классифицируемая картинка (\emph{ якорь }).

\subsection{ Основы вычислительной алгебры. Линейные модели }

В слайдах лекции будут слова, выделенные \textbf{ красным }.
Это вопрос.
Человек, набирающий три балла по итогам вопросов со слайдов, может отказаться от любого ДЗ из четырёх.

\subsubsection{ Основные опирации с матрицами }

Матрицы можно складывать, перемножать, умножать на константу, транспонировать, обращать.
Можно вычислять определитель (сумму всех возможных произведений перестановок элементов), след, и можно \emph{ произведение Кронекера }.

% TODO +1 point
\[ A \times B = \begin{pmatrix}
TODO + TODO_circled_times
\end{pmatrix} \]

Можно брать производную от матрицы по матрице.

Производная от матрицы по вектору:
$$ \frac{\partial z}{\partial x} = \begin{pmatrix}
TODO
\end{pmatrix} $$

% TODO можно написать на почту Георгию решение МНК-задачки через производные

\subsubsection{ Производная следа }

\[ \frac{\partial}{\partial X} tr(F(X)) = f(X)^T, \thickspace f = F' \]
\[ \frac{\partial}{\partial X} tr(XA) = A^T \]
\[ \frac{\partial}{\partial X} tr(X^T A) = A \]
\[ \frac{\partial}{\partial X} tr(AX^T B) = AB \]

В последнем случае мы использовали свойство -- циклический сдвиг:
\[ tr(ABCD) = tr(DABC) = tr(CDAB) = tr(BCDA) \]

\[ \frac{ \partial }{ \partial X } tr(X^T AX) = AX + AX^T \]

\subsubsection{ ММП: многомерное нормальное распределение }

Распределение Уишерта -- распределение положительно определённых матриц, где каждый элемент распределён нормально.

Трюк: след произведения двух матриц $ n \times n $ можно посчитать за $ O(n^2) $

\subsubsection{ Разложение Холецкого }

Треугольную матрицу можно обратить за $ n^2 $.
Определитель треугольной матрицы равен произведению диагональных элементов (сложность $ O(n) $).

\subsubsection{ Разложение по собственным числам (Eigendecomposition) }

Собственные (сингулярные) числа

\[ A = Q \Lambda Q^{-1} \]

Докажем, что $ A = A^T \Rightarrow A  = Q \Lambda Q^T $

\subsection{ Разложение по сингулярным числам (SVD) }

SVD -- обобщение разложения по собственным числам на прямоугольные матрицы.

Пусть $ A $ -- матрица размера $ n \times m $,

$ A^T A $ % TODO

Докажем, что у $ AB $ и $ BA $ одинаковые собственные числа.

\[ ABx = \lambda x \]
\[ BABx = \lambda Bx \]
\[ BA(Bx) = \lambda (Bx) \]

\subsubsection{Применение SVD / Eigendecomposition}

\begin{enumerate}[noitemsep]
	\item PCA
	\item Аппроксимация матрицей меньшего ранга
	
	Ранг матрицы $ \sim $ количество информации в этой матрице
\end{enumerate}

\subsection{ GWAS }

Полногеномный поиск ассоциаций:

визуализация -- ManhattanPlot:
по горизонтали -- хромосома (от 1 до 12), по вертикали -- $ - \ln $ от статзначимости.

Поправка на множественное тестирование:
через каждые

\subsubsection{STSL GWAS}
Single Trait Single Locus GWAS -- просто линейная регрессия.

\textbf{ Тестирование гипотезы H0 для ММП }

Оценки ММП распределены нормально.
% TODO

\subsubsection{ Случайный эффекты }

Ковариация -- косинус угла между наблюдениями.
% TODO

$ K $ -- мощный инструмент!
Например, мы можем считать, что элементы $ K $ задают расстояние между элементами.
\[ g(i,j) = \frac{1}{ 1 - d(i,j) } \]

\subsubsection{ Multi Trait Single Locus }

Можно анализировать множество фенотипов сразу и дополнительно анализировать ковариацию, т. к. фенотипы могут быть скоррелированы.
Тогда $ Y $ -- уже матрица $ n \times m $, $ B = B(1 \times m) $, $ E = E(n\times m) $ -- случайная матрица;
\[ Y = g_{(k)}B + E \]

% TODO

Упрощаем полученную формулу для $ D $ $ \Rightarrow $ получаем 

% TODO

\subsubsection{ REML }

Restricted Maximum Likelihood: ... % TODO
есть несколько способов выбрать $ P $.
Можно выбрать так, что это ортогональная матрица (что нам может пригодиться).

\subsubsection{ (ST/MT) Multi Locus }

Почему не можем вместо одного снипа $ g_{k} $ взять сразу все SNP?
Снипов настолько много, что вариативность слишком большая и ими можно объяснить любые данные $ X $.

\subsubsection{ SEM }

Structured Equation Model -- обобщение линейной модели.
Там могут присутствовать и латентные переменные...
Латентные переменные <<высасывают>> дисперсию из переменных.

\subsection{ Задание (ДЗ 2) }

Вывести плотность распределения

\subsection{ ДЗ 3 }

Запрограммировать STSL-модель

\subsection{ ДЗ 4 }

То же, но более 1 переменной

ДЗ отправлять на почту: homework@georgy.top

През

\textbf{ }

\end{document}
