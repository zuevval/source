\documentclass[main.tex]{subfiles}
\begin{document}
\section{  }
\subsection{Категориальные данные}
	
\subsubsection{Вероятностная модель эксперимента}

Наблюдение -- вектор $ T = (T_1, \dots, T_r) $

Каждая наблюдаемая переменная имеет конечное число уровней.
Переменные могут быть как ординальными, так и категориальными.
Могут быть количественными группированными (что делает их ординальными).
Не умаляя общности, считаем $ T_j \in \{ 1, \dots, d_j \}, j = 1;r $.

Вероятностная модель: полностью характеризуется совместным распределнием 

(в совместном распределении нулевые вероятности могут иметь смысл)

% TODO a bit

Параметр $ \Theta $

\begin{leftbar}
	На слайдах опечатка, размерность параметра $ d_1 \cdot \dots \cdot d_r - 1 $
\end{leftbar}

\subsubsection{Получения и редукция данных}

Выборка - <<безотказный способ накопления данных>>.
Но всё дело в реальных задачах в размерности параметра.

Обычно ограничиваются небольшим количеством наблюдаемых характеристик (2-3) и для них делают статистический анализ.
Что касается основной конструкции, на основе которой мы делаем статистические выводы: помним, что полезна \emph{функция правдоподобия}, которая в силу леммы Неймана-Фишера несёт всю информацию о параметре, притом это функция от достаточной статистики.

Выборка $ (T_1, \cdot, T_r) $ (здесь нижние индексы отвечают векторам, а не элементам векторов, как в предыдущем разделе).

% TODO a bit
Мультииндекс

Набор $ n_{i_1 \dots i_r} $, как видим, является достаточной статистикой.
Более того, это минимальная достаточная статистика.
Такой набор образует \emph{массив сопряжённости} признаков размерности $ d_1 \times \dots \times d_n $

\subsubsection{Мультиномиальное распределение и его свойства}

\[ \nu = (\nu_1, \dots, \nu_m) \in Mult(p_1, \dots, p_m;n), m,n \in \mathds N \]

В нашем случае $ m = \prod_k p_{i_k} $

Параметры распределения -- вероятности, которые должны удовлетворять условию $ p_i \ge 0 $.
Ничто не мешает включить сюда и нулевые вероятности, но есть ограничение: $ \sum_{i=1}^{m} p_i =1 $

Одни и те же значения $ \{ \nu_i \} $ могут получаться разныим реализациями выборки (элементы могут поступать в различном порядке).
Поэтому появляется мультиномиальный коэффициент
$ \frac{n!}{TODO} $

Свойства мультиномиального распределения:
\begin{enumerate}
	\item Инвариантность (переставляем аргументы -- получаем такое же мультиномиальное распределение, но вероятности тоже меняются местами).
	\item Объединение уровней:
	\[ (\nu_1, \dots, \nu_{m-2}, \nu_{m-1}+\nu_m) \in Mult(p_1, \dots, p_{m-2}, p_{m-1}+p_m) \]
	
	\item Оценки МП для параметров совпадают с частотами встреч элементов в выборке
	\item Можно говорить об асимптотической нормальности.
	Вопрос о регулятности здесь не стоит.
	
	Но есть проблема: матрица ковариации предельного распределения вырожденная (что неудивительно, само распределение вырожденное).
	
	Известно, что, если вероятности неотрицательные, то ранг матрицы есть размерность параметра  $ -1 $. % TODO a bit
	
	\item Критерий $ \chi^2 $: строится квадратичная форма; находится матрица смежности % TODO a bit
	
	Это простой хи-квадрат.
\end{enumerate}

\subsubsection{Двумерные таблицы сопряжённости}

Двумерную таблицу сопряжённости можно нарисовать.
Элементы -- ненормированные частоты.
Такие популярны в биомедицинских исследованиях.

Мы не говорили об условных распределениях.
Если признаков большое количество, % TODO a bit

Сама таблица сопряжённости -- это $ X \times Y $, а последний столбец и строка содержат сумму.
Параметр модели -- матрица $ d_1 \times d_2 $.

Можем ввести параметры одномерных мультиномиальных распределений

% TODO ask: одномерные распределения путём сложений?

Также можем зафиксировать одну из компонент и рассматривать условные распределения.
В данном случае фиксируется значение $ X $ и рассматривается распределение $ Y $ при каждом выбранном значении.

В условном распределении величины тоже имеют мультиномиальное распределение (получается всего).

% TODO do later

Гипотеза независимости признаков $ p_{i|j}=p_{i|1} $ (условие не даёт никакой информации).

\subsubsection{Планы эксперимента с двумя признаками}

Два подхода: активный и пассивный.
При пассивном можно использовать совместное распределение признаков или условное распределение.

В активном подходе, когда одна из величин наблюдаемая и другая контролируемая, 

\subsubsection{TODO}

\subsubsection{Таблицы сопряжённости 2 x 2}

... относительный риск может быть более естественным.
Но если нас интересует именно отношение вероятностей того, что признак присутствует...
естественной характеристикой является \emph{отношение шансов} $ \frac{\pi_1}{\pi_0} $, которое обеспечивает некую симметрию.

Наиболее часто используется отношение шансов, но относительный риск и <...> -- тоже часто встречаются.

\end{document}