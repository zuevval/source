\documentclass[main.tex]{subfiles}
\begin{document}
\section{ Лекция 23 ноября }

Пуассоновская модель не всегда хорошо согласуется с имеющимися данными.
Жёсткое ограничение: МО и дисперсия распределения Пуассона одинаковы.
Это может значительно нарушаться при работе с реальными данными, и может получиться, что мат. ожидание и дисперсия различаются на порядок.
Надо искать достаточно гибкое семейство распределений, г

Один из путей -- использование Conway-Maxwell-Poisson Distribution.
Его можно использовать как в случае недостаточной, так и в случае избыточной дисперсии.

Функция правдоподобия представляется в следующем виде, достаточно сложном:
\[ L(y; \lambda; \nu) = \frac{\lambda^y}{C(\lambda,\nu)(y!)^\nu} = \exp(y \log \lambda - \log C (\lambda, \nu) - \nu \log (y!)) \]

Также можно вспомнить геометрическое распределение и отрицательное биномиальное.
Отрицательное биномиальное, вообще говоря, к экспоненциальным семействам не относится.

Геометрическое распределение есть частный случай биномиального.

При $ \alpha \to \infty $ получаем распределение Пуассона с параметром $ \lambda $.

Компоненты информационной матрицы % TODO a bit

\subsection{Выбор оптимальной модели}

Излишнее упрощение может не позволить достаточно точно описать эксперимент и выводы будут сделаны в далёких от реальности предположениях.
Если же модель слишком сложна, никакие статистические выводы будет сделать невозможно.

В моделях линейной и обобщённой линейной регрессии логично ставить линейные гипотезы.

\subsubsection{Метод исключения и включения параметров}

Иногда... % TODO a bit

Метод исключения: исследование начинается с наиболее общей модели (saturated) \\

Надо придерживаться некоторого порядка.
Если полиномиальная модель, нет смысла рассматривать квадратичную модель без линейного члена.

Речь о поправке не идёт.
Надо просто дойти до определённого уровня и остановиться, оставив только значимые гипотезы.

Метод включения:

Исследование начинается с наиболее простой модели intercept only.

% TODO a bit

Есть комбинированные пути: включается <<с запасом>>, потом исключается...
Но это очень субъективный метод, притом это трудно реализовать.

\subsubsection{Информационные критерии}

Довольно трудно доказать, что одна модель лучше другой.
Поэтому были выработаны некоторые критерии оценки.

Наилучшей моделью считается та, у которой критерий принимает наименьшее значение.

Байесовский критерий: принцип тот же, но пенализацию даёт больше 

\section{ Часть 2 лекции 23 ноября 2021 г. Смешанные линейные модели }

Статистическая модель:

Дл одного наблюдения: линейная регрессия

\[ \mathds E (Y|z) = x(z)^T \beta \]

% TODO a bit

Задача точечного оценивания: принципы те же самые.
Предположения: % TODO a bit

Вопрос: какие функции параметра допускают несмещённое оценивание?
Оказывается -- того же вида, что и в обычной линейной регрессии: натянутые на строки матрицы $ X $.

Мешающего параметра нет

\[ Var(\hat \psi ) = C^T (X \sigma^{-1}X^T)^- C \]
где $ A^- $ -- обобщённая обратная матрица ($ A A^- A = A $).

Функция правдоподобия пропорциональна сумме квадратов, поэтому оценка МП будет вычисляться по той же формуле, что и оценка методом наименьших квадратов.

Доверительные эллипсоиды для ДНО-функции параметра строятся по тем же принципам.
Число степеней свободы в числителе есть число столбцов матрицы $ C $.
Знаменателя нет, в отличие от классической модели, поскольку мешающего параметра нет.

В частном случае (одно измерение, скалярная функция параметра)

Можно построить статистику критерия типа Вальда % TODO a bit

\subsection{Методология при неизвестной Sigma}

Если удастся использовать состоятельную оценку $ \Sigma $, можно использовать её для оценки параметра $ \beta $.
Но важно, чтобы оценка $ \hat \Sigma $ была положительно определённой.
Если оценивать матрицу эмпирически, предположение может нарушаться, но с ростом числа наблюдений вероятность того, что $ \hat \Sigma $  будет положительно определена, увеличивается и стремится к $ 1 $.

Оценка $ \hat \psi = C^T \hat \beta $ % TODO a bit

Напомним, мы хотим, чтобы оценка была состоятельной.
Если нет предположений о дисперсиях, состоятельная оценка невозможна.
Надо как-то параметризовать её форму.
Часто форму диктует сама модель. 

Бывает, что матрица ковариации оказывается блочно-диагональной.
Можно говорить о какой-то стационарности по $ \varepsilon $ % TODO ask: what is epsilon?

Если есть 

% TODO a bit

До сих пор есть споры, что лучше: усечённое правдоподобие или просто оценка максимального правдоподобия.
Возможно, зависит от того, какая модель, какие коэффициенты.

Методы построения доверительных эллипсоидов, конечно же, уже асимптотические, а не точные.
Конечно же, для применения этих методов требуется положительная определённость матрицы  $ \hat \Sigma $.

При малом числе наблюдений распределение $ \chi^2 $ может давать слишком оптимистичные оценки, поэтому используют методы оценивания т.н.

Обычно подход Кенворда-Роджера даёт более консервативные оценки (p-values чуть выше), чем Уэлша % TODO not everything cought

\subsection{ Линейная регрессия со случайными параметрами }

Модель формулируется похожим образом:

\[ Y = X^T b + e \]
$ b $,  а не $ \beta $, т.к. случайный вектор.

Условная матрица ковариации 

\begin{leftbar}
	Это уже <<продвинутая>> теория вероятностей.
\end{leftbar}

Оценивание:
предположим, что матрицы ковариации известны.

Надо отметить, что эффект параметром модели не является!
Это случайная величина, нет смысла её оценивать.
Но имеет смысл найти наилучшее несмещённое линейное предсказание при известном $ \mu $ (Best Linear Unbiased Preictor, BLUP).
Это предсказание не получается интерпретировать статистически, но 

% slide 13

\end{document}