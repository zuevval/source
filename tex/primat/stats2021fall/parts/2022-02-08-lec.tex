\documentclass[main.tex]{subfiles}
\begin{document}
\section{Обобщённые смешанные модели}
Эта лекция должна была относиться к прошлому семестру.

В этом семестре мне бы хотелось, чтобы мы рассматривали более специальные задачи, которые мы будем решать как с помощью классических методов, так и с помощью тех, что мы изучали в прошлом семестре.
Далее мы будем заниматься анализом данных типа времени жизни (\emph{я этим занимался долгое время}) -- этому посвящу целый семестр.
Вообще, это редкое направление, поскольку, пожалуй, $ 80\% $ биомедицинских исследований -- анализ таблиц сопряжённости, категориальных данных.
Но я этим занимался долгое время и накопленный опыт хочу передать вам.

В этом семестре мы увидим, как смешанные модели применяются к анализу данных когортных исследований. \\

После этой лекции мы получим лабораторную -- довольно сложную -- по когортным исследованиям.

\subsection{Модель}

Мы рассматривали два подхода: (1) ковариации наблюдений (если распределение нормальное, то зависимость определяется ковариационными характеристиками).
Но часто уместно ввести т.н. \emph{случайные эффекты}, которые позволяют

Мы говорили, что можно смешивать эти два подхода: говорить о независимых ошибках или о коррелированных ошибках.
Логичным кажется рассмотрение обобщённых моделей со случайными эффектами.
Оказывается, это немного сложнее, но всё же некие общие подходы разработаны.

\begin{leftbar}
	Всячески приветствуется задавать вопросы в течение лекции.
\end{leftbar}

Обобщённая смешанная модель формируется аналогично классической.

Когда мы говорили о смешанных моделях, говорили, что \emph{схема независимых блоков} весьма востребована: она подразумевает накопление статистических данных; более общие конструкции, накапливающие данные, построить можно, но это практически не удаётся.
Грубо говоря, рассматриваются независимые блоки (независимые случайные векторы), внутри которых, возможно, есть зависимости.

\begin{itemize}[noitemsep]
	\item $ (Y_1, \dots, Y_k) $ -- независимые случайные векторы, $ Y_i = (Y_{i1}, \dots, Y_{in_i}) $
	\item При каждом фиксированном $ i $ предполагаем справедливость смешанной модели
	\[ g(\mathds E_{\theta} (Y_i | b_i)) = X_i' \beta + Z_i' b_i \]
	
	Здесь $ \beta $ -- регрессор; может быть линейная модель, полиномиальная модель, дисперсионный анализ (простая группировка, или же группировка по нескольким фактрам).
	Второе слагаемое $ Z_i' b_i $ формируется аналогично первому, но $ b_i $ -- случайный вектор,  $ b_i \sim \mathcal N (0, \Upsilon_i(\sigma)) $.
\end{itemize}

Матрицы ковариации разные, потому что векторы в схеме независимых блоков, возможно, разного размера.
Подчеркнём, что наблюдения не считаем нормально распределёнными, но параметры $ b_i $ -- считаем.

\begin{leftbar}
	Мы рассматриваем асимптотическую модель.
	Если бы взяли $ \beta $ разный для разного $ i $, то размерность параметра с ростом числа наблюдений стала бы меняться!!
\end{leftbar}

\begin{leftbar}
	Не стоит переоценивать методы моделирования. % TODO посмот
\end{leftbar}

Сигма ($ \sigma: \Upsilon_i = \Upsilon_i(\sigma) $) -- параметр, который позволяет реализовать асимптотическую схему накопления информации.
Можно считать его мешающим параметром.

Величины $ Y_i $ условно независимы при каждом выбранном $ b_i $.
Будем считать, что $ Y_{ij} $ принадлежит некоторому экспоненциальному семейству (обычно регулярному).
В прошлом семестре мы рассматривали однопараметрические экспоненциальные семейства.
Скоро рассмотрим и многопараметрическое. \\

Плотности (или дискретные плотности) распределений

\[ f(y; \theta, \phi) = \exp \left( \frac{y\theta - b(\theta)}{a(\phi)+c(y;\phi)} \right) \]

где

Нам всё же интересны ковариационные характеристики величин $ Y_i $.
Но оказывается, что их не так просто посчитать.

Матожидание $ Y_i $ получается интегрированием по распределению, а интегрировать приходится сложную конструкцию ($b_i$ -- случайная величина).
Но если $ g(u)=\log u $, $ \log (\mathds E_\theta(Y_i)) = X_i' \beta $.

Матрица ковариации: есть обобщение на многомерный случай формулы для дисперсии

\[ Var_\theta(Y_i) = Var_\theta(\mathds E_\theta(Y_i|b_i)) + \mathds E_\theta(Var_\theta (Y_i|b_i)) \]
Получается достаточно сложное выражение. \\

\subsection{Оценка параметров модели}

$ \phi $ -- мешающий параметр -- формально не входит в модель, но иногда появляется (например, в нормальном распределении).
Но в данную модель это не входит, поскольку считаем, что $ Y_i $ условно независимы.

Функция правдоподобия

\[ L(Y; \beta, \sigma, \phi) = \prod_{i=1}^{k} h(Y_i; \beta, \sigma, \phi) \]
Произведение по блокам, поскольку блоки независимы.
Каждое выражение $ h(...) $ вычисляется не очень просто.
\begin{enumerate}[noitemsep]
	\item $ h(Y_i; \beta, \sigma, \phi) = \int_{\mathds R^{q_i}} l(Y_i; b, \beta, \phi) p_\sigma (b) db $ ( в лекциях $ h(Y_i; \beta, \sigma, \phi_i) $, но имеется в виду различный мешающий параметр в конечном числе различных блоков, и в силу векторности можно, не умаляя общности, считать его одинаковым по всем блокам).
	\item $ l(Y; \beta, \phi) = \prod_{i=1}^{n_i} f(Y_{i,j}; \theta_{ij}, \phi_i) $
\end{enumerate}

Оценивание производится по методу максимального правдоподобия; но есть и другие подходы (подход т. н. усечённого правдоподобия).
Чаще всего -- ММП.

Обычно переходят к логарифмам (поскольку обычно нет потребности рассматривать нерегулярные семейства).
Оценку можно найти из системы уравнений.
\[ U \] % TODO formula
Может возникнуть проблема, связанная с тем, что решение не единственное.
Обратная к матрице Гёссе будет неотрицательно определена, но не обязательно положительно определена.

Но если мы берём матожидание и семейство регулярное, то это матрица информации, а она положительно определена.
Поэтому с ростом $ n $ вероятность того, что матрица будет положительно определена, растёт.

Вместо решения системы можно использовать последовательные приближения:

\[ \theta^{(i+1)} = \theta^{(i)} + \mathds I ((\theta^{(i)})^{-1}) \cdot U(Y; \theta^{(i)}) \]

Наилучшее предсказание случайного эффекта: в данном случае используется байесовская оценка (считаем, что $ b_i $ -- параметр, но параметр, имеющий распределение).
Expected \emph{a posteriori} (EAP) -- не в полной мере байесовская оценка.

Критерии типа Вальда здесь уже неприменимы.
Если оцениваем $ \sigma $, оценка не может быть распределена асимптотически нормально (отрицательным значение параметра быть не может, т. о., некая масса в нуле).
Но оказывается, что иногда можно подобрать некую смесь вырожденного и усечённого нормального распределения, котрая будет предельной для параметра.
Впрочем, такие методы, как оказывается, довольно сложны в реализации (и нет уверенности в том, что методы, реализованные в  R, работают так, как мы ожидаем).

Так или иначе, гипотеза об отсутствии случайного эффекта не очень интересна в такой модели.

\subsection{Логистическая регрессия}

Можно рассмотреть определённые модели.
Логистическая регрессия основана на распределении Бернулли: величина принимает значение $ 0 $ или $ 1 $; её можно представить в виде экспоненциального семейства.

\[ f(y;p) = p^y (1-p)^{1-y} = \exp \left( \frac{y\theta - b(\theta)}{a(\phi) +c(y;\phi)} \right) \]

Канонический параметр $ \theta $ выражается через канонический: $ \theta = \log \left( \frac{p}{1-p} \right) = logit(p) $.
Несложно показать, что матожидание... % TODO a bit

\subsection{ Модель с простым эффектом индивида }
Обычно вопрос исследования ставится не отностительно индивида, а относительно популяции.
Если эффект случайный (случайно выбираем из популяции набор индвидов, потом по этой выборке делаем выводы) и считаем, что эффект индивида -- некая нормально распределённая аддитивная добавка с нулевым матожиданием (к каждому наблюдению добавляется некая аддитивная величина); может быть чисто случайное отклонение, которое уже нельзя, как в классической модели, изобразить в виде ошибки.
Такой подход позволяет формировать зависимость определённого рода между наблюдениями, соответствующими одному индвиду.

\[ g(\mathds E_\theta (Y|z,v)) = X(z)^T\beta + v \]
$v \sim \mathcal N(, \sigma_v^2)$ -- простой эффект индивида.

При фиксированных значениях % TODO a bit

Если мы по индивиду (по эффекту) усредняем, получается интеграл, который в явном виде не считается, и оставляем его в той форме, в которой мы его получаем.
Можно преобразовать, но под интегралом всё равно остаётся конструкция, которая просто не вычисляется.
Как вариант, можно разложить интеграл в ряд Тейлора, и члены разложения будут моментами нормального распределения.

% TODO some (distracted: AI challenge)

Бинарный результат эксперимента: $ Y_{ij} = 1_{\{Y_{ij}^* > 0\}} $.
Здесь $ Y_{ij}^* $ -- скрытый количественный признак,
\[ Y_{ij}^* = X(z_{ij})^T \beta + v_i + e_{ij} \]

\begin{leftbar}
	"У меня есть некие замечания к такому предположению. Фактически это сделано для того, чтобы провести хотя бы частичную аналогию с моделю классической линейной регрессии".
\end{leftbar}

Оценки ММП: можно использовать всякие численные методы -- итеративный, оптимизацию.

Наилучший прогноз (как мы говорили, это байесовская оценка), по сути, не охватывает никакие наблюдения, кроме $ i $-го.
Получается явная формула
\[ \tilde v_i = \mathds E (v_i | Y) = \frac{1}{L_i(Y_i)} \int_{\mathds R} v L_i(...) \phi(...) dv \]
\textbf{Здесь $ \phi $  --  не мешающий параметр, а плотность нормального распределения!}
Формулы сложные, но в принципе реализуемые, и кто-то их уже реализовал :)

% TODO a bit

\subsection{Случайный эффект}

Что касается неслучайного параметра, тут всё основано на оценках максимального правдоподобия, и всё, что мы изучали для обобщённых моделей, переносится сюда.
Со случайным эффектом вопрос более сложный.
Впрочем, и задача обычно не такая интересная.

Может оказаться, что достаточно плохо будет искаться оценка для параметра регрессии и мешающего параметра, поэтому по возможности отделяют задачу оценки мешающего параметра от параметра от регрессии (вначала находят какую-то состоятельную оценку параметра регрессии, потом мешающего параметра, а потом уже некую асимптотически нормальную оценку "псевдомаксимального правдоподобия").

Если семейство регулярное, МП-оценка будет асимптотически оптимальна (напомним, в асимптотическом неравенстве на МП-оценке достигается равенство).

Что касается доверительных областей: можно строить доверительные области на основе МП-оценок, но есть тонкости.
Если параметр внутри доверительной области, всё хорошо и при некоторых естественных предположениях будет достигаться асимптотическая нормальность.

Проверка гипотез: проверка гипотезы о наличии некого фиксированного случайного эффекта... % TODO

Ввиду высокой размерности сложно представить себе множество элементов, на которых матрица ковариации неотрицательно определённая.

Если параметр лежит на границе, дисперсия равна нулю и уже так просто не построить критерий.

\begin{leftbar}
	Вывод: проверка гипотезы отсутствия случайного эффекта -- задача сложная и требующая специального подхода.
	Классическим подходом её решить не получается.
\end{leftbar}

\subsection{Выбор наилучшей модели}

Для классических и даже обобщённых моделей были разработаны критерии Акаике и байесовский (принадлежащие классу информационных).
Они созданы для того, чтобы сбалансировать имеющееся количество статистической информации и сложность модели.
Если модель будет слишком простая, в реальности это может никогда не выполняться.

Для смешанных моделей эти критерии неприменимы (в доказательстве теории каждого из критериев случайные эффекты не учитываются).

Тем не менее, оптимизировать как-то приходится.
Есть некоторые исследования, которые позволяют выбирать наилучшую модель.
"Из того, что мне известно -- conditional Akaike information criteria, cAIC".
Если модель без случайного эффекта, условный должен соответствовать обычному.

\end{document}