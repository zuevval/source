% !TeX spellcheck = en_US
% !TeX program = xelatex

\documentclass[a4paper,12pt]{article}
\usepackage[utf8]{inputenc} % russian, do not change
\usepackage[T2A, T1]{fontenc} % russian, do not change
\usepackage[english, russian]{babel} % russian, do not change

% fonts
\usepackage{fontspec} % different fonts
\setmainfont{Times New Roman}
\usepackage{setspace,amsmath}
\usepackage{amssymb} %common math symbols
\usepackage{dsfont}

\usepackage{enumitem}

\usepackage{hyperref}
\hypersetup{pdfstartview=FitH,  linkcolor=blue, urlcolor=blue, colorlinks=true}
\usepackage{graphicx}

\begin{document}

\title{Машинное обучение -- Трансформеры}
\maketitle
\tableofcontents
	
\section{Mar 12, 2025 (Wed) | Self-attention }

\subsection{A brief recap}

Мы сказали, что attention -- главный элемент современных систем.
Attention основан на рергессии Надарая-Уотсона.
Мне кажется, это более понятная

Есть два основных вида Attention: additive attention (фактически копируют RNN -- рекурсивные НН).
И главная -- Dot Product Attention, иначе называемый Scaled Dot Product Attention.
Скоринговая функция в этом attention'е -- softmax, то есть вероятности от произведения матриц.
Эти произведения матриц -- фактически произведения векторов в матричной форме.

\[ softmax \left( \frac{QK^T}{\sqrt d} \right) V \in \mathds R ^{n \times n}  \]
\[ V = X * W_V, \dots \]

Важно, что матрицы $ W_K, W_Q, W_V $ \emph{обучаемые}.

Говорят, что трансформер -- нейросеть, но фактически мы уже отходим от нейросетей.
Похоже на нейросеть вот в чём: мы вводим нелинейность, чтобы модель была более общей; а ещё -- обучаем методом обратного распространения ошибки.

И ещё мы рассмотрели важный элемент -- self-attention.
\[ \{ x_1, \dots, x_n \} \to \{ x_1^*, \dots, x_n^* \}, x_i \in \mathds R^d \]
Это когда мы пытаемся из последовательности старых элементов (векторов) каким-то образом получить новые.
И мы считаем attention между одной точкой и всей последовательностью.

\[ x, \{ x_1, \dots, x_n \} \]
\[ x^* = \sum_{i=1}^n \alpha (x, x_i) \cdot x_i \]

По сути это выпуклая комбинация всех точек последовательности.

После применения self-attention точки будут сближенны.
И это можно делать сколько угодно раз.
В первом трансформере ("Vanilla Transformer", Attention is all you need) предложили сделать self-attention 6 раз -- якобы 5 мало, 7 много.

Основная проблема: каждый вектор должен "поздороваться" с $ n-1 $ другими векторами контекста, поэтому сложность self-attention -- квадрат.

Self-attention как метод сглаживания (non-local means denoising)

Далее мы перешли к NLP, сказали, что есть RNN с вектором скрытых состоряний $ h_t $ в момент $ t $, который по сути является в некотором роде вектором внимания.
Он всё время обновляется.
На выходе имеем $ y $.
Фактически это внимание с аддитивной скоринговой функцией.

Проблема, как мы говорили, -- быстрое забывание.
Предложена LSTM, которая постаралась решить проблемы быстрого забывания, но, на мой взгляд, это не решило до конца проблему.

Ещё одна изначальная проблема метода: нужно как-то "развязать" выходную последовательность и входную, то есть сделать так, чтобы количество $ y $ и $ x $.
Вспомнили, что есть автокодеры -- encoder, decoder; т.о. сделали скрытый вектор (латентное представление) между encoder-частью и decoder-частью.
Это тоже была проблема: 
Но это тоже был прогресс, хоть и небольшой.

Прогресс пошёл, когда придумали attention layer.

Ещё мы рассмотрели многомерное внимание (multi-head attention), и сказали, что оно нужно, чтобы посмотреть на наши данные как бы с нескольких сторон.
Точнее, так: когда инициализируем веса, результат зависит от случайности.
Поэтому инициализируем несколько наборов, и потом результаты либо конкатенируются, либо усредняются.
Чтобы элементы были максимально различны, была предложена регуляризация.
Матрица должна быть ортогональна.

\[  || AA^T - I || ^2 \to \min  \]

\subsection{Self-Attention}

Есть входной набор токенов (например, Bank at the river).
Пусть для простоты один токен соответствует одному слову ($ t_i, i \in 1 \dots 4 $).
Для каждого токена $ t_i $ генерируем вектор $ v_i $, получаем набор векторов.

К полученной последовательности векторов применяем self-attention.
И наша задача -- чтобы вектор, соответствующий слову Bank, был близок в латентном пространстве к слову "берег"\hspace*{0pt}, а не к слову "банк".

Включается Vector Dot Product (это проще, чем считать взвешенную сумму).
Рассмотрим для примера вектор $ v_3 $.
Считаем его attention с остальными векторами.

\[
s_{31} = v_3 \cdot v_1 \quad % TODO multiline
s_{32} = v_3 \cdot v_2
\dots 
\]

А дальше нормализация: каждый из весов делим на сумму всех.
Вспоминаем Надарая-Уотсона.

\[ w_{3i} = \alpha (v_3, v_i) = \frac{K(v_3, v_i)}{\sum_{j=1}^4 K(v_3, v_j)}, \quad \sum_i w_{3i} = 1 \]
\[ y_3 = \sum_i w_{3i} v_i \]

Т.о. преобразовали исходную последовательность векторов в новую с учётом их внутреннего контекста.

А чего не хватает?
Не хватает обучения :)

Можно и не учить, в каком-то смысле пройдёт и так -- будет учитывать близость.
Но лучше обучить.
Добавляем веса к Values, Keys и Queries.

\[ v_i \Rightarrow v_i M_k,  \dots \]

$ M_k $ мы в предыдущем параграфе обозначали $ W_K $

Итак, делаем линейное преобразование $ v_i $, умножая на матрицы весов Keys, Queries, Values.
Потом Keys и Queries перемножаются (точнее, делается self-attention), нормализуем, ...

\subsection{Трансформер для машинного перевода}

На самом деле, архитектур трансформеров очень-очень много.
Vanilla Transformer был реализован для перевода, поэтому он брал архитектуру свёрточной нейросети с энкодером и декодером.
Это чисто авторегрессионная модель.

Важны не только слова, но и их последовательность.
Поэтому к каждому вектору добавляем некоторую информацию о порядке.
Просто конкатенируем.

позиционное кодирование (position coding): добавляем позиционные векторы $ p_i $ так: $ x_i \to x_i + p_i $

На выходе трансформера-кодировщика получаем фактически те же $ y_i $, но с positional encoding, с нескольким многократно применённым self-attention, с полносвязным слоем.
Что только не делают с нашими исходными словами в предложении, но несмотря на то, что кажется какой-то ерундой, это работает.

\hrule

Позиционное кодирование уже на следующей лекции.

\section{ Mar 19, 2025 (Wed) | Positional encoding }
\subsection{ A brief recap }

Самовнимание -- множество операций квадратичной сложности (эмбеддинг каждого токена аттендится с остальными).
Базой является регрессия Надарая-Уотсона.
Всё это умножается друг на друга, складывается; при этом каждый элемент умножается на матрицу.
Три матрицы: keys, queries, values (они обучаемы, и это самое главное).
Затем делается multi-head attention: несколько параллельно процессов self-attention.
Если $ m $ голов, то обучается $ 3 \cdot m $ различных матриц.

Архитектура трансформера условно состоит условно из двух элементов -- кодировщик и декодировщик.
Иногда декодировщик не нужен, но для перевода, генерации новых токенов (скажем, GPT и так далее) используются оба модуля.

Структура кодировщика:
позиционное кодирование (просто прибавляем вектор позиционного кодирования к исходному) $ \to $ multi-head attention $ \to $ нормировка (стандартизация): сводим какое-то распределение к стандартному нормальному (средне 0, дисперсия 1).
Дальше используется полносвязная сеть; зачем она нужна? -- вводится ещё нелинейность.
Дальше опять нормировка.

В некоторых ситуациях можно ускорить квадрат, если нам не нужно аттендить всё со всем.


\subsection{Позиционное кодирование}

Много идей было, это целое направление исследований.
Алгоритмов позиционного кодирования сейчас много, есть эффективные; мы рассмотрим классику.
Похоже на то, как в деревьях решений есть классический алгоритм (CART), а есть ещё куча алгоритмов, которые, возможно, в чём-то и эффективнее, но до сих пор используют классику -- CART.

<<Тупая>> идея: каждой позиции поставить в соответствие число в интервале [0;1].
Но плохо работает.
Критерии: должно быть уникальным; модель должна обобщаться на более длинные, притом расстояние между двумя временными шагами должно быть всегда одинаковым.

Пусть $ x_j \in \mathds R^d : xp_j = x + \hat p_j $

\[ \hat p_t^i = \begin{cases}
	\sin \left( w_k t \right), i = 2k \\
	\cos \left( w_k t \right), i = 2k + 1
\end{cases}, w_k = \frac{1}{TODO} \]

чтобы постоянно при увеличении k вектор $ w $ уменьшался и мы сжимались

Почему сложение, а не конкатенация?

\subsection{Декодер}

Авторегрессионная модель.

Всё то же: к \textbf{исходной последовательности токенов} применяем multi-head attention, ... и под конец используем векторы $ z $, полученные на выходе кодировщика -- пытаемся сделать attention с вектором $ h_t' $ (это называется cross-attention).

При помощи softmax получаем вероятности различных токенов.
Наиболее вероятный поступает на выход.

Эмбеддинг начала (<BOS> - begin of sentence) нужен для инициализации авторегрессии.
В конце выдаётся токен <EOS> -- end of sentence

Критерии обучения:
если перевод, то стандартная метрика -- максимальное правдоподобие (максимизируем сумму логарифмов правдоподобия).

Для машинных переводчиков  $ n $-грамы сейчас прошлое.
Более интересная вещь -- BERT: кодировщик без декодировщика.
Служит для обучения многих задач, и часто предобученный BERT служит для fine-tuning.
BERT -- в некотором смысле самообучающаяся модель: MLM -- masked language modeling.

\subsection{ViT = vision transformer}

Поскольку трансформеры оказались очень успешными для NLP, появилась идея: почему бы их не использовать в CV?
И оказалось, что трансформеры хороши, могут конкурировать с нейросетями, и даже не просто в анализе изображений, а в анализе мультимодальных данных.
Идея: почему бы не представить изображение в качестве последовательности токенов?
(на самом деле не только к картинкам так можно; у Андрея, например, была идея с деревьями).

Patch + Positional Encoding $ \to $ последовательность эмбеддингов патчей.
Добавляется ещё один символ -- CLS (классификационный символ), некоторый дополнительный эмбеддинг, который кодирует класс изображения.
Последовательность прогоняем через Transformer Encoder; всё стандартно, как в NLP.
На выходе трансформера у нас единственное декодирование -- эмбеддинг класса декодируем обратно в класс.

Что ещё интересного в ViT? (придумано позже): хорошо, пусть мы разбили на некоторые патчи, а можем на ещё более мелкие, и параллельно реализовать несколько ViT, которые одновременно обучаются end-to-end.
Немножко похоже на то, что было в нейросетях: скользящее окно и пулинг.

\subsection{Tabular data}

Изначальная идея -- Бабенко (Яндекс).
Потом за ними пошли кучи других трансформеров для табличных данных.

Идея -- сделать т.н. feature tokenizer.
Самая простая идея -- некоторая линейная функция от исходных признаков.
А где контекст?

Одна матрица для одного признака.
Категориальные -- обычно one-hot encoding (но после умножения на матрицу всё равно один вектор)

\section{Mar 26, 2025 | XAI}
\subsection{Recap}

Как объяснить "чёрный ящик"?
Объясняемость моделей растёт с простотой.
Деревья решений почти не используются для объяснения, а вот линейные модели -- вполне.

Есть feature selection -- определяет, какие признаки наиболее значимы \textit{в датасете в целом}.
XAI -- похожий подход, но мы говорим только об одном экземпляре.
Притом для картинок желательно произвести какие-то объясняющие слова, а для остальных типов данных -- просто выбрать наиболее значимые признаки.

\subsection{XAI}

Для картинок используют heatmap'ы.
В графовых данных примерно то же -- показываем, какие узлы и связи наиболее значимы.

На самом деле, методов объяснения огромное количество.
Наиболее интересные -- те, которые работают с моделью как с "чёрным ящиком"\hspace{0pt} (post-hoc, агностические).

Объяснение может быть глобальным или локальным.
Можем объяснять один пример; это обычно более интересно, чем объяснить, почему в среднем по больнице температура 36.6.
Но можно попробовать объяснить, почему у некоторой когорты больных температура понижена (или повышена).

\subsection{Общая идея локальной интерпретации}

Главная идея -- построение метамодели (дополнительной модели), которая что-то объясняет.
Построим линейную функцию $ g(x) $, которая локально объясняет нашу $ f(x) $.
 
\[ g(x) = \sum_i a_i x_i \]

Может быть логистическая регрессия как модификация линейной для задачи классификации; GAM (обобщённая аддитивная модель).

$ \theta $ -- вектор параметров метамодели

Задача оптимизации:
\[ L(f, g, \theta) + \Omega(g) \to min  \]

(если задача поставлена некорректно, добавляется регуляризация - $ \Omega(g) $)

\subsection{LIME}

Заметьте, что кружочки в LIME разного размера.
Чем ближе точка к нашему примеру, тем больший вклад она вносит -- вводим веса.
Вспомним регрессию Надаррая-Уотсона (1968 год).

Но что делать, если вход высокой размерности (скажем, картинка)?
Возмущать каждый признак по отдельности?
Все вместе?
Можно использовать т.н. суперпиксели.

Ещё одна проблема -- может быть существенная нелинейность в локальной области.

\subsection{GAM}

Можно попробовать решать проблему нелинейности в локальной области.

\[ g(x) = \sum_i g_i(x_i) \]

Это всё красиво на рисунке, на самом же деле всё не так хорошо обучается.

И ещё минус GAM: если в линейной модели мы получаем коэффициенты, здесь мы получаем какие-то функции -- и что?

NAM -- Neural additive models

\subsection{SHAP}

Шепли -- Нобелевский лауреат по экономике за теорию коалиционных игр.
Числа Шепли описывают вклад каждого игрока.

Особенность SHAP -- не нужно генерировать новые признаки!

Пусть у нас есть признаки $ x_1, \dots, x_d $; обозначим за $ F = \{ 1, \dots, d \} $ множество индексов.
Составим множество мощности (power set, булеан) -- множество всех подмножеств (размер -- $ 2^d $).

\[ \phi_i = \sum_{S \subseteq  F \backslash \{i\}} \frac{|S|! (|F|-|S|-1)!}{|F|!} [ f_{S \cup \{i\}} \left( x_{S \cup \{i\}} \right) - f_S \left(x_S\right) ] \]

$ F $ -- полная коалиция, $ S $ -- подмножество коалиции, не включающее игрока $ i $.

Одна из проблем -- число комбинаций, которые нужно перебирать, экспоненциально растёт с ростом числа признаков.

Removal Problem -- проблема удаления признаков (пожалуй, даже более существенная).
Самое популярное решение -- заменить значение признака среним, хотя оно не всегда обосновано.

Числа Шепли могут быть отрицательными.
Это означает, что "игрок"-признак влияет в негативном смысле.

\subsection{example-based explanation}

До сих пор мы делали feature-based explanation.
Используется "прототип"\hspace{0pt} -- похожий пример.
И смотрим, насколько данный пример далёк от данного прототипа.
Пример -- kNN.

Counterfactual explanations.
Интересная задача: хотим не только знать, в какой класс попадаем, но и как с минимальными затратами попасть в другой класс.

\section{Apr 2, 2025 | One-Shot Learning}
\subsection{Intro}

Omniglot: 20 алфавитов.
Требуется определить, к какому алфавиту принадлежит никогда ранее не виденная буква.

KNN -- ищем одного ближайшего соседа.
Например, по Евклидову расстоянию.
На Omniglot KNN даёт точность окло $ 25\% $, угадывание -- $ 5\% $, человек -- $ 95.5\% $
Иерархическая байесовская оптимизация даёт $ 95\% $

Один из лучших с точки зрения LVU методов -- \emph{Сиамские сети}.

\subsection{Сиамские сети}

Пусть есть 4 класса: кошка, собака, волк, тигр.
Если судить только по картинке, то кошка близка к тигру, волк -- к собаке.
Но семантически кошка должна быть ближе к собаке (оба домашние животные), а волк -- к тигру.
Сиамские сети -- пара энкодеров с одним и тем же преобразованием, которая пытается выучить такое отображение.

Если в кажом классе по $ m $ примеров, можно сгенерировать $ C_m^{m-1} = m \cdot (m-1) $ примеров пар внутри класса и $ m^k $ пар вне класса, где $ k $ -- число классов.

Но две сетки -- это условно.
На самом деле это \emph{одна и та же сетка}.

Какая используется loss-функция?
Обычно contrastive loss.

\[ l(x_i, x_j, z_{ij}) = z_{ij} d_k(e_i, e_j) + (1 - z_{ij}) \max(0, T - d(e_i, e_j)) \]

где $ z_{ij} = 1 $ в случае принадлежности к одному классу и $ 0 $ в противном случае; $e_{(\cdot)} $ -- эмбеддинг.

\subsection{Zero-Shot Learning}

CLIP (Contrastive Language Image Pretraining) -- OpenAI сделали несколько лет назад.
Это и сегодня одно из интересных приложений.

Идея: хотим по изображению получить некоторый текст, а по тексту -- изображение.

Как пишут OpenAI, это unsupervised learning, но с точки зрения ЛВ это хитрость, которая скорее обманывает людей.

Не зря названо pre-training.
Предобучение есть, и это связано с foundational models.

В отличие от сиамской сети, здесь две разные сетки.

Текст -- трансформер; картинка -- ResNet, или ViT, или ещё что-то.
А дальше как в self-attention: произведение векторов.
Если произведение векторов максимально, объекты близки друг к другу, иначе далеки.
Составляется огромная таблица -- миллион на миллион пар; самые большие должны быть вектора по диагонали!
Обучение -- с помощью примерно той же лосс-функции, что для сиамской сети.

Чтобы качество было хорошим, нужно огромное количество примеров, миллионы примеров.
Поэтому в медицине CLIP работает плохо: допустим, есть рентгеновские снимки и описания, хотим получить описание по снимку; но (а) данных мало и сложно обучить, (б) маленькое изменение эмбеддинга влечёт за собой полное изменение текста, что недопустимо.
Чтобы уйти от этого, было создано т.н. concept-based learning, когда мы описываем каждый новый объект ограниченным числом \emph{концептов}.

\section{Apr 2, 2025 | Функции потерь и показатели качества}
\subsection{Intro}

Главное в любой модели -- функция потерь.
Обычно минимизируем сумму по всем примерам некоторой функции от метки $y_i$ и $ f(x_i) $.
Также можно прибавлять регуляризацию $ R(f) $; на практике обычно регуляризация -- функция параметров нашего решающего правила.

Все алгоритмы обучаются с помощью градиентного спуска.

\subsection{Функции потерь для регрессии}

Функции потерь: может быть дивергенция Кульбака-Лейблера, Hinge Loss function, Cosine Similarity, ...
Есть функции, пришедшие из физики -- energy-based functions; почти все похожи.
Есть функции ранжирования; Contrastive loss -- частный случай ранжирования.
Есть минимаксные, расстояние Вассенштайна; диффузионные модели (связано с GANами).

Huber loss -- робастная оценка, которая позволяет немного нивелировать выбросы.
Когда расстояние между всеми $ f(x_i) $ и $ y_i $ мало, это просто MSE.

Логарифмический косинус -- встречается редко.

Есть Root Mean Squared Logarithmic Error Loss.

\subsection{Функции потерь для классификации}

Hinge Loss.
Zero-One Loss (ступенька) -- невыпуклая и недиффиренцируемая; в SVM используются её приближения.
Кросс-энтропия часто используется с нейросетями.

Есть сглаженные функции, но ЛВ их практически не встречал.

Cosine Similarity loss -- более популярная; достаточно просто вычисляется, когда $ y $ -- вектор (к примеру, вектор вероятности классов).

Самая популярная, конечно, кросс-энтропия.
Она пошла от оценки МП.

Кульбака-Лейблера -- тоже похожа на кросс-энтропию, тоже определяет близость между двумя плотностями распределения.
Но это не расстояние, а именно дивергенция, потому что не обладает свойсвом симметричности и удовлетворяет неравенству треугольника.

\section{}
\subsection{}

\begin{enumerate}[noitemsep]
	\item 
\end{enumerate}
	
	
\end{document}